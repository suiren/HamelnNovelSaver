#!/usr/bin/env python3
"""
å®Ÿéš›ã®ãƒ“ãƒ«ãƒ‰çµæœã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from hameln_scraper_final import HamelnFinalScraper

def test_actual_build():
    """å®Ÿéš›ã®ãƒ“ãƒ«ãƒ‰çµæœã‚’ãƒ†ã‚¹ãƒˆ"""
    print("=== å®Ÿéš›ã®ãƒ“ãƒ«ãƒ‰ãƒ†ã‚¹ãƒˆ ===")
    
    # ãƒ†ã‚¹ãƒˆç”¨URLï¼ˆç›®æ¬¡ãƒšãƒ¼ã‚¸ï¼‰
    test_url = "https://syosetu.org/novel/378070/"
    
    print(f"ãƒ†ã‚¹ãƒˆURL: {test_url}")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–
    scraper = HamelnFinalScraper()
    
    try:
        # å…¨è©±å–å¾—ã‚’å®Ÿè¡Œ
        print("å…¨è©±å–å¾—ã‚’å®Ÿè¡Œä¸­...")
        result = scraper.scrape_novel(test_url)
        
        if result:
            print(f"âœ“ å–å¾—æˆåŠŸ: {result}")
        else:
            print("âœ— å–å¾—å¤±æ•—")
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        print("\n=== ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ===")
        import os
        if os.path.exists("saved_novels"):
            for root, dirs, files in os.walk("saved_novels"):
                for file in files:
                    if file.endswith('.html'):
                        full_path = os.path.join(root, file)
                        print(f"ğŸ“„ {full_path}")
        
        # ç¬¬2è©±ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
        print("\n=== ãƒªãƒ³ã‚¯ç¢ºèªï¼ˆç¬¬2è©±ï¼‰ ===")
        html_files = []
        if os.path.exists("saved_novels"):
            for root, dirs, files in os.walk("saved_novels"):
                for file in files:
                    if file.endswith('.html') and 'ç¬¬ä¸€è©±' in file:
                        html_files.append(os.path.join(root, file))
        
        if html_files:
            test_file = html_files[0]
            print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {test_file}")
            
            with open(test_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å‰ã®è©±ãƒ»æ¬¡ã®è©±ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')
            
            # å…¨ã¦ã®aã‚¿ã‚°ã‚’ç¢ºèª
            links = soup.find_all('a', href=True)
            prev_links = []
            next_links = []
            
            for link in links:
                text = link.get_text(strip=True)
                href = link.get('href')
                
                if 'å‰' in text or 'prev' in text.lower():
                    prev_links.append((text, href))
                    print(f"å‰ã®è©±ãƒªãƒ³ã‚¯: {text} -> {href}")
                
                if 'æ¬¡' in text or 'next' in text.lower():
                    next_links.append((text, href))
                    print(f"æ¬¡ã®è©±ãƒªãƒ³ã‚¯: {text} -> {href}")
            
            print(f"å‰ã®è©±ãƒªãƒ³ã‚¯æ•°: {len(prev_links)}")
            print(f"æ¬¡ã®è©±ãƒªãƒ³ã‚¯æ•°: {len(next_links)}")
        else:
            print("HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    finally:
        scraper.close()

if __name__ == "__main__":
    test_actual_build()