#!/usr/bin/env python3
"""
ãƒãƒ¼ãƒ¡ãƒ«ãƒ³å°èª¬ä¿å­˜ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ - æœ€çµ‚ç‰ˆ
å®Œå…¨ãªãƒšãƒ¼ã‚¸ä¿å­˜æ©Ÿèƒ½ï¼ˆCSSã€JavaScriptã€ç”»åƒç­‰ã‚’å«ã‚€ï¼‰
ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸæ§‹é€ 
"""

import os
import sys
from hameln_scraper.core.scraper import HamelnScraper
from hameln_scraper.core.config import ScraperConfig

class HamelnFinalScraper(HamelnScraper):
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        config = ScraperConfig()
        config.enable_novel_info_saving = False
        config.enable_comments_saving = False
        super().__init__(config)


import time
import re
import base64
import requests
import cloudscraper
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from urllib.parse import urljoin, urlparse
from PIL import Image
import io
import logging
import traceback
from datetime import datetime
import gzip
import zlib
import brotli
import copy

class HamelnFinalScraperLegacy:
    def __init__(self, base_url="https://syosetu.org"):
        self.base_url = base_url
        self.driver = None
        self.cloudscraper = None
        self.session = requests.Session()
        self.debug_mode = True
        
        # ğŸ›ï¸ æ©Ÿèƒ½åˆ¶å¾¡ãƒ•ãƒ©ã‚°ï¼ˆNortonæ¤œå‡ºå•é¡Œè§£æ±ºã«ã‚ˆã‚Šã€æ–°æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–ï¼‰
        self.enable_novel_info_saving = True   # å°èª¬æƒ…å ±ä¿å­˜æ©Ÿèƒ½
        self.enable_comments_saving = True     # æ„Ÿæƒ³ä¿å­˜æ©Ÿèƒ½
        
        self.resource_cache = {}  # URL -> local_filename mapping
        
        self.setup_logging()
        self.setup_scrapers()
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
        logging.basicConfig(
            level=logging.DEBUG if self.debug_mode else logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('hameln_scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–é–‹å§‹")
        
    def debug_log(self, message, level="INFO"):
        """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚¢ãƒ—ãƒªå†…è¡¨ç¤ºï¼‹å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼‰"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        formatted_message = f"[{timestamp}] {level}: {message}"
        print(formatted_message)
        
        # å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å‡ºåŠ›
        try:
            log_file = "hameln_debug.log"
            with open(log_file, 'a', encoding='utf-8') as f:
                full_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{full_timestamp}] {level}: {message}\n")
        except Exception as e:
            print(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        if level == "ERROR":
            self.logger.error(message)
        elif level == "WARNING":
            self.logger.warning(message)
        elif level == "DEBUG":
            self.logger.debug(message)
        else:
            self.logger.info(message)
        
    def setup_scrapers(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’è¨­å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        try:
            self.debug_log("CloudScraperåˆæœŸåŒ–é–‹å§‹")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒªã‚¹ãƒˆ
            self.user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            self.current_ua_index = 0
            
            # CloudScraperè¨­å®šï¼ˆã‚ˆã‚Šé€²æ­©çš„ãªè¨­å®šï¼‰
            self.cloudscraper = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'desktop': True
                },
                delay=10,  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶
                debug=False
            )
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
            self.cloudscraper.headers.update({
                'User-Agent': self.user_agents[0],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            })
            
            self.debug_log("CloudScraperè¨­å®šå®Œäº†")
            
            # Selenium/ChromeåˆæœŸåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆCloudScraperã®ã¿ä½¿ç”¨ï¼‰
            self.debug_log("Chrome/ChromiumãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€CloudScraperã®ã¿ä½¿ç”¨")
            self.driver = None
            
        except Exception as e:
            self.debug_log(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            self.debug_log(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}", "ERROR")
            self.driver = None
            
    def rotate_user_agent(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
        new_ua = self.user_agents[self.current_ua_index]
        self.cloudscraper.headers.update({'User-Agent': new_ua})
        self.debug_log(f"User-Agentã‚’å¤‰æ›´: {new_ua[:50]}...")
        return new_ua
    
    def decompress_response(self, response):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®åœ§ç¸®è§£å‡å‡¦ç†ï¼ˆCloudScraperã®è‡ªå‹•å‡¦ç†ã‚’æ´»ç”¨ï¼‰"""
        # CloudScraperã¯è‡ªå‹•çš„ã«åœ§ç¸®ã‚’è§£å‡ã™ã‚‹ãŸã‚ã€response.textã‚’ç›´æ¥ä½¿ç”¨
        try:
            html_content = response.text
            self.debug_log(f"CloudScraperã§å–å¾—ã•ã‚ŒãŸHTMLãƒ†ã‚­ã‚¹ãƒˆé•·: {len(html_content)}")
            return html_content
        except Exception as e:
            self.debug_log(f"HTMLãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•è§£å‡ã‚’è©¦ã™
            content = response.content
            encoding = response.headers.get('content-encoding', '').lower()
            
            self.debug_log(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£å‡é–‹å§‹ - Content-Encoding: {encoding}")
            
            try:
                if encoding == 'gzip':
                    content = gzip.decompress(content)
                elif encoding == 'deflate':
                    content = zlib.decompress(content)
                elif encoding == 'br':
                    content = brotli.decompress(content)
                
                return content.decode('utf-8')
            except Exception as decode_error:
                self.debug_log(f"æ‰‹å‹•è§£å‡ã‚‚å¤±æ•—: {decode_error}", "ERROR")
                return ""
    
    def get_page(self, url, retry_count = 3):
        """ãƒšãƒ¼ã‚¸å–å¾—: CloudScraper â†’ Selenium ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        self.debug_log(f"ãƒšãƒ¼ã‚¸å–å¾—é–‹å§‹: {url}")
        
        # URLå½¢å¼ã®æ¤œè¨¼
        if not url.startswith('http'):
            self.debug_log(f"ç„¡åŠ¹ãªURLå½¢å¼: {url}", "ERROR")
            return None
        
        # ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã‚’è¨­ã‘ã¦ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è»½æ¸›
        time.sleep(2)  # ãƒ™ãƒ¼ã‚·ãƒƒã‚¯ãªå¾…æ©Ÿæ™‚é–“
        
        # ã¾ãšCloudScraperã‚’è©¦ã™ï¼ˆé«˜é€Ÿï¼‰
        for attempt in range(retry_count):
            try:
                if attempt > 0:
                    self.debug_log(f"CloudScraperå†è©¦è¡Œ {attempt + 1}/{retry_count}")
                    # User-Agentã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                    self.rotate_user_agent()
                    # å¾…æ©Ÿæ™‚é–“ã‚’æ¼¸é€²çš„ã«å¢—ã‚„ã™
                    wait_time = 5 + (attempt * 3)
                    self.debug_log(f"ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã‚’å–ã‚Šã¾ã™: {wait_time}ç§’")
                    time.sleep(wait_time)
                else:
                    self.debug_log("CloudScraperã§åˆå›è©¦è¡Œä¸­...")
                    
                response = self.cloudscraper.get(url, timeout=30)
                
                self.debug_log(f"CloudScraperãƒ¬ã‚¹ãƒãƒ³ã‚¹: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹={response.status_code}, ã‚µã‚¤ã‚º={len(response.content)}bytes")
                
                if response.status_code == 403:
                    self.debug_log("CloudScraper: 403 Forbidden - botæ¤œçŸ¥ã«ã‚ˆã‚Šæ‹’å¦", "WARNING")
                    if attempt < retry_count - 1:
                        continue  # å†è©¦è¡Œ
                elif response.status_code == 404:
                    self.debug_log("CloudScraper: 404 Not Found - ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ã¾ã›ã‚“", "WARNING")
                    return None  # 404ã¯å†è©¦è¡Œã—ãªã„
                elif response.status_code == 503:
                    self.debug_log("CloudScraper: 503 Service Unavailable - ã‚µãƒ¼ãƒãƒ¼ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä¸­", "WARNING")
                    if attempt < retry_count - 1:
                        continue  # å†è©¦è¡Œ
                elif response.status_code == 429:
                    self.debug_log("CloudScraper: 429 Too Many Requests - ãƒ¬ãƒ¼ãƒˆåˆ¶é™", "WARNING")
                    if attempt < retry_count - 1:
                        wait_time = 30 + (attempt * 15)  # ã‚ˆã‚Šé•·ã„å¾…æ©Ÿ
                        self.debug_log(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãŸã‚ {wait_time}ç§’å¾…æ©Ÿ...")
                        time.sleep(wait_time)
                        continue
                else:
                    response.raise_for_status()
                
                # åœ§ç¸®è§£å‡å‡¦ç†
                html_content = self.decompress_response(response)
                
                if not html_content:
                    self.debug_log("HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã§ã™", "ERROR")
                    if attempt < retry_count - 1:
                        continue
                    return None
                
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # ãƒšãƒ¼ã‚¸å†…å®¹ã®è©³ç´°åˆ†æ
                self.analyze_page_content(soup, "CloudScraper")
                
                # ãƒšãƒ¼ã‚¸ãŒæ­£å¸¸ã«å–å¾—ã§ããŸã‹ãƒã‚§ãƒƒã‚¯
                if self.validate_page(soup, url):
                    self.debug_log(f"CloudScraperã§å–å¾—æˆåŠŸ: {url}")
                    return soup
                else:
                    self.debug_log("CloudScraperã§å–å¾—ã—ãŸãƒšãƒ¼ã‚¸ãŒç„¡åŠ¹", "WARNING")
                    if attempt < retry_count - 1:
                        continue  # å†è©¦è¡Œ
                    else:
                        self.debug_log("CloudScraperã§ã®å…¨è©¦è¡ŒãŒå¤±æ•—ã€Seleniumã§å†è©¦è¡Œ...", "WARNING")
                        break
                        
            except requests.exceptions.Timeout:
                self.debug_log("CloudScraperã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", "ERROR")
                if attempt < retry_count - 1:
                    continue
            except requests.exceptions.ConnectionError:
                self.debug_log("CloudScraperæ¥ç¶šã‚¨ãƒ©ãƒ¼", "ERROR")
                if attempt < retry_count - 1:
                    continue
            except Exception as e:
                self.debug_log(f"CloudScraperã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
                self.debug_log(f"CloudScraperã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}", "DEBUG")
                if attempt < retry_count - 1:
                    continue
        
        # Seleniumã§å†è©¦è¡Œ
        if self.driver:
            for attempt in range(retry_count):
                try:
                    self.debug_log(f"Seleniumè©¦è¡Œ {attempt + 1}/{retry_count}")
                    
                    self.driver.get(url)
                    
                    # Cloudflareã®èªè¨¼ãƒã‚§ãƒƒã‚¯å¾…æ©Ÿï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·ï¼‰
                    try:
                        WebDriverWait(self.driver, 15).until(
                            lambda driver: "Cloudflare" not in driver.title and "Just a moment" not in driver.title
                        )
                        self.debug_log("Cloudflareèªè¨¼ã‚¯ãƒªã‚¢")
                    except:
                        self.debug_log("Cloudflareèªè¨¼å¾…æ©Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€ç¶šè¡Œ...", "WARNING")
                    
                    # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
                    time.sleep(8)
                    
                    # ç”»åƒã®é…å»¶èª­ã¿è¾¼ã¿ã‚’å¼·åˆ¶å®Ÿè¡Œï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§ç™ºç«ï¼‰
                    try:
                        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)
                        self.driver.execute_script("window.scrollTo(0, 0);")
                        time.sleep(2)
                    except Exception as e:
                        print(f"ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                    
                    # ãƒšãƒ¼ã‚¸å†…å®¹ã®è©³ç´°åˆ†æ
                    self.analyze_page_content(soup, "Selenium")
                    
                    if self.validate_page(soup, url):
                        self.debug_log(f"Seleniumã§å–å¾—æˆåŠŸ: {url}")
                        return soup
                    else:
                        self.debug_log(f"ç„¡åŠ¹ãªãƒšãƒ¼ã‚¸ã€å†è©¦è¡Œ {attempt + 1}/{retry_count}", "WARNING")
                        time.sleep(3)
                        
                except Exception as e:
                    self.debug_log(f"Seleniumè©¦è¡Œã‚¨ãƒ©ãƒ¼ {attempt + 1}/{retry_count}: {e}", "ERROR")
                    self.debug_log(f"Seleniumã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}", "DEBUG")
                    if attempt < retry_count - 1:
                        time.sleep(5)
        else:
            self.debug_log("Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“", "ERROR")
        
        self.debug_log(f"å…¨ã¦ã®æ–¹æ³•ã§å¤±æ•—: {url}", "ERROR")
        return None
        
    def analyze_page_content(self, soup, method):
        """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’è©³ç´°åˆ†æ"""
        if not soup:
            self.debug_log(f"{method}: soupãŒNone", "ERROR")
            return
            
        title = soup.title.string if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        body_length = len(soup.get_text()) if soup else 0
        
        self.debug_log(f"{method}å–å¾—ãƒšãƒ¼ã‚¸åˆ†æ:")
        self.debug_log(f"  - ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        self.debug_log(f"  - æœ¬æ–‡é•·: {body_length}æ–‡å­—")
        
        # HTMLã®æœ€åˆã®500æ–‡å­—ã‚’ç¢ºèª
        html_snippet = str(soup)[:500]
        self.debug_log(f"  - HTMLå…ˆé ­éƒ¨åˆ†: {html_snippet}...")
        
        # ç‰¹å®šã®è¦ç´ ã®å­˜åœ¨ç¢ºèª
        elements_check = {
            'h1': len(soup.find_all('h1')),
            'div': len(soup.find_all('div')),
            'a': len(soup.find_all('a')),
            'novel_body': len(soup.find_all('div', class_='novel_body')),
            'novel_view': len(soup.find_all('div', class_='novel_view'))
        }
        
        for element, count in elements_check.items():
            self.debug_log(f"  - {element}è¦ç´ æ•°: {count}")
        
        # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
        error_indicators = ['404', 'Error', 'Forbidden', 'Access Denied', 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹']
        for indicator in error_indicators:
            if indicator in title:
                self.debug_log(f"  - ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸æ¤œå‡º: {indicator}", "WARNING")
                
    def validate_page(self, soup, url):
        """ãƒšãƒ¼ã‚¸ãŒæ­£å¸¸ã«å–å¾—ã§ããŸã‹ãƒã‚§ãƒƒã‚¯"""
        if not soup:
            self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: soupãŒNone", "ERROR")
            return False
            
        title = soup.title.string if soup.title else ""
        
        # Cloudflareã®èªè¨¼ãƒšãƒ¼ã‚¸ãƒã‚§ãƒƒã‚¯
        if 'Cloudflare' in title or 'Just a moment' in title:
            self.debug_log(f"ãƒšãƒ¼ã‚¸æ¤œè¨¼: Cloudflareèªè¨¼ãƒšãƒ¼ã‚¸æ¤œå‡º - {title}", "ERROR")
            return False
            
        # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒã‚§ãƒƒã‚¯
        error_keywords = ['404', 'Error', 'Forbidden', 'Access Denied', 'Not Found', 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹']
        for keyword in error_keywords:
            if keyword in title:
                self.debug_log(f"ãƒšãƒ¼ã‚¸æ¤œè¨¼: ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸æ¤œå‡º - {keyword} in {title}", "ERROR")
                return False
            
        # æœ€ä½é™ã®å†…å®¹ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        text_content = soup.get_text(strip=True)
        
        # åŸºæœ¬çš„ãªHTMLæ§‹é€ ãƒã‚§ãƒƒã‚¯ï¼ˆbodyã‚¿ã‚°ãŒãªã„å ´åˆã‚‚è¨±å¯ï¼‰
        if not soup.find('body'):
            self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: bodyã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆSPAã®å¯èƒ½æ€§ï¼‰", "WARNING")
            # bodyã‚¿ã‚°ãŒãªãã¦ã‚‚ã€æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Œã°ç¶šè¡Œ
            if len(text_content) > 500:  # ååˆ†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆ
                self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: bodyã‚¿ã‚°ãªã—ã§ã‚‚ååˆ†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ãŸã‚ç¶šè¡Œ", "INFO")
            else:
                self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: bodyã‚¿ã‚°ãªã—ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚ä¸ååˆ†", "ERROR")
                return False
        elif len(text_content) < 100:
            self.debug_log(f"ãƒšãƒ¼ã‚¸æ¤œè¨¼: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå°‘ãªã™ãã¾ã™ ({len(text_content)}æ–‡å­—)", "WARNING")
            return False
        
        # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ãƒã‚§ãƒƒã‚¯
        if 'syosetu.org' in url:
            # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ç¢ºèª
            if not any(indicator in text_content for indicator in ['ãƒãƒ¼ãƒ¡ãƒ«ãƒ³', 'syosetu', 'å°èª¬']):
                self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ãƒšãƒ¼ã‚¸ã§ã¯ãªã„å¯èƒ½æ€§", "WARNING")
                return False
        
        self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: æ­£å¸¸ãªãƒšãƒ¼ã‚¸")
        return True
            
    def download_resource(self, url, base_path):
        """ãƒªã‚½ãƒ¼ã‚¹ï¼ˆç”»åƒã€CSSã€JSç­‰ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            # çµ¶å¯¾URLã«å¤‰æ›ï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹åŒ–ï¼‰
            if not url.startswith('http'):
                if url.startswith('./resources/'):
                    # ./resources/style.css -> https://img.syosetu.org/css/style.css
                    resource_file = url.replace('./resources/', '')
                    if resource_file.endswith('.css'):
                        url = f"https://img.syosetu.org/css/{resource_file}"
                    elif resource_file.endswith('.js'):
                        url = f"https://img.syosetu.org/js/{resource_file}"
                    else:
                        url = f"https://img.syosetu.org/image/{resource_file}"
                elif url.startswith('./'):
                    # ./banner.png -> https://img.syosetu.org/image/banner.png
                    url = f"https://img.syosetu.org/image/{url[2:]}"
                else:
                    url = urljoin(self.base_url, url)
            
            if url in self.resource_cache:
                cached_filename = self.resource_cache[url]
                cached_path = os.path.join(base_path, cached_filename)
                if os.path.exists(cached_path):
                    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {cached_filename}")
                    return cached_filename
                else:
                    del self.resource_cache[url]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)
            if not filename or '.' not in filename:
                # æ‹¡å¼µå­ãŒãªã„å ´åˆã¯æ¨æ¸¬
                if 'css' in url:
                    filename = f"style_{hash(url) % 10000}.css"
                elif 'js' in url:
                    filename = f"script_{hash(url) % 10000}.js"
                elif any(ext in url for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg']):
                    ext = url.split('.')[-1].split('?')[0]
                    filename = f"image_{hash(url) % 10000}.{ext}"
                else:
                    filename = f"resource_{hash(url) % 10000}.txt"
            
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ
            local_path = os.path.join(base_path, filename)
            
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿è­·ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç ´æé˜²æ­¢ï¼‰
            if os.path.exists(local_path):
                print(f"æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼ˆä¸Šæ›¸ãé˜²æ­¢ï¼‰: {filename}")
                self.resource_cache[url] = filename
                return filename
            
            # ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = self.cloudscraper.get(url, timeout=10)
            response.raise_for_status()
            
            # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è€ƒæ…®
            if filename.endswith('.css'):
                response.encoding = 'utf-8'
                with open(local_path, 'w', encoding='utf-8') as f:
                    f.write(response.text)
            else:
                with open(local_path, 'wb') as f:
                    f.write(response.content)
            
            print(f"ãƒªã‚½ãƒ¼ã‚¹ä¿å­˜: {filename}")
            
            self.resource_cache[url] = filename
            
            return filename
            
        except Exception as e:
            print(f"ãƒªã‚½ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return url  # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®URLã‚’è¿”ã™
            
    def adjust_resource_paths_only(self, soup, output_dir):
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã›ãšã€ãƒ‘ã‚¹èª¿æ•´ã®ã¿å®Ÿè¡Œï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰"""
        resources_dir_name = "resources"
        
        # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®hrefå±æ€§ã‚’èª¿æ•´
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href and href.startswith(('http', '//')):
                filename = os.path.basename(urlparse(href).path)
                if filename:
                    link['href'] = f"./{resources_dir_name}/{filename}"
        
        # JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã®srcå±æ€§ã‚’èª¿æ•´
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src and src.startswith(('http', '//')):
                filename = os.path.basename(urlparse(src).path)
                if filename:
                    script['src'] = f"./{resources_dir_name}/{filename}"
        
        # ç”»åƒã®srcå±æ€§ã‚’èª¿æ•´
        for img in soup.find_all('img', src=True):
            src = img.get('src')
            if src and src.startswith(('http', '//')):
                filename = os.path.basename(urlparse(src).path)
                if filename:
                    img['src'] = f"./{resources_dir_name}/{filename}"
        
        print(f"ğŸ“‚ ãƒ‘ã‚¹èª¿æ•´ã®ã¿å®Œäº†ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return soup
    
    def process_html_resources(self, soup, base_path):
        """HTMLã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‡¦ç†ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ä¿å­˜ï¼‰"""
        # ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ä½¿ç”¨
        resources_dir_name = getattr(self, 'browser_compatible_name', 'resources')
        resources_dir = os.path.join(base_path, resources_dir_name)
        os.makedirs(resources_dir, exist_ok=True)
        
        print("=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ãƒªã‚½ãƒ¼ã‚¹ä¿å­˜é–‹å§‹ ===")
        
        # 1. CSS ãƒªãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆã‚¹ã‚¿ã‚¤ãƒ«ã‚·ãƒ¼ãƒˆï¼‰
        css_links = soup.find_all('link', {'rel': 'stylesheet'})
        print(f"CSS ãƒªãƒ³ã‚¯æ•°: {len(css_links)}")
        for link in css_links:
            href = link.get('href')
            if href:
                print(f"CSSå‡¦ç†ä¸­: {href}")
                local_file = self.download_and_process_css(href, resources_dir)
                link['href'] = f"./{resources_dir_name}/{local_file}"
        
        # 2. ã™ã¹ã¦ã®å¤–éƒ¨ãƒªãƒ³ã‚¯ãƒªã‚½ãƒ¼ã‚¹ï¼ˆãƒ•ã‚¡ãƒ“ã‚³ãƒ³ã€ã‚¢ã‚¤ã‚³ãƒ³ç­‰ï¼‰
        icon_rels = ['icon', 'shortcut icon', 'apple-touch-icon', 'apple-touch-icon-precomposed', 
                     'mask-icon', 'fluid-icon']
        icon_links = soup.find_all('link', {'rel': icon_rels})
        print(f"ã‚¢ã‚¤ã‚³ãƒ³ãƒªãƒ³ã‚¯æ•°: {len(icon_links)}")
        for link in icon_links:
            href = link.get('href')
            if href:
                print(f"ã‚¢ã‚¤ã‚³ãƒ³å‡¦ç†ä¸­: {href}")
                local_file = self.download_resource(href, resources_dir)
                if local_file != href:
                    link['href'] = f"./{resources_dir_name}/{local_file}"
        
        # 3. JavaScript ãƒ•ã‚¡ã‚¤ãƒ«
        scripts = soup.find_all('script', {'src': True})
        print(f"JavaScript ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(scripts)}")
        for script in scripts:
            src = script.get('src')
            if src:
                print(f"JSå‡¦ç†ä¸­: {src}")
                local_file = self.download_resource(src, resources_dir)
                if local_file != src:
                    script['src'] = f"./{resources_dir_name}/{local_file}"
        
        # 4. å…¨ã¦ã®ç”»åƒï¼ˆimg ã‚¿ã‚°ï¼‰
        images = soup.find_all('img', {'src': True})
        print(f"ç”»åƒæ•°: {len(images)}")
        for img in images:
            src = img.get('src')
            if src:
                print(f"ç”»åƒå‡¦ç†ä¸­: {src}")
                local_file = self.download_resource(src, resources_dir)
                if local_file != src:
                    img['src'] = f"./{resources_dir_name}/{local_file}"
        
        # 5. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ã®èƒŒæ™¯ç”»åƒ
        styled_elements = soup.find_all(style=True)
        print(f"ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«è¦ç´ æ•°: {len(styled_elements)}")
        for element in styled_elements:
            style = element.get('style')
            if 'url(' in style:
                import re
                urls = re.findall(r'url\([\'"]?([^\'"]+)[\'"]?\)', style)
                for url in urls:
                    print(f"ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ç”»åƒå‡¦ç†ä¸­: {url}")
                    local_file = self.download_resource(url, resources_dir)
                    if local_file != url:
                        style = style.replace(url, f"./{resources_dir_name}/{local_file}")
                element['style'] = style
        
        # 6. ãã®ä»–ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚½ãƒ¼ã‚¹ï¼ˆvideo, audio, embed, objectï¼‰
        media_tags = ['video', 'audio', 'embed', 'object', 'source']
        for tag_name in media_tags:
            elements = soup.find_all(tag_name)
            for element in elements:
                # src, data, href å±æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                for attr in ['src', 'data', 'href', 'poster']:
                    url = element.get(attr)
                    if url and url.startswith(('http', '//')):
                        print(f"{tag_name}ã®ãƒªã‚½ãƒ¼ã‚¹å‡¦ç†ä¸­: {url}")
                        local_file = self.download_resource(url, resources_dir)
                        if local_file != url:
                            element[attr] = f"./{resources_dir_name}/{local_file}"
        
        # 7. CSSå†…ã®@importæ–‡ã‚„font-faceç­‰ã®å‡¦ç†ã‚’å¼·åŒ–
        for style_tag in soup.find_all('style'):
            if style_tag.string:
                css_content = style_tag.string
                # @importæ–‡ã‚’å‡¦ç†
                import re
                imports = re.findall(r'@import\s+[\'"]([^\'"]+)[\'"]', css_content)
                for import_url in imports:
                    print(f"CSS @importå‡¦ç†ä¸­: {import_url}")
                    local_file = self.download_and_process_css(import_url, resources_dir)
                    css_content = css_content.replace(import_url, f"./{resources_dir_name}/{local_file}")
                
                # url()å‚ç…§ã‚’å‡¦ç†ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ãƒ»çµ¶å¯¾ãƒ‘ã‚¹ä¸¡æ–¹ã«å¯¾å¿œï¼‰
                urls = re.findall(r'url\([\'"]?([^\'"]+)[\'"]?\)', css_content)
                for url in urls:
                    # çµ¶å¯¾URLãƒ»ç›¸å¯¾URLãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã™ã¹ã¦ã‚’å‡¦ç†
                    if url.startswith(('http', '//', '/')):
                        # çµ¶å¯¾URLã®å ´åˆ
                        print(f"CSSå†…ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {self.base_url if url.startswith('/') and not url.startswith('//') else 'https:' if url.startswith('//') else ''}{url}")
                        local_file = self.download_resource(url, resources_dir)
                        if local_file != url:
                            # CSSå†…ã®ç›¸å¯¾ãƒ‘ã‚¹ãƒ»çµ¶å¯¾ãƒ‘ã‚¹ä¸¡æ–¹ã«å¯¾å¿œ
                            css_content = css_content.replace(f"url({url})", f"url({local_file})")
                            css_content = css_content.replace(f"url('{url}')", f"url('{local_file}')")
                            css_content = css_content.replace(f'url("{url}")', f'url("{local_file}")')
                    elif url and not url.startswith(('data:', '#', './')):
                        # ç›¸å¯¾ãƒ•ã‚¡ã‚¤ãƒ«åã®å ´åˆï¼ˆbanner.pngç­‰ï¼‰- æ—¢ã«ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿
                        full_url = f"https://img.syosetu.org/image/{url}"
                        print(f"CSSå†…ç›¸å¯¾ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {full_url}")
                        local_file = self.download_resource(full_url, resources_dir)
                        if local_file and local_file != url:
                            # ç›¸å¯¾ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç½®æ›
                            css_content = css_content.replace(f"url({url})", f"url({local_file})")
                            css_content = css_content.replace(f"url('{url}')", f"url('{local_file}')")
                            css_content = css_content.replace(f'url("{url}")', f'url("{local_file}")')
                            print(f"CSSå†…ç”»åƒãƒ‘ã‚¹æ›´æ–°: {url} -> {local_file}")
                
                style_tag.string = css_content
        
        # 8. data-srcå±æ€§ï¼ˆé…å»¶èª­ã¿è¾¼ã¿ç”»åƒï¼‰ã‚‚å‡¦ç†
        lazy_images = soup.find_all(attrs={'data-src': True})
        print(f"é…å»¶èª­ã¿è¾¼ã¿ç”»åƒæ•°: {len(lazy_images)}")
        for img in lazy_images:
            data_src = img.get('data-src')
            if data_src:
                print(f"é…å»¶èª­ã¿è¾¼ã¿ç”»åƒå‡¦ç†ä¸­: {data_src}")
                local_file = self.download_resource(data_src, resources_dir)
                if local_file != data_src:
                    img['data-src'] = f"./{resources_dir_name}/{local_file}"
                    # srcsetã‚„data-srcsetå±æ€§ã‚‚å­˜åœ¨ã™ã‚‹å ´åˆã¯å‡¦ç†
                    if img.get('data-srcset'):
                        img['data-srcset'] = f"./{resources_dir_name}/{local_file}"
                    # é…å»¶èª­ã¿è¾¼ã¿ç”»åƒã‚’srcã«ã‚‚è¨­å®š
                    if not img.get('src'):
                        img['src'] = f"./{resources_dir_name}/{local_file}"
        
        # 9. ãã®ä»–ã®ç”»åƒå±æ€§ã‚‚å‡¦ç†ï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®å±æ€§ï¼‰
        for attr in ['data-original', 'data-lazy-src', 'data-echo']:
            attr_images = soup.find_all(attrs={attr: True})
            print(f"{attr}å±æ€§ç”»åƒæ•°: {len(attr_images)}")
            for img in attr_images:
                img_src = img.get(attr)
                if img_src:
                    print(f"{attr}å±æ€§ç”»åƒå‡¦ç†ä¸­: {img_src}")
                    local_file = self.download_resource(img_src, resources_dir)
                    if local_file != img_src:
                        img[attr] = f"./{resources_dir_name}/{local_file}"
        
        print("=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ãƒªã‚½ãƒ¼ã‚¹ä¿å­˜å®Œäº† ===")
        return soup
    
    def fix_local_navigation_links(self, soup, chapter_mapping, current_chapter_url, index_file = None, novel_info_file = None, comments_file = None):
        """ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã«ä¿®æ­£ï¼ˆå¼·åŒ–ç‰ˆ + å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³å¯¾å¿œï¼‰"""
        print("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç”¨ã«ä¿®æ­£ä¸­...")
        
        # 1. ç›®æ¬¡ãƒªãƒ³ã‚¯ã®ä¿®æ­£ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
        index_patterns = [
            # åŸºæœ¬çš„ãªç›®æ¬¡URLãƒ‘ã‚¿ãƒ¼ãƒ³
            lambda tag: tag.name == 'a' and tag.get('href') and '/novel/' in tag.get('href') and tag.get('href').endswith('/'),
            # å®Œå…¨URLå½¢å¼
            lambda tag: tag.name == 'a' and tag.get('href') == 'https://syosetu.org/novel/378070/',
            # ç›®æ¬¡ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒªãƒ³ã‚¯
            lambda tag: tag.name == 'a' and 'ç›®æ¬¡' in tag.get_text() and 'syosetu.org' in str(tag.get('href', ''))
        ]
        
        if index_file:
            for pattern in index_patterns:
                links = soup.find_all(pattern)
                for link in links:
                    old_href = link.get('href')
                    if old_href and old_href != index_file:
                        link['href'] = index_file
                        print(f"ç›®æ¬¡ãƒªãƒ³ã‚¯ä¿®æ­£: {old_href} -> {index_file}")
        
        # 2. ç« é–“ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã®ä¿®æ­£
        # å…¨ã¦ã®aã‚¿ã‚°ã‚’èª¿æŸ»ã—ã¦ã€ç« URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href')
            if not href:
                continue
            
            # çµ¶å¯¾URLã«æ­£è¦åŒ–
            normalized_href = href
            if href.startswith('/'):
                normalized_href = self.base_url + href
            elif href.startswith('./') and href.endswith('.html'):
                # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã‚’çµ¶å¯¾URLã«å¤‰æ›
                relative_part = href[2:]
                try:
                    novel_id = current_chapter_url.split('/')[-2]
                    normalized_href = f"{self.base_url}/novel/{novel_id}/{relative_part}"
                except:
                    continue
            elif not href.startswith('http') and not (href.endswith('.html') and any(char in href for char in ['ç¬¬', 'è©±'])):
                continue
            
            # ãƒãƒ£ãƒ—ã‚¿ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if normalized_href in chapter_mapping:
                local_file = chapter_mapping[normalized_href]
                if href != local_file:
                    link['href'] = local_file
                    print(f"ç« ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {local_file}")
            
            # çŸ­ç¸®å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚‚å‡¦ç†ï¼ˆç¬¬1è©±.htmlç­‰ï¼‰
            elif href.endswith('.html') and any(char in href for char in ['ç¬¬', 'è©±']):
                print(f"çŸ­ç¸®å½¢å¼ãƒªãƒ³ã‚¯å€™è£œæ¤œå‡º: {href}")
                # çŸ­ç¸®å½¢å¼ã‹ã‚‰ç« ç•ªå·ã‚’æŠ½å‡ºã—ã¦ãƒãƒƒãƒãƒ³ã‚°
                import re
                chapter_match = re.search(r'ç¬¬(\d+)è©±', href)
                if chapter_match:
                    chapter_num = chapter_match.group(1)
                    print(f"çŸ­ç¸®ãƒªãƒ³ã‚¯æ¤œå‡º: {href}, ç« ç•ªå·: {chapter_num}")
                    
                    # chapter_mappingã‹ã‚‰ãƒãƒƒãƒã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¢ã™
                    found_mapping = False
                    for url, filename in chapter_mapping.items():
                        # URLã‹ã‚‰ç« ç•ªå·ã‚’æŠ½å‡º
                        url_match = re.search(r'/(\d+)\.html', url)
                        if url_match and url_match.group(1) == chapter_num:
                            print(f"ãƒãƒƒãƒ”ãƒ³ã‚°ç™ºè¦‹: URL={url}, ç« ç•ªå·={url_match.group(1)}, ãƒ•ã‚¡ã‚¤ãƒ«å={filename}")
                            link['href'] = filename
                            print(f"çŸ­ç¸®ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {filename}")
                            found_mapping = True
                            break
                    
                    if not found_mapping:
                        # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã¯ç„¡åŠ¹åŒ–
                        print(f"å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯: {href} -> ç„¡åŠ¹åŒ–")
                        link['href'] = 'javascript:void(0);'
                        link['class'] = link.get('class', []) + ['disabled']
                        link['style'] = 'color: #999; cursor: not-allowed; text-decoration: none;'
        
        # 3. ç©ºãƒªãƒ³ã‚¯ã‚„ç„¡åŠ¹ãªãƒªãƒ³ã‚¯ã®å‡¦ç†
        empty_links = soup.find_all('a', href='#')
        for link in empty_links:
            link_text = link.get_text().strip()
            if link_text == 'Ã—':
                # ã€ŒÃ—ã€ãƒªãƒ³ã‚¯ã¯ãã®ã¾ã¾ä¿æŒï¼ˆå…ƒã®çŠ¶æ…‹ã‚’ç¶­æŒï¼‰
                print(f"Ã—ãƒªãƒ³ã‚¯ä¿æŒ: {link_text}")
            elif 'æ¬¡ã®è©±' in link_text:
                # ç„¡åŠ¹ãªã€Œæ¬¡ã®è©±ã€ãƒªãƒ³ã‚¯ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯å¤‰æ›´ã—ãªã„ï¼‰
                link['href'] = 'javascript:void(0);'
                link['class'] = link.get('class', []) + ['disabled']
                link['style'] = 'color: #999; cursor: not-allowed; text-decoration: none;'
                print(f"ç„¡åŠ¹ãªæ¬¡ã®è©±ãƒªãƒ³ã‚¯ä¿®æ­£: {link_text} -> ç„¡åŠ¹åŒ–")
        
        # 4. ğŸ†• å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ä¿®æ­£
        if novel_info_file or comments_file:
            info_and_comment_links = soup.find_all('a', href=True)
            for link in info_and_comment_links:
                href = link.get('href')
                if not href:
                    continue
                
                # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ä¿®æ­£
                if novel_info_file and ('mode=ss_detail' in href or 'å°èª¬æƒ…å ±' in link.get_text()):
                    link['href'] = novel_info_file
                    print(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {novel_info_file}")
                
                # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ä¿®æ­£
                elif comments_file and ('mode=review' in href or 'æ„Ÿæƒ³' in link.get_text()):
                    # æ„Ÿæƒ³ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã«ä¿®æ­£
                    link['href'] = 'æ„Ÿæƒ³/æ„Ÿæƒ³ - ãƒšãƒ¼ã‚¸1.html'
                    print(f"æ„Ÿæƒ³ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> æ„Ÿæƒ³/æ„Ÿæƒ³ - ãƒšãƒ¼ã‚¸1.html")
        
        return soup
    
    def download_and_process_css(self, url, resources_dir):
        """CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…éƒ¨ã®ç”»åƒå‚ç…§ã‚‚å‡¦ç†ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        try:
            # çµ¶å¯¾URLã«å¤‰æ›
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)
            if not filename or '.' not in filename:
                filename = f"style_{hash(url) % 10000}.css"
            
            print(f"CSSè©³ç´°å‡¦ç†ä¸­: {url}")
            
            # CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = self.cloudscraper.get(url, timeout=10)
            response.raise_for_status()
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«è¨­å®š
            response.encoding = 'utf-8'
            css_content = response.text
            
            # CSSå†…ã®ã™ã¹ã¦ã®ãƒªã‚½ãƒ¼ã‚¹å‚ç…§ã‚’å‡¦ç†
            import re
            
            # 1. url()å‚ç…§ã‚’å‡¦ç†ï¼ˆèƒŒæ™¯ç”»åƒã€ãƒ•ã‚©ãƒ³ãƒˆç­‰ï¼‰- å®Œå…¨ãªãƒãƒƒãƒãƒ³ã‚°ã§å‡¦ç†
            import re
            
            def replace_url_func(match):
                full_match = match.group(0)  # url(...) å…¨ä½“
                img_url = match.group(1)     # URLéƒ¨åˆ†ã®ã¿
                
                if img_url.startswith('data:'):
                    return full_match  # ãƒ‡ãƒ¼ã‚¿URLã¯ãã®ã¾ã¾
                
                original_img_url = img_url
                
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                if not img_url.startswith('http'):
                    if img_url.startswith('//'):
                        img_url = 'https:' + img_url
                    elif img_url.startswith('/'):
                        base_domain = '/'.join(url.split('/')[:3])
                        img_url = base_domain + img_url
                    else:
                        # ç›¸å¯¾ãƒ‘ã‚¹
                        base_css_url = '/'.join(url.split('/')[:-1])
                        img_url = urljoin(base_css_url + '/', img_url)
                
                # URLæ­£è¦åŒ–
                cleaned_url = img_url.split(')')[0]
                if '?' in cleaned_url:
                    cleaned_url = cleaned_url.split('?')[0]
                
                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                print(f"CSSå†…ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {cleaned_url}")
                local_img = self.download_resource(cleaned_url, resources_dir)
                if local_img != cleaned_url:
                    browser_compatible_path = f"./{local_img}"
                    print(f"CSSå†…ãƒ‘ã‚¹ç½®æ›: {original_img_url} -> {browser_compatible_path}")
                    return full_match.replace(original_img_url, browser_compatible_path)
                else:
                    return full_match
            
            # æ­£è¦è¡¨ç¾ã§ url() ã‚’æ¤œå‡ºã—ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§ç½®æ›
            css_content = re.sub(r'url\([\'"]?([^\'"]+?)[\'"]?\)', replace_url_func, css_content)
            
            # 2. @importæ–‡ã‚’å‡¦ç†
            imports = re.findall(r'@import\s+[\'"]([^\'"]+)[\'"]', css_content)
            for import_url in imports:
                if not import_url.startswith('http'):
                    if import_url.startswith('//'):
                        import_url = 'https:' + import_url
                    elif import_url.startswith('/'):
                        base_domain = '/'.join(url.split('/')[:3])
                        import_url = base_domain + import_url
                    else:
                        base_css_url = '/'.join(url.split('/')[:-1])
                        import_url = urljoin(base_css_url + '/', import_url)
                
                print(f"CSS @importå‡¦ç†: {import_url}")
                local_css = self.download_and_process_css(import_url, resources_dir)
                if local_css != import_url:
                    # @importæ–‡ã§æ­£ç¢ºã«ç½®æ›
                    browser_compatible_css = f"./{local_css}"
                    css_content = css_content.replace(f'@import "{import_url}"', f'@import "{browser_compatible_css}"')
                    css_content = css_content.replace(f"@import '{import_url}'", f"@import '{browser_compatible_css}'")
            
            # 3. @font-faceå†…ã®srcå‚ç…§ã‚‚å‡¦ç†ï¼ˆä¸Šè¨˜ã®url()å‡¦ç†ã§æ—¢ã«å‡¦ç†æ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            # ãƒ•ã‚©ãƒ³ãƒˆã¯æ—¢ã«ä¸Šè¨˜ã®url()å‡¦ç†ã§å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€é‡è¤‡å‡¦ç†ã‚’é¿ã‘ã‚‹
            
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ
            local_path = os.path.join(resources_dir, filename)
            with open(local_path, 'w', encoding='utf-8') as f:
                f.write(css_content)
            
            print(f"CSSå‡¦ç†å®Œäº†: {filename}")
            return filename
            
        except Exception as e:
            print(f"CSSå‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return self.download_resource(url, resources_dir)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
    def extract_novel_info(self, soup):
        """å°èª¬ã®åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡ºï¼ˆ2024å¹´ç‰ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³å¯¾å¿œï¼‰"""
        info = {}
        
        self.debug_log("å°èª¬æƒ…å ±æŠ½å‡ºé–‹å§‹")
        
        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆGeminiåˆ†æã«ã‚ˆã‚‹ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹åŒ–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼‰
        title_selectors = [
            # â˜… Geminiç™ºè¦‹ï¼šãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®æ•°å­—ã‚¯ãƒ©ã‚¹æ§‹é€ 
            ('div', {'class': 'section1'}),  # ã‚¿ã‚¤ãƒˆãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³å€™è£œ
            ('div', {'class': 'section2'}),  # ã‚¿ã‚¤ãƒˆãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³å€™è£œ
            ('h1', {'class': lambda x: x and any(cls.startswith('section') for cls in x if isinstance(cls, str))}),
            # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç”¨
            ('h1', {'class': 'p-novel-title'}),
            ('h1', {'class': 'novel-title'}),
            ('div', {'class': 'p-novel-title'}),
            ('span', {'class': 'novel-title'}),
            # å¾“æ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('h1', {'class': 'title'}),
            ('h1', {'class': 'novel_title'}),
            ('div', {'class': 'novel_title'}),
            ('h1', {}),
            ('title', {})
        ]
        
        self.debug_log("ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºè©¦è¡Œä¸­...")
        for tag, attrs in title_selectors:
            self.debug_log(f"ã‚¿ã‚¤ãƒˆãƒ«ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            title_elem = soup.find(tag, attrs) if attrs else soup.find(tag)
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®å ´åˆã€ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä½™åˆ†ãªæ–‡å­—ã‚’é™¤å»
                if ' - ãƒãƒ¼ãƒ¡ãƒ«ãƒ³' in title_text:
                    title_text = title_text.replace(' - ãƒãƒ¼ãƒ¡ãƒ«ãƒ³', '')
                if title_text and title_text not in ['Unknown Title', '']:
                    info['title'] = title_text
                    self.debug_log(f"ã‚¿ã‚¤ãƒˆãƒ«å–å¾—æˆåŠŸ: {title_text}")
                    break
            else:
                self.debug_log(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ {tag} {attrs} ã§è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # ä½œè€…æŠ½å‡ºï¼ˆå¹…åºƒã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼‰
        author_selectors = [
            # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç”¨
            ('a', {'class': 'p-novel-author'}),
            ('span', {'class': 'p-novel-author'}),
            ('div', {'class': 'novel-author'}),
            ('a', {'class': 'author-link'}),
            # å¾“æ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('a', {'href': lambda x: x and '/user/' in x}),
            ('div', {'class': 'author'}),
            ('span', {'class': 'author'}),
            ('a', {'class': 'author'})
        ]
        
        self.debug_log("ä½œè€…æŠ½å‡ºè©¦è¡Œä¸­...")
        for tag, attrs in author_selectors:
            self.debug_log(f"ä½œè€…ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            if callable(attrs.get('href')):
                author_elem = soup.find(tag, href=attrs['href'])
            else:
                author_elem = soup.find(tag, attrs)
            
            if author_elem:
                author_text = author_elem.get_text(strip=True)
                if author_text and author_text not in ['Unknown Author', '']:
                    info['author'] = author_text
                    self.debug_log(f"ä½œè€…å–å¾—æˆåŠŸ: {author_text}")
                    break
            else:
                self.debug_log(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ {tag} {attrs} ã§è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # æƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã®è©³ç´°èª¿æŸ»
        if not info.get('title') or not info.get('author'):
            self.debug_log("åŸºæœ¬æƒ…å ±å–å¾—å¤±æ•—ã€è©³ç´°èª¿æŸ»ã‚’å®Ÿè¡Œ", "WARNING")
            self.investigate_page_structure(soup)
        
        return info
        
    def investigate_page_structure(self, soup):
        """ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’è©³ç´°èª¿æŸ»"""
        self.debug_log("=== ãƒšãƒ¼ã‚¸æ§‹é€ è©³ç´°èª¿æŸ» ===")
        
        # åˆ©ç”¨å¯èƒ½ãªh1ã‚¿ã‚°
        h1_tags = soup.find_all('h1')
        self.debug_log(f"h1ã‚¿ã‚°æ•°: {len(h1_tags)}")
        for i, h1 in enumerate(h1_tags[:5]):
            classes = h1.get('class', [])
            text = h1.get_text(strip=True)[:50]
            self.debug_log(f"  h1[{i}]: class={classes}, text={text}...")
        
        # åˆ©ç”¨å¯èƒ½ãªãƒªãƒ³ã‚¯
        links = soup.find_all('a', href=True)
        self.debug_log(f"ãƒªãƒ³ã‚¯æ•°: {len(links)}")
        user_links = [link for link in links if '/user/' in link.get('href', '')]
        self.debug_log(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªãƒ³ã‚¯æ•°: {len(user_links)}")
        for i, link in enumerate(user_links[:3]):
            classes = link.get('class', [])
            text = link.get_text(strip=True)[:30]
            href = link.get('href')
            self.debug_log(f"  userlink[{i}]: class={classes}, text={text}..., href={href}")
        
        # ç‰¹æ®Šãªã‚¯ãƒ©ã‚¹åã‚’æ¤œç´¢
        divs_with_class = soup.find_all('div', class_=True)
        unique_classes = set()
        for div in divs_with_class:
            for cls in div.get('class', []):
                if any(keyword in cls.lower() for keyword in ['title', 'author', 'novel', 'name']):
                    unique_classes.add(cls)
        
        self.debug_log(f"é–¢é€£ã‚¯ãƒ©ã‚¹å: {sorted(unique_classes)}")
        
        # spanã‚¿ã‚°ã‚‚ãƒã‚§ãƒƒã‚¯
        spans_with_class = soup.find_all('span', class_=True)
        span_classes = set()
        for span in spans_with_class:
            for cls in span.get('class', []):
                if any(keyword in cls.lower() for keyword in ['title', 'author', 'novel', 'name']):
                    span_classes.add(cls)
        
        self.debug_log(f"é–¢é€£spanã‚¯ãƒ©ã‚¹å: {sorted(span_classes)}")
        
    def extract_novel_info_url(self, soup):
        """ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡º"""
        try:
            # topicPathã‹ã‚‰å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            topic_path = soup.find('ol', class_='topicPath')
            if topic_path:
                info_link = topic_path.find('a', href=lambda x: x and 'mode=ss_detail' in x)
                if info_link:
                    href = info_link.get('href')
                    if href.startswith('?'):
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https://syosetu.org/{href}"
                    elif href.startswith('//'):
                        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ç›¸å¯¾URLã‚’HTTPSçµ¶å¯¾URLã«å¤‰æ›
                        return f"https:{href}"
                    elif href.startswith('/'):
                        # ãƒ«ãƒ¼ãƒˆç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https://syosetu.org{href}"
                    return href
            
            self.debug_log("å°èª¬æƒ…å ±URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
            return None
        except Exception as e:
            self.debug_log(f"å°èª¬æƒ…å ±URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def extract_comments_url(self, soup):
        """ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡º"""
        try:
            # topicPathã‹ã‚‰æ„Ÿæƒ³ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
            topic_path_selectors = [
                'ol.topicPath',
                'div.topicPath',
                '.topicPath'
            ]
            
            topic_path = None
            for selector in topic_path_selectors:
                topic_path = soup.select_one(selector)
                if topic_path:
                    break
            
            if topic_path:
                comments_link = topic_path.find('a', href=lambda x: x and 'mode=review' in x)
                if comments_link:
                    href = comments_link.get('href')
                    if href.startswith('?'):
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https://syosetu.org/{href}"
                    elif href.startswith('//'):
                        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https:{href}"
                    elif href.startswith('/'):
                        # ãƒ‘ã‚¹ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https://syosetu.org{href}"
                    return href
            
            self.debug_log("æ„Ÿæƒ³URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
            return None
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def save_novel_info_page(self, info_url, output_dir, novel_title, index_file_name=None, comments_file_name=None, vertical_file_name=None):
        """å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—ãƒ»ä¿å­˜"""
        try:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {info_url}")
            
            # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            info_soup = self.get_page(info_url)
            if not info_soup:
                self.debug_log("å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
            
            # ä¿å­˜å‰ã«å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ä¿®æ­£
            info_soup = self.fix_novel_info_page_links(info_soup, index_file_name, comments_file_name, vertical_file_name)
            
            # ä¿å­˜å‡¦ç†
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
            info_filename = f"{safe_title} - å°èª¬æƒ…å ±"
            
            info_file_path = self.save_complete_page(
                info_soup,
                info_url,
                info_filename,
                output_dir,
                info_url
            )
            
            if info_file_path:
                self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {os.path.basename(info_file_path)}")
                return info_file_path
            else:
                self.debug_log("å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
                
        except Exception as e:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def fix_novel_info_page_links(self, soup, index_file_name=None, comments_file_name=None, vertical_file_name=None):
        """å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£"""
        try:
            # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if not href:
                    continue
                
                # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                if 'mode=review' in href:
                    # æ„Ÿæƒ³ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã«ä¿®æ­£
                    link['href'] = 'æ„Ÿæƒ³/æ„Ÿæƒ³ - ãƒšãƒ¼ã‚¸1.html'
                    self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸æ„Ÿæƒ³ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> æ„Ÿæƒ³/æ„Ÿæƒ³ - ãƒšãƒ¼ã‚¸1.html")
                
                # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                elif '/novel/' in href and href.endswith('/'):
                    if index_file_name:
                        link['href'] = index_file_name
                        self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ç›®æ¬¡ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {index_file_name}")
                    else:
                        link['href'] = 'ç›®æ¬¡.html'
                        self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ç›®æ¬¡ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> ç›®æ¬¡.html")
                
                # å°èª¬æœ¬æ–‡ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ï¼ˆç« ãƒšãƒ¼ã‚¸ï¼‰
                elif '/novel/' in href and re.search(r'/\d+\.html', href):
                    # ç« ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã¯ç›¸å¯¾ãƒ‘ã‚¹ã§ä¿®æ­£
                    chapter_num = re.search(r'/(\d+)\.html', href)
                    if chapter_num:
                        chapter_title = f"ç¬¬{chapter_num.group(1)}è©±"
                        link['href'] = f"{chapter_title}.html"
                        self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ç« ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {chapter_title}.html")
                
                # ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                elif 'mode=ss_detail3' in href and vertical_file_name:
                    link['href'] = vertical_file_name
                    self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ç¸¦æ›¸ããƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {vertical_file_name}")
            
            return soup
            
        except Exception as e:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return soup

    def save_comments_page(self, comments_url, output_dir, novel_title, index_file_name=None, info_file_name=None):
        """ğŸ†• æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’å„ãƒšãƒ¼ã‚¸å€‹åˆ¥ã«ä¿å­˜ï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³å…ƒæ§‹é€ ä¿æŒï¼‰"""
        try:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {comments_url}")
            
            # æ„Ÿæƒ³ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
            comments_dir = os.path.join(output_dir, "æ„Ÿæƒ³")
            os.makedirs(comments_dir, exist_ok=True)
            self.debug_log(f"æ„Ÿæƒ³ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {comments_dir}")
            
            # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º
            first_page_soup = self.get_page(comments_url)
            if not first_page_soup:
                self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
            page_links = self.detect_comments_pagination(first_page_soup, comments_url)
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸æ•°: {len(page_links)}ãƒšãƒ¼ã‚¸")
            
            saved_files = []
            
            # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«ä¿å­˜
            for page_num, page_url in enumerate(page_links, 1):
                self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ {page_num}/{len(page_links)} ã‚’ä¿å­˜ä¸­: {page_url}")
                
                # ãƒšãƒ¼ã‚¸ã‚’å–å¾—
                if page_num == 1:
                    page_soup = first_page_soup
                else:
                    time.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                    page_soup = self.get_page(page_url)
                    if not page_soup:
                        self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ {page_num} ã®å–å¾—ã«å¤±æ•—", "WARNING")
                        continue
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
                comments_filename = f"æ„Ÿæƒ³ - ãƒšãƒ¼ã‚¸{page_num}"
                
                # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
                page_file_path = self.save_complete_page(
                    page_soup,
                    page_url,
                    comments_filename,
                    comments_dir,
                    page_url
                )
                
                if page_file_path:
                    saved_files.append(page_file_path)
                    self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸{page_num}ä¿å­˜å®Œäº†: {os.path.basename(page_file_path)}")
            
            if saved_files:
                # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸é–“ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸é–“ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
                self.fix_comments_page_links(saved_files, page_links, index_file_name, info_file_name)
                self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {len(saved_files)}ãƒšãƒ¼ã‚¸ä¿å­˜")
                return saved_files[0]  # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ãƒ‘ã‚¹ã‚’è¿”ã™ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            else:
                self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
                
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def fix_comments_page_links(self, saved_files, page_urls, index_file_name=None, info_file_name=None):
        """æ„Ÿæƒ³ãƒšãƒ¼ã‚¸é–“ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£"""
        try:
            # ãƒšãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«åã¨URLã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            page_mapping = {}
            for i, (file_path, url) in enumerate(zip(saved_files, page_urls), 1):
                page_mapping[url] = os.path.basename(file_path)
            
            # å„ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
            for i, file_path in enumerate(saved_files):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                
                # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    if not href:
                        continue
                    
                    # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®URLå½¢å¼ã‚’æ¤œå‡ºï¼ˆp=ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å«ã‚€ã€HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚‚è€ƒæ…®ï¼‰
                    is_comments_page = (
                        'mode=review' in href or 
                        ('p=' in href and '/novel/' in href) or
                        ('&amp;p=' in href and '/novel/' in href)
                    )
                    
                    if is_comments_page:
                        # ã‚ˆã‚Šç²¾å¯†ãªãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè£…
                        matched_file = self.find_matching_comments_page(href, page_mapping)
                        if matched_file:
                            link['href'] = matched_file
                            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {matched_file}")
                    
                    # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                    elif ('/novel/' in href and href.endswith('/')) or 'ç›®æ¬¡' in link.get_text():
                        # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç›®æ¬¡ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã«ä¿®æ­£
                        if index_file_name:
                            link['href'] = f'../{index_file_name}'
                            self.debug_log(f"ç›®æ¬¡ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> ../{index_file_name}")
                        else:
                            link['href'] = '../ç›®æ¬¡.html'
                            self.debug_log(f"ç›®æ¬¡ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> ../ç›®æ¬¡.html")
                    
                    # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                    elif 'mode=ss_detail' in href or 'å°èª¬æƒ…å ±' in link.get_text():
                        # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã«ä¿®æ­£
                        if info_file_name:
                            link['href'] = f'../{info_file_name}'
                            self.debug_log(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> ../{info_file_name}")
                        else:
                            link['href'] = '../å°èª¬æƒ…å ±.html'
                            self.debug_log(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> ../å°èª¬æƒ…å ±.html")
                
                # ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¿å­˜
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
    
    def find_matching_comments_page(self, href, page_mapping):
        """æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®URLã‹ã‚‰æ­£ç¢ºãªãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™"""
        try:
            # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è§£é™¤
            import html
            href = html.unescape(href)
            
            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿®æ­£
            if '?' not in href and ('&p=' in href or '&page=' in href):
                href = href.replace('&', '?', 1)
            
            # ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
            target_page_num = self.extract_page_number(href)
            
            # åŸºæœ¬URLï¼ˆnidéƒ¨åˆ†ï¼‰ã‚’æŠ½å‡º
            target_base_url = self.extract_base_comments_url(href)
            
            # ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for original_url, local_file in page_mapping.items():
                original_page_num = self.extract_page_number(original_url)
                original_base_url = self.extract_base_comments_url(original_url)
                
                # åŸºæœ¬URLã¨ãƒšãƒ¼ã‚¸ç•ªå·ãŒä¸€è‡´ã™ã‚‹å ´åˆ
                if (target_base_url == original_base_url and 
                    target_page_num == original_page_num):
                    return local_file
            
            return None
            
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ãƒãƒƒãƒãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def extract_page_number(self, url):
        """URLã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º"""
        try:
            from urllib.parse import urlparse, parse_qs
            
            # URLã‚’ãƒ‘ãƒ¼ã‚¹
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            
            # page ã¾ãŸã¯ p ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å–å¾—
            if 'page' in params:
                return int(params['page'][0])
            elif 'p' in params:
                return int(params['p'][0])
            else:
                return 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ãƒšãƒ¼ã‚¸ç›®
                
        except Exception as e:
            self.debug_log(f"ãƒšãƒ¼ã‚¸ç•ªå·æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "DEBUG")
            return 1
    
    def extract_base_comments_url(self, url):
        """URLã‹ã‚‰åŸºæœ¬URLï¼ˆnidéƒ¨åˆ†ï¼‰ã‚’æŠ½å‡º"""
        try:
            from urllib.parse import urlparse, parse_qs
            
            # URLã‚’ãƒ‘ãƒ¼ã‚¹
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            
            # åŸºæœ¬çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿æŒ
            base_params = {}
            if 'mode' in params:
                base_params['mode'] = params['mode'][0]
            if 'nid' in params:
                base_params['nid'] = params['nid'][0]
            
            # åŸºæœ¬URLã‚’æ§‹ç¯‰
            base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            if base_params:
                param_str = '&'.join([f"{k}={v}" for k, v in base_params.items()])
                base_url += f"?{param_str}"
            
            return base_url
            
        except Exception as e:
            self.debug_log(f"åŸºæœ¬URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return url

    def get_all_comments_pages(self, base_comments_url):
        """ğŸ†• è¤‡æ•°ãƒšãƒ¼ã‚¸ã®æ„Ÿæƒ³ã‚’å…¨ã¦å–å¾—ã—ã¦çµ±åˆ"""
        try:
            self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã‚’é–‹å§‹")
            
            # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            first_page_soup = self.get_page(base_comments_url)
            if not first_page_soup:
                return None
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
            page_links = self.detect_comments_pagination(first_page_soup, base_comments_url)
            
            if len(page_links) <= 1:
                # å˜ä¸€ãƒšãƒ¼ã‚¸ã®å ´åˆ
                self.debug_log("æ„Ÿæƒ³ã¯1ãƒšãƒ¼ã‚¸ã®ã¿ã§ã™")
                return first_page_soup
            
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸æ•°: {len(page_links)}ãƒšãƒ¼ã‚¸")
            
            # å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            all_comments = []
            
            for page_num, page_url in enumerate(page_links, 1):
                self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ {page_num}/{len(page_links)} ã‚’å–å¾—ä¸­: {page_url}")
                
                if page_num == 1:
                    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã¯æ—¢ã«å–å¾—æ¸ˆã¿
                    page_soup = first_page_soup
                else:
                    # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã‚’å–å¾—
                    time.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                    page_soup = self.get_page(page_url)
                    if not page_soup:
                        self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ {page_num} ã®å–å¾—ã«å¤±æ•—", "WARNING")
                        continue
                
                # æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                comments_content = self.extract_comments_content(page_soup)
                if comments_content:
                    all_comments.extend(comments_content)
            
            if not all_comments:
                self.debug_log("æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
                return first_page_soup
            
            # çµ±åˆã•ã‚ŒãŸHTMLã‚’ä½œæˆ
            integrated_soup = self.create_integrated_comments_page(first_page_soup, all_comments, len(page_links))
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆå®Œäº†: {len(all_comments)}ä»¶ã®æ„Ÿæƒ³ã‚’çµ±åˆ")
            
            return integrated_soup
            
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def detect_comments_pagination(self, soup, base_url):
        """ğŸ†• æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            page_links = []
            base_page_num = self.extract_page_number(base_url)
            max_page_num = 1
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
            pagination_selectors = [
                # ä¸€èˆ¬çš„ãªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                'div.pagination a',
                'div.pager a', 
                'div.page-nav a',
                # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                'a[href*="mode=review"]',
                'a[href*="&page="]',
                'a[href*="&p="]'
            ]
            
            all_pagination_links = []
            
            # ã™ã¹ã¦ã®selectorã‹ã‚‰ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’åé›†
            for selector in pagination_selectors:
                pagination_links = soup.select(selector)
                if pagination_links:
                    self.debug_log(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ç™ºè¦‹: {selector} ({len(pagination_links)}å€‹ã®ãƒªãƒ³ã‚¯)")
                    all_pagination_links.extend(pagination_links)
            
            # é‡è¤‡ã‚’é™¤å»
            unique_links = []
            seen_hrefs = set()
            for link in all_pagination_links:
                href = link.get('href')
                if href and href not in seen_hrefs:
                    seen_hrefs.add(href)
                    unique_links.append(link)
            
            # ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒšãƒ¼ã‚¸URLã‚’æŠ½å‡º
            for link in unique_links:
                href = link.get('href')
                link_text = link.get_text(strip=True)
                
                if href and ('page=' in href or 'p=' in href or 'mode=review' in href):
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if href.startswith('?'):
                        # ?page=2 ã¾ãŸã¯ ?p=2 å½¢å¼
                        full_url = base_url.split('?')[0] + href
                    elif href.startswith('./'):
                        # ./?page=2 ã¾ãŸã¯ ./?p=2 å½¢å¼
                        full_url = base_url.split('?')[0] + href[2:]  # ./ ã‚’å‰Šé™¤ã—ã¦å‡¦ç†
                    elif href.startswith('//'):
                        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        full_url = f"https:{href}"
                    elif href.startswith('/'):
                        # /path?page=2 ã¾ãŸã¯ /path?p=2 å½¢å¼
                        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿®æ­£ãŒå¿…è¦ãªå ´åˆ
                        if '?' not in href and ('&p=' in href or '&page=' in href):
                            # &ã‚’æœ€åˆã®?ã«å¤‰æ›
                            href = href.replace('&', '?', 1)
                        full_url = 'https://syosetu.org' + href
                    elif href.startswith('http'):
                        # https://... å½¢å¼
                        full_url = href
                    else:
                        continue
                    
                    # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
                    page_num = self.extract_page_number(full_url)
                    max_page_num = max(max_page_num, page_num)
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ãƒ™ãƒ¼ã‚¹ï¼‰
                    if not any(self.extract_page_number(existing_url) == page_num for existing_url in page_links):
                        page_links.append(full_url)
                        self.debug_log(f"ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯è¿½åŠ : {link_text} -> {full_url} (ãƒšãƒ¼ã‚¸{page_num})")
                
                # æ•°å­—ã®ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ¨å®š
                elif link_text.isdigit():
                    page_num = int(link_text)
                    max_page_num = max(max_page_num, page_num)
                    
                    # ãƒ™ãƒ¼ã‚¹URLã‹ã‚‰ãƒšãƒ¼ã‚¸URLã‚’æ§‹ç¯‰
                    if 'p=' in base_url:
                        constructed_url = base_url.split('p=')[0] + f'p={page_num}'
                    elif 'page=' in base_url:
                        constructed_url = base_url.split('page=')[0] + f'page={page_num}'
                    else:
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼ã§æ§‹ç¯‰
                        if '?' in base_url:
                            constructed_url = base_url + f'&p={page_num}'
                        else:
                            constructed_url = base_url + f'?p={page_num}'
                    
                    if not any(self.extract_page_number(existing_url) == page_num for existing_url in page_links):
                        page_links.append(constructed_url)
                        self.debug_log(f"æ•°å­—ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒšãƒ¼ã‚¸æ§‹ç¯‰: {link_text} -> {constructed_url} (ãƒšãƒ¼ã‚¸{page_num})")
            
            # æœ€å¤§ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ¤œå‡ºã—ãŸå ´åˆã€ä¸è¶³ã—ã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã‚’è£œå®Œ
            if max_page_num > len(page_links):
                self.debug_log(f"æœ€å¤§ãƒšãƒ¼ã‚¸ç•ªå· {max_page_num} ã‚’æ¤œå‡ºã€ä¸è¶³ãƒšãƒ¼ã‚¸ã‚’è£œå®Œä¸­...")
                
                # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèª
                if 'p=' in base_url:
                    param_format = 'p='
                elif 'page=' in base_url:
                    param_format = 'page='
                else:
                    param_format = 'p='
                
                # 1ã‹ã‚‰max_page_numã¾ã§ã®ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
                for page_num in range(1, max_page_num + 1):
                    if not any(self.extract_page_number(existing_url) == page_num for existing_url in page_links):
                        if param_format in base_url:
                            constructed_url = base_url.split(param_format)[0] + f'{param_format}{page_num}'
                        else:
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼ã§æ§‹ç¯‰
                            if '?' in base_url:
                                constructed_url = base_url + f'&{param_format}{page_num}'
                            else:
                                constructed_url = base_url + f'?{param_format}{page_num}'
                        
                        page_links.append(constructed_url)
                        self.debug_log(f"è£œå®Œãƒšãƒ¼ã‚¸è¿½åŠ : ãƒšãƒ¼ã‚¸{page_num} -> {constructed_url}")
            
            # ãƒ™ãƒ¼ã‚¹URLãŒãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
            if not any(self.extract_page_number(url) == base_page_num for url in page_links):
                page_links.append(base_url)
            
            # ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
            page_links.sort(key=lambda url: self.extract_page_number(url))
            
            self.debug_log(f"æ¤œå‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸: {len(page_links)}ãƒšãƒ¼ã‚¸ (æœ€å¤§ãƒšãƒ¼ã‚¸ç•ªå·: {max_page_num})")
            for i, url in enumerate(page_links, 1):
                self.debug_log(f"  ãƒšãƒ¼ã‚¸{i}: {url}")
            
            return page_links
            
        except Exception as e:
            self.debug_log(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return [base_url]

    def extract_comments_content(self, soup):
        """ğŸ†• æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
        try:
            comments = []
            
            # æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®å®Ÿéš›ã®æ§‹é€ ã«å¯¾å¿œï¼‰
            comment_selectors = [
                'div[id*="review"]',     # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®å®Ÿéš›ã®æ§‹é€ : div.review_7612892
                'div.review-item',       # ä¸€èˆ¬çš„ãªæ§‹é€ 
                'div.comment-item', 
                'div.impression',
                'tr[id*="review"]',      # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®æ„Ÿæƒ³
                'div[class*="review_"]', # review_ã§å§‹ã¾ã‚‹ã‚¯ãƒ©ã‚¹
                'div[class*="review"]',
                'div[class*="comment"]'
            ]
            
            for selector in comment_selectors:
                comment_elements = soup.select(selector)
                if comment_elements:
                    self.debug_log(f"æ„Ÿæƒ³è¦ç´ ç™ºè¦‹: {selector} ({len(comment_elements)}ä»¶)")
                    comments.extend(comment_elements)
                    break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‹ã‚‰æ„Ÿæƒ³ã‚’æŠ½å‡º
            if not comments:
                table_rows = soup.find_all('tr')
                for row in table_rows:
                    # æ„Ÿæƒ³ã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¡Œã‚’æ¤œå‡º
                    text = row.get_text().strip()
                    if len(text) > 20 and any(keyword in text for keyword in ['é¢ç™½', 'è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„', 'æ„Ÿå‹•', 'ç¶šã']):
                        comments.append(row)
            
            self.debug_log(f"æŠ½å‡ºã•ã‚ŒãŸæ„Ÿæƒ³: {len(comments)}ä»¶")
            return comments
            
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return []

    def create_integrated_comments_page(self, base_soup, all_comments, total_pages):
        """ğŸ†• çµ±åˆã•ã‚ŒãŸæ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ"""
        try:
            # ãƒ™ãƒ¼ã‚¹HTMLã‚’ã‚³ãƒ”ãƒ¼
            integrated_soup = copy.deepcopy(base_soup)
            
            # æ—¢å­˜ã®æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‰Šé™¤ï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®å®Ÿéš›ã®æ§‹é€ ã«å¯¾å¿œï¼‰
            for selector in ['div[id*="review"]', 'div.review-item', 'div.comment-item', 'tr[id*="review"]']:
                existing_comments = integrated_soup.select(selector)
                for comment in existing_comments:
                    comment.decompose()
            
            # æ„Ÿæƒ³ã‚’æŒ¿å…¥ã™ã‚‹å ´æ‰€ã‚’ç‰¹å®š
            content_area = integrated_soup.find('div', class_='content') or integrated_soup.find('div', class_='main') or integrated_soup.find('body')
            
            if content_area and all_comments:
                # å…¨æ„Ÿæƒ³ã‚’æŒ¿å…¥
                for comment in all_comments:
                    if comment:  # Noneãƒã‚§ãƒƒã‚¯
                        content_area.append(copy.deepcopy(comment))
            
            self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆå®Œäº†")
            return integrated_soup
            
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            import traceback
            self.debug_log(f"çµ±åˆã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}", "ERROR")
            return base_soup
        
    def get_chapter_links(self, soup, base_novel_url):
        """ç« ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹åŒ–ç‰ˆï¼‰"""
        chapter_links = []
        
        print("ç« ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ä¸­...")
        
        # ç¾åœ¨ã®ä½œå“IDã‚’æŠ½å‡º
        novel_id_match = re.search(r'/novel/(\d+)', base_novel_url)
        if not novel_id_match:
            print("ä½œå“IDã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            return []
        
        novel_id = novel_id_match.group(1)
        print(f"å¯¾è±¡ä½œå“ID: {novel_id}")
        
        # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ç« ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ï¼ˆä½œå“IDé™å®šï¼‰
        chapter_selectors = [
            # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®ä¸€èˆ¬çš„ãªç« ãƒªã‚¹ãƒˆ
            ('div', {'class': 'chapter_list'}),
            ('ul', {'class': 'episode_list'}),
            ('div', {'class': 'episode_list'}),
            # ç‰¹å®šä½œå“ã®ç« ã®ã¿ï¼ˆä½œå“IDé™å®šï¼‰
            ('a', {'href': lambda x: x and f'/novel/{novel_id}/' in x and x.count('/') >= 4 and x.endswith('.html')}),
            # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã®ç« ãƒªãƒ³ã‚¯ï¼ˆ./2.html, ./3.htmlç­‰ï¼‰
            ('a', {'href': lambda x: x and re.match(r'\./\d+\.html$', x)}),
            ('li', {'class': 'chapter'}),
            ('div', {'class': 'novel_sublist'})
        ]
        
        for tag, attrs in chapter_selectors:
            print(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            
            if callable(attrs.get('href')):
                elements = soup.find_all(tag, href=attrs['href'])
            else:
                elements = soup.find_all(tag, attrs)
            
            print(f"è¦‹ã¤ã‹ã£ãŸè¦ç´ æ•°: {len(elements)}")
            
            for element in elements:
                if tag == 'a':
                    href = element.get('href')
                    title = element.get_text(strip=True)
                    if href:
                        full_url = urljoin(base_novel_url, href)
                        
                        # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã®ç« ãƒªãƒ³ã‚¯ã®å ´åˆã€ä½œå“IDæ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç›´æ¥è¿½åŠ 
                        if re.match(r'\./\d+\.html$', href):
                            if full_url not in chapter_links:
                                chapter_links.append(full_url)
                                print(f"âœ“ ç« ãƒªãƒ³ã‚¯è¿½åŠ ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰: {title[:30]}... -> {full_url}")
                            else:
                                print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                        # çµ¶å¯¾ãƒ‘ã‚¹å½¢å¼ã®å ´åˆã¯ä½œå“IDæ¤œè¨¼
                        elif f'/novel/{novel_id}/' in full_url:
                            if full_url not in chapter_links:
                                chapter_links.append(full_url)
                                print(f"âœ“ ç« ãƒªãƒ³ã‚¯è¿½åŠ ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰: {title[:30]}... -> {full_url}")
                            else:
                                print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                        else:
                            print(f"âœ— ä½œå“IDä¸ä¸€è‡´ã§ã‚¹ã‚­ãƒƒãƒ—: {full_url} (æœŸå¾…ID: {novel_id})")
                else:
                    # div ã‚„ ul ã®å ´åˆã¯å†…éƒ¨ã®aã‚¿ã‚°ã‚’æ¢ã™
                    links = element.find_all('a', href=True)
                    print(f"ã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒªãƒ³ã‚¯æ•°: {len(links)}")
                    for link in links:
                        href = link.get('href')
                        if href and '/novel/' in href:
                            title = link.get_text(strip=True)
                            full_url = urljoin(self.base_url, href)
                            # ä½œå“IDæ¤œè¨¼
                            if f'/novel/{novel_id}/' in full_url:
                                if full_url not in chapter_links:
                                    chapter_links.append(full_url)
                                    print(f"âœ“ ç« ãƒªãƒ³ã‚¯è¿½åŠ : {title[:30]}... -> {full_url}")
                                else:
                                    print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                            else:
                                print(f"âœ— ä½œå“IDä¸ä¸€è‡´ã§ã‚¹ã‚­ãƒƒãƒ—: {full_url} (æœŸå¾…ID: {novel_id})")
        
        # é€šå¸¸ã®ãƒªãƒ³ã‚¯æ¤œç´¢ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä½œå“IDæ¤œè¨¼ä»˜ãï¼‰
        if not chapter_links:
            print("é€šå¸¸ã®ãƒªãƒ³ã‚¯æ¤œç´¢ã«åˆ‡ã‚Šæ›¿ãˆ...")
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã®ç« ãƒªãƒ³ã‚¯ã‚‚ãƒã‚§ãƒƒã‚¯
                if re.match(r'\./\d+\.html$', href):
                    full_url = urljoin(base_novel_url, href)
                    if full_url not in chapter_links:
                        chapter_links.append(full_url)
                        print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç« ãƒªãƒ³ã‚¯ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰: {text[:30]}... -> {full_url}")
                    else:
                        print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                # ã‚ˆã‚Šå³å¯†ãªç« ãƒªãƒ³ã‚¯ã®æ¡ä»¶ï¼ˆä½œå“IDæ¤œè¨¼ã‚’å«ã‚€ï¼‰
                elif (href and '/novel/' in href and 
                    len(href.split('/')) >= 4 and 
                    href != base_novel_url and
                    not any(x in href for x in ['user', 'tag', 'search', 'ranking'])):
                    
                    full_url = urljoin(self.base_url, href)
                    
                    # ä½œå“IDæ¤œè¨¼ã‚’è¿½åŠ 
                    if f'/novel/{novel_id}/' in full_url:
                        if full_url not in chapter_links:
                            chapter_links.append(full_url)
                            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç« ãƒªãƒ³ã‚¯ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰: {text[:30]}... -> {full_url}")
                        else:
                            print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                    else:
                        print(f"âœ— ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œå“IDä¸ä¸€è‡´ã§ã‚¹ã‚­ãƒƒãƒ—: {full_url} (æœŸå¾…ID: {novel_id})")
        
        # é‡è¤‡å‰Šé™¤ã¨ä¸¦ã³é †ç¢ºèª
        unique_links = []
        for link in chapter_links:
            if link not in unique_links:
                unique_links.append(link)
        
        print(f"æœ€çµ‚çš„ãªç« æ•°: {len(unique_links)}")
        return unique_links
        
    def create_complete_html(self, title, author, chapters, output_dir):
        """å®Œå…¨ãªHTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        
        # ãƒ¡ã‚¤ãƒ³HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        html_template = f"""<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="ja">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title} - ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ç‰ˆï¼‰</title>
    <style>
        body {{
            font-family: 'Helvetica Neue', Arial, 'Hiragino Kaku Gothic ProN', 'Hiragino Sans', Meiryo, sans-serif;
            line-height: 1.8;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fafafa;
        }}
        .header {{
            border-bottom: 2px solid #ddd;
            padding-bottom: 20px;
            margin-bottom: 30px;
            text-align: center;
        }}
        .title {{
            font-size: 2.5em;
            color: #333;
            margin-bottom: 10px;
        }}
        .author {{
            font-size: 1.3em;
            color: #666;
            margin-bottom: 20px;
        }}
        .navigation {{
            background-color: white;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }}
        .nav-title {{
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #444;
        }}
        .chapter-nav {{
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }}
        .chapter-link {{
            background-color: #007acc;
            color: white;
            padding: 8px 15px;
            text-decoration: none;
            border-radius: 3px;
            font-size: 0.9em;
        }}
        .chapter-link:hover {{
            background-color: #005a9e;
        }}
        .content {{
            background-color: white;
            padding: 30px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        .chapter {{
            margin-bottom: 50px;
            padding-bottom: 30px;
            border-bottom: 1px solid #eee;
        }}
        .chapter:last-child {{
            border-bottom: none;
        }}
        .chapter-title {{
            font-size: 1.8em;
            color: #444;
            border-left: 4px solid #007acc;
            padding-left: 15px;
            margin-bottom: 20px;
            scroll-margin-top: 20px;
        }}
        .chapter-content {{
            font-size: 1.1em;
            line-height: 1.9;
        }}
        .chapter-content p {{
            margin-bottom: 15px;
            text-indent: 1em;
        }}
        .back-to-top {{
            position: fixed;
            bottom: 20px;
            right: 20px;
            background-color: #007acc;
            color: white;
            padding: 10px 15px;
            border-radius: 50%;
            text-decoration: none;
            font-size: 1.2em;
        }}
        .back-to-top:hover {{
            background-color: #005a9e;
        }}
        @media (max-width: 768px) {{
            .chapter-nav {{
                flex-direction: column;
            }}
            .chapter-link {{
                text-align: center;
            }}
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1 class="title">{title}</h1>
        <div class="author">ä½œè€…: {author}</div>
    </div>
    
    <div class="navigation">
        <div class="nav-title">ç›®æ¬¡</div>
        <div class="chapter-nav">
            {self.create_chapter_navigation(chapters)}
        </div>
    </div>
    
    <div class="content">
        {self.create_chapters_html(chapters)}
    </div>
    
    <a href="#" class="back-to-top">â†‘</a>
    
    <script>
        // ã‚¹ãƒ ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {{
            anchor.addEventListener('click', function (e) {{
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {{
                    target.scrollIntoView({{
                        behavior: 'smooth'
                    }});
                }}
            }});
        }});
        
        // ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹ãƒœã‚¿ãƒ³
        document.querySelector('.back-to-top').addEventListener('click', function(e) {{
            e.preventDefault();
            window.scrollTo({{
                top: 0,
                behavior: 'smooth'
            }});
        }});
    </script>
</body>
</html>"""
        
        return html_template
        
    def create_chapter_navigation(self, chapters):
        """ç« ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        nav_html = []
        for i, chapter in enumerate(chapters, 1):
            title = chapter.get('title', f'ç¬¬{i}ç« ')
            nav_html.append(f'<a href="#chapter-{i}" class="chapter-link">{title}</a>')
        return '\n'.join(nav_html)
        
    def create_chapters_html(self, chapters):
        """ç« ã®HTMLã‚’ä½œæˆï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³é¢¨ï¼‰"""
        chapters_html = []
        for i, chapter in enumerate(chapters, 1):
            title = chapter.get('title', f'ç¬¬{i}ç« ')
            content = chapter.get('content', '')
            
            # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³é¢¨ã®ç« æ§‹é€ 
            chapter_html = f'''
            <div id="chapter-{i}" class="chapter-separator">
                <h2>{title}</h2>
                <br/>
                <div>
                    <div id="honbun">
                        {content}
                    </div>
                </div>
            </div>'''
            chapters_html.append(chapter_html)
        return '\n'.join(chapters_html)
        
    def extract_chapter_content(self, soup, chapter_url):
        """ç« ã®æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆ2024å¹´ç‰ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³å¯¾å¿œï¼‰"""
        self.debug_log(f"æœ¬æ–‡æŠ½å‡ºé–‹å§‹: {chapter_url}")
        
        # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®æœ¬æ–‡ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        content_selectors = [
            # â˜… å®Ÿéš›ã®ãƒãƒ¼ãƒ¡ãƒ«ãƒ³æ§‹é€ ï¼ˆ2024å¹´æœ€æ–°ï¼‰
            ('div', {'id': 'honbun'}),  # â† ã“ã‚ŒãŒå®Ÿéš›ã®æœ¬æ–‡IDï¼
            ('div', {'id': 'entry_box'}),  # æœ¬æ–‡ã‚’å«ã‚€å¤–å´ã®ã‚³ãƒ³ãƒ†ãƒŠ
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®å¤ã„æ§‹é€ 
            ('div', {'class': 'section3'}),
            ('div', {'class': 'section1'}),
            ('div', {'class': 'section2'}),
            ('div', {'class': 'section4'}),
            ('div', {'class': 'section5'}),
            ('div', {'class': 'section6'}),
            ('div', {'class': 'section7'}),
            ('div', {'class': 'section8'}),
            ('div', {'class': 'section9'}),
            # æ•°å­—ä»˜ãsectionã‚¯ãƒ©ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            ('div', {'class': lambda x: x and any(cls.startswith('section') and len(cls) > 7 and cls[7:].isdigit() for cls in x if isinstance(cls, str))}),
            # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç”¨ã®æ–°ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('div', {'class': 'p-novel-text'}),
            ('div', {'class': 'novel-text'}),
            ('section', {'class': 'p-novel-text'}),
            ('div', {'class': 'p-chapter-text'}),
            ('div', {'class': 'chapter-text'}),
            ('div', {'class': 'p-story-text'}),
            ('div', {'class': 'story-text'}),
            ('div', {'class': 'episode-text'}),
            ('div', {'class': 'p-episode-text'}),
            ('div', {'class': 'p-content-text'}),
            ('div', {'class': 'content-text'}),
            # IDãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('div', {'id': 'novel_body'}),
            ('div', {'id': 'main_text'}),
            ('div', {'id': 'chapter_body'}),
            ('div', {'id': 'story_body'}),
            ('div', {'id': 'content_body'}),
            ('div', {'id': 'episode_body'}),
            # å¾“æ¥ã®ãƒãƒ¼ãƒ¡ãƒ«ãƒ³æœ¬æ–‡ã‚¯ãƒ©ã‚¹
            ('div', {'class': 'novel_body'}),
            ('div', {'class': 'novel_view'}),
            ('div', {'class': 'novel_content'}),
            ('div', {'class': 'chapter_body'}),
            ('div', {'class': 'ss_body'}),
            ('div', {'class': 'contents'}),
            ('div', {'class': 'main_content'}),
            ('div', {'class': 'story_content'}),
            # ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('article', {'class': None}),
            ('main', {'class': None}),
            ('section', {'class': None}),
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰
            ('div', {'class': lambda x: x and any(keyword in ' '.join(x).lower() for keyword in ['body', 'content', 'text', 'story', 'chapter', 'episode', 'novel'])}),
            # æœ€å¾Œã®æ‰‹æ®µï¼šå¤§ããªdivã‚¿ã‚°ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒå¤šã„ï¼‰
            ('div', {'data-content': 'main'}),
            ('div', {'role': 'main'}),
        ]
        
        for tag, attrs in content_selectors:
            self.debug_log(f"æœ¬æ–‡ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            
            try:
                if callable(attrs.get('class')):
                    elements = soup.find_all(tag, class_=attrs['class'])
                elif callable(attrs.get('id')):
                    elements = soup.find_all(tag, id=attrs['id'])
                elif attrs:
                    elements = soup.find_all(tag, attrs)
                else:
                    elements = soup.find_all(tag)
                
                self.debug_log(f"è¦‹ã¤ã‹ã£ãŸè¦ç´ æ•°: {len(elements)}")
                
                for element in elements:
                    content_text = element.get_text(strip=True)
                    content_length = len(content_text)
                    
                    # ã‚ˆã‚Šè©³ç´°ãªæ¡ä»¶ãƒã‚§ãƒƒã‚¯
                    if content_length > 50:  # åŸºæº–ã‚’ç·©å’Œï¼ˆçŸ­ã„ç« ã«ã‚‚å¯¾å¿œï¼‰
                        # æœ¬æ–‡ã‚‰ã—ã„å†…å®¹ã‹ãƒã‚§ãƒƒã‚¯
                        if self.is_likely_novel_content(content_text):
                            self.debug_log(f"æœ¬æ–‡å–å¾—æˆåŠŸ: {content_length}æ–‡å­—")
                            # å®Œå…¨ãªè¦‹ãŸç›®ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€å…ƒã®HTMLæ§‹é€ ã‚’ä¿æŒ
                            return self.preserve_original_formatting(element)
                        else:
                            self.debug_log(f"æœ¬æ–‡å€™è£œã ãŒå†…å®¹ãŒé©åˆ‡ã§ãªã„: {content_length}æ–‡å­—")
                    else:
                        self.debug_log(f"è¦ç´ ãŒçŸ­ã™ãã¾ã™: {content_length}æ–‡å­—")
                        
            except Exception as e:
                self.debug_log(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
        
        self.debug_log("æœ¬æ–‡å–å¾—å¤±æ•—: é©åˆ‡ãªè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
        
        # è©³ç´°èª¿æŸ»ã‚’å®Ÿè¡Œ
        self.investigate_content_structure(soup)
        
        # æœ€å¾Œã®æ‰‹æ®µï¼šæœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¦ç´ ã‚’é¸æŠ
        self.debug_log("æœ€å¾Œã®æ‰‹æ®µï¼šæœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’æ¤œç´¢")
        longest_element = None
        longest_length = 0
        
        for div in soup.find_all('div'):
            content_text = div.get_text(strip=True)
            if len(content_text) > longest_length and len(content_text) > 100:
                # æ˜ã‚‰ã‹ã«ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if not any(keyword in content_text for keyword in ['ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒ˜ãƒƒãƒ€ãƒ¼', 'ãƒ•ãƒƒã‚¿ãƒ¼']):
                    longest_element = div
                    longest_length = len(content_text)
        
        if longest_element:
            self.debug_log(f"æœ€é•·ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’ä½¿ç”¨: {longest_length}æ–‡å­—")
            return self.preserve_original_formatting(longest_element)
        
        return ""
        
    def preserve_original_formatting(self, element):
        """å…ƒã®HTMLæ§‹é€ ã‚’ä¿æŒã—ã¦è¦‹ãŸç›®ã‚’å®Œå…¨å†ç¾"""
        # å…ƒã®HTMLã‚¿ã‚°ã‚’ä¿æŒ
        html_content = str(element)
        
        # ä¸è¦ãªå±æ€§ã‚’å‰Šé™¤ï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®ãŸã‚ï¼‰
        import re
        # onclick, onloadç­‰ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
        html_content = re.sub(r'\s*on\w+\s*=\s*["\'][^"\'>]*["\']', '', html_content)
        # data-trackç­‰ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°å±æ€§ã‚’å‰Šé™¤
        html_content = re.sub(r'\s*data-track\w*\s*=\s*["\'][^"\'>]*["\']', '', html_content)
        
        self.debug_log(f"å…ƒã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿æŒ: {len(html_content)}ãƒã‚¤ãƒˆ")
        return html_content
        
    def is_likely_novel_content(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆãŒå°èª¬ã®æœ¬æ–‡ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        # åŸºæœ¬çš„ãªé•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        if len(text) < 30:  # ã•ã‚‰ã«ç·©å’Œ
            return False
        
        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚„ä¸è¦ãªè¦ç´ ã‚’é™¤å¤–
        exclusion_keywords = [
            'ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒ˜ãƒƒãƒ€ãƒ¼', 'ãƒ•ãƒƒã‚¿ãƒ¼',
            'ã‚µã‚¤ãƒ‰ãƒãƒ¼', 'åºƒå‘Š', 'ã‚¢ãƒ‰ãƒã‚¿ã‚¤ã‚º', 'ã‚³ãƒ¡ãƒ³ãƒˆ',
            'ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'ãŠçŸ¥ã‚‰ã›', 'åˆ©ç”¨è¦ç´„', 'æ¤œç´¢',
            'ãƒ­ã‚°ã‚¤ãƒ³', 'ãƒã‚¤ãƒšãƒ¼ã‚¸', 'ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',
            'ã‚¿ã‚°ä¸€è¦§', 'ã‚«ãƒ†ã‚´ãƒª', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«',
            'ãƒ•ã‚©ãƒ­ãƒ¼', 'ã„ã„ã­', 'ã‚·ã‚§ã‚¢', 'ãƒ„ã‚¤ãƒ¼ãƒˆ',
            'ã‚³ãƒ”ãƒ¼', 'URL', 'ãƒªãƒ³ã‚¯', 'ã‚½ãƒ¼ã‚·ãƒ£ãƒ«'
        ]
        
        for keyword in exclusion_keywords:
            if keyword in text:
                self.debug_log(f"é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {keyword}")
                return False
        
        # å°èª¬ã‚‰ã—ã„è¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        novel_indicators = [
            'ã€‚', 'ã€Œ', 'ã€', 'ã ', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™',
            'ã—ãŸ', 'ã™ã‚‹', 'ãã®', 'ã“ã®', 'ã‚ã®', 'ãŒ', 'ã‚’', 'ã«', 'ã¯', 'ã§'
        ]
        
        indicator_count = sum(1 for indicator in novel_indicators if indicator in text)
        if indicator_count < 2:  # 3â†’2ã«ç·©å’Œ
            self.debug_log(f"å°èª¬ã‚‰ã—ã„è¦ç´ ãŒå°‘ãªã„: {indicator_count}/{len(novel_indicators)}")
            return False
        
        # â˜… ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®ä½œå“èª¬æ˜æ–‡ã¯æœ¬æ–‡ã§ã¯ãªã„ï¼ˆé™¤å¤–ï¼‰
        if ('ç·åˆè©•ä¾¡ï¼š' in text or 'è©•ä¾¡ï¼š' in text or 'é€£è¼‰ï¼š' in text or 
            'æ›´æ–°æ—¥æ™‚ï¼š' in text or 'å°èª¬æƒ…å ±' in text or 'href="//syosetu.org/' in str(text)):
            self.debug_log("ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ä½œå“èª¬æ˜æ–‡ã¨ã—ã¦é™¤å¤–")
            return False
        
        self.debug_log(f"å°èª¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã—ã¦èªè­˜: {indicator_count}å€‹ã®æŒ‡æ¨™ã‚’ç¢ºèª")
        return True
        
    def investigate_content_structure(self, soup):
        """æœ¬æ–‡æ§‹é€ ã‚’è©³ç´°èª¿æŸ»"""
        self.debug_log("=== æœ¬æ–‡æ§‹é€ è©³ç´°èª¿æŸ» ===")
        
        # ãƒ†ã‚­ã‚¹ãƒˆé‡ã®å¤šã„è¦ç´ ã‚’æ¤œç´¢
        text_elements = []
        for element in soup.find_all(['div', 'section', 'article', 'main', 'p']):
            text = element.get_text(strip=True)
            if len(text) > 50:
                text_elements.append({
                    'tag': element.name,
                    'class': element.get('class', []),
                    'id': element.get('id', ''),
                    'length': len(text),
                    'preview': text[:100] + '...' if len(text) > 100 else text
                })
        
        # é•·ã•é †ã§ã‚½ãƒ¼ãƒˆ
        text_elements.sort(key=lambda x: x['length'], reverse=True)
        
        self.debug_log(f"ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ä¸Šä½5ä»¶:")
        for i, elem in enumerate(text_elements[:5]):
            self.debug_log(f"  [{i+1}] {elem['tag']} class={elem['class']} id={elem['id']} length={elem['length']}")
            self.debug_log(f"      preview: {elem['preview']}")
        
        # ã‚¯ãƒ©ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        all_classes = set()
        for div in soup.find_all('div', class_=True):
            all_classes.update(div.get('class', []))
        
        content_related_classes = [cls for cls in all_classes 
                                 if any(keyword in cls.lower() 
                                       for keyword in ['text', 'content', 'body', 'story', 'novel', 'chapter', 'section'])]
        
        self.debug_log(f"æœ¬æ–‡é–¢é€£ã‚¯ãƒ©ã‚¹åå€™è£œ: {sorted(content_related_classes)}")
    
    def process_single_chapter(self, chapter_url):
        """å˜ä¸€ã®ç« ãƒšãƒ¼ã‚¸ã‚’å®Œå…¨ä¿å­˜"""
        print(f"=== å˜è©±å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ===")
        print(f"URL: {chapter_url}")
        soup = self.get_page(chapter_url)
        
        if not soup:
            print("ç« ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç°¡å˜ã«æŠ½å‡ºï¼ˆtitleã‚¿ã‚°ã‹ã‚‰ï¼‰
        title = soup.title.string if soup.title else "ç« ãƒšãƒ¼ã‚¸"
        print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        
        # #honbunè¦ç´ ã®å­˜åœ¨ã‚’ç¢ºèª
        honbun_element = soup.find('div', {'id': 'honbun'})
        if honbun_element:
            print(f"#honbunè¦ç´ ã‚’ç™ºè¦‹: {len(honbun_element.get_text(strip=True))}æ–‡å­—")
            self.debug_log(f"#honbunè¦ç´ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {honbun_element.get_text(strip=True)[:200]}...")
        else:
            print("è­¦å‘Š: #honbunè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.debug_log("è­¦å‘Š: #honbunè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - æ§‹é€ èª¿æŸ»ã‚’å®Ÿè¡Œ")
            # ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’èª¿æŸ»
            all_divs = soup.find_all('div', id=True)
            print(f"IDä»˜ãdivè¦ç´ æ•°: {len(all_divs)}")
            for div in all_divs[:10]:  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
                div_id = div.get('id')
                text_preview = div.get_text(strip=True)[:50]
                print(f"  ID: {div_id}, ãƒ†ã‚­ã‚¹ãƒˆ: {text_preview}...")
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆãƒ–ãƒ©ã‚¦ã‚¶äº’æ›å½¢å¼ï¼‰
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_dir = os.path.join("saved_novels", safe_title)
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’è¨­å®š
        self.browser_compatible_name = safe_title + "_files"
        
        # å®Œå…¨ãªãƒšãƒ¼ã‚¸ã¨ã—ã¦ä¿å­˜ï¼ˆæ§‹é€ ã‚’ãã®ã¾ã¾ä¿æŒï¼‰
        base_url = "https://syosetu.org"
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_file = self.save_complete_page(soup, base_url, safe_title, output_dir, chapter_url)
        print(f"å®Œå…¨ãªãƒšãƒ¼ã‚¸ã¨ã—ã¦ä¿å­˜: 1ãƒšãƒ¼ã‚¸")
        
        return output_file
    
    def save_complete_page(self, soup, base_url, 
                          title, save_dir, page_url):
        """ãƒšãƒ¼ã‚¸ã‚’å®Œå…¨ãªå½¢ã§ä¿å­˜ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã¨åŒç­‰ï¼‰"""
        print("=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ä¿å­˜é–‹å§‹ ===")
        
        # ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’å–å¾—
        resources_dir_name = getattr(self, 'browser_compatible_name', 'resources')
        
        # ãƒªã‚½ãƒ¼ã‚¹ã‚’å‡¦ç†ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
        soup = self.process_html_resources(soup, save_dir)
        
        # ãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã®ã‚ˆã†ã«ã€å…ƒURLã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
        html_tag = soup.find('html')
        if html_tag:
            from bs4 import Comment
            comment = Comment(f' saved from url=({len(page_url):04d}){page_url} ')
            html_tag.insert(0, comment)
        
        # ãƒ™ãƒ¼ã‚¹URLã‚’è¨­å®š
        base_url = '/'.join(page_url.split('/')[:3])  # https://syosetu.org
        
        # ç›¸å¯¾ãƒªãƒ³ã‚¯ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã¨åŒæ§˜ï¼‰
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                if href.startswith('//'):
                    link['href'] = 'https:' + href
                elif href.startswith('/') and not href.startswith('//'):
                    link['href'] = base_url + href
                elif href.startswith('./'):
                    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                    current_dir = '/'.join(page_url.split('/')[:-1])
                    link['href'] = current_dir + '/' + href[2:]
        
        # ã™ã¹ã¦ã®ç”»åƒã®srcã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        for img in soup.find_all('img', src=True):
            src = img.get('src')
            if src and not src.startswith('./' + resources_dir_name + '/'):  # æ—¢ã«ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ
                if src.startswith('//'):
                    img['src'] = 'https:' + src
                elif src.startswith('/') and not src.startswith('//'):
                    img['src'] = base_url + src
        
        # CSS ã‚„ JS ã®å‚ç…§ã‚‚åŒæ§˜ã«å‡¦ç†
        for link in soup.find_all('link', href=True):
            href = link.get('href')
            if href and not href.startswith('./' + resources_dir_name + '/'):
                if href.startswith('//'):
                    link['href'] = 'https:' + href
                elif href.startswith('/') and not href.startswith('//'):
                    link['href'] = base_url + href
        
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src and not src.startswith('./' + resources_dir_name + '/'):
                if src.startswith('//'):
                    script['src'] = 'https:' + src
                elif src.startswith('/') and not src.startswith('//'):
                    script['src'] = base_url + src
        
        # ãƒ¡ã‚¿æƒ…å ±ã‚’ä¿æŒï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã®ã‚ˆã†ã«ï¼‰
        head = soup.find('head')
        if head:
            # ä¿å­˜æ—¥æ™‚ã‚’è¿½åŠ 
            from datetime import datetime
            save_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            meta_save = soup.new_tag('meta')
            meta_save['name'] = 'save-date'
            meta_save['content'] = save_time
            head.append(meta_save)
            
            # ä¿å­˜å…ƒURLã‚’è¿½åŠ 
            meta_source = soup.new_tag('meta')
            meta_source['name'] = 'source-url'
            meta_source['content'] = page_url
            head.append(meta_source)
        
        # å®Œå…¨ãªHTMLã¨ã—ã¦ä¿å­˜
        safe_filename = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_file = os.path.join(save_dir, f"{safe_filename}.html")
        
        # UTF-8 BOMã‚’å«ã‚ã¦ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›æ€§ã‚’å‘ä¸Š
        html_content = str(soup)
        
        with open(output_file, 'w', encoding='utf-8-sig') as f:
            f.write(html_content)
        
        print(f"=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ä¿å­˜å®Œäº†: {output_file} ===")
        return output_file
        
    def scrape_novel(self, novel_url):
        """å°èª¬å…¨ä½“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ä¿å­˜ï¼ˆå®Œå…¨ãƒ¢ãƒ¼ãƒ‰ä¸€æœ¬åŒ–ï¼‰"""
        print(f"=== å°èª¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ===")
        print(f"URL: {novel_url}")
        
        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®åˆæœŸå¾…æ©Ÿ
        time.sleep(2)
        
        # ç« ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä¾‹: /novel/378070/2.htmlï¼‰
        is_chapter_page = bool(re.search(r'/novel/\d+/\d+\.html$', novel_url))
        print(f"ç« ãƒšãƒ¼ã‚¸åˆ¤å®š: {is_chapter_page}")
        
        if is_chapter_page:
            print("ç« ãƒšãƒ¼ã‚¸ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨è©±å–å¾—ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            # ç« ãƒšãƒ¼ã‚¸ã®URLã‹ã‚‰ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®URLã‚’æ§‹ç¯‰
            # ä¾‹: https://syosetu.org/novel/378070/2.html â†’ https://syosetu.org/novel/378070/
            match = re.search(r'(https?://[^/]+/novel/\d+)/', novel_url)
            print(f"URLæ­£è¦è¡¨ç¾ãƒãƒƒãƒçµæœ: {match}")
            if match:
                index_url = match.group(1) + '/'
                print(f"ç›®æ¬¡ãƒšãƒ¼ã‚¸URL: {index_url}")
                print(f"å…ƒã®URL: {novel_url}")
                print(f"URLæ¯”è¼ƒ: {index_url} != {novel_url} = {index_url != novel_url}")
                
                # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼šç¾åœ¨ã®URLãŒæ—¢ã«ç›®æ¬¡ãƒšãƒ¼ã‚¸ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                if index_url != novel_url:
                    print("ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨è©±å–å¾—ã‚’å®Ÿè¡Œã—ã¾ã™...")
                    # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨è©±å–å¾—ã‚’å®Ÿè¡Œ
                    return self.scrape_novel(index_url)
                else:
                    print("æ—¢ã«ç›®æ¬¡ãƒšãƒ¼ã‚¸ã§ã™ã€‚é€šå¸¸å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
            else:
                print("ç›®æ¬¡ãƒšãƒ¼ã‚¸URLã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å˜è©±å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
                return self.process_single_chapter(novel_url)
        
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        soup = self.get_page(novel_url)
            
        if not soup:
            print("ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
            
        # å°èª¬æƒ…å ±ã‚’æŠ½å‡º
        novel_info = self.extract_novel_info(soup)
        title = novel_info.get('title', 'Unknown Title')
        author = novel_info.get('author', 'Unknown Author')
        
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        print(f"ä½œè€…: {author}")
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_dir = os.path.join("saved_novels", safe_title)
        os.makedirs(output_dir, exist_ok=True)
        
        # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ï¼ˆè¤‡æ•°ç« ãŒã‚ã‚‹å ´åˆï¼‰
        index_file_path = None
        
        # ç« ãƒªãƒ³ã‚¯ã‚’å–å¾—
        chapter_links = self.get_chapter_links(soup, novel_url)
        print(f"ç« æ•°: {len(chapter_links)}")
        
        # å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³ãƒ•ã‚¡ã‚¤ãƒ«åã®åˆæœŸåŒ–ï¼ˆç« å‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ãŸã‚äº‹å‰ã«å®šç¾©ï¼‰
        info_file_name = None
        comments_file_name = None
        
        # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã¨ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if len(chapter_links) > 1:
            print("ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ä¸­...")
            # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
            index_file_path = self.save_complete_page(
                soup, 
                novel_url,
                f"{safe_title} - ç›®æ¬¡",
                output_dir, 
                novel_url
            )
            if index_file_path:
                print(f"ğŸ“– ç›®æ¬¡ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {os.path.basename(index_file_path)}")
            
            print("ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆCSSã€JSã€ç”»åƒç­‰ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            # ãƒªã‚½ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œï¼ˆå„ç« ã§ã¯ãƒªã‚½ãƒ¼ã‚¹å†å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            self.process_html_resources(soup, output_dir)
            print("ğŸ“ ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å„ç« ã§ã¯ãƒªã‚½ãƒ¼ã‚¹å†å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            
            # ğŸ†• æ–°æ©Ÿèƒ½: å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³ä¿å­˜ï¼ˆãƒ•ãƒ©ã‚°ã§åˆ¶å¾¡ï¼‰
            
            # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’å…ˆã«ä¿å­˜ï¼ˆå°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
            if self.enable_comments_saving:
                print("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ä¸­...")
                comments_url = self.extract_comments_url(soup)
                if comments_url:
                    # index_file_pathãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å˜ä¸€ãƒšãƒ¼ã‚¸ãªã®ã§ None ã‚’æ¸¡ã™
                    index_file_name = os.path.basename(index_file_path) if index_file_path else None
                    comments_file_path = self.save_comments_page(comments_url, output_dir, title, index_file_name, info_file_name)
                    if comments_file_path:
                        comments_file_name = os.path.basename(comments_file_path)
                        print(f"ğŸ’¬ æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {comments_file_name}")
                    else:
                        print("âš ï¸ æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        comments_file_name = None
                else:
                    print("âš ï¸ æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    comments_file_name = None
            else:
                comments_file_name = None
            
            if self.enable_novel_info_saving:
                print("å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ä¸­...")
                info_url = self.extract_novel_info_url(soup)
                if info_url:
                    # index_file_pathãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å˜ä¸€ãƒšãƒ¼ã‚¸ãªã®ã§ None ã‚’æ¸¡ã™
                    index_file_name = os.path.basename(index_file_path) if index_file_path else None
                    info_file_path = self.save_novel_info_page(info_url, output_dir, title, index_file_name, comments_file_name)
                    if info_file_path:
                        info_file_name = os.path.basename(info_file_path)
                        print(f"ğŸ“ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {info_file_name}")
                        
                        # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’å†ä¿®æ­£ï¼ˆå°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ï¼‰
                        if comments_file_path and os.path.exists(comments_file_path):
                            print("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã‚’å†ä¿®æ­£ä¸­...")
                            comments_dir = os.path.dirname(comments_file_path)
                            if os.path.exists(comments_dir):
                                # æ„Ÿæƒ³ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã™ã¹ã¦ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                                for comments_file in os.listdir(comments_dir):
                                    if comments_file.endswith('.html'):
                                        comments_file_full_path = os.path.join(comments_dir, comments_file)
                                        with open(comments_file_full_path, 'r', encoding='utf-8') as f:
                                            content = f.read()
                                        
                                        soup_comments = BeautifulSoup(content, 'html.parser')
                                        
                                        # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
                                        for link in soup_comments.find_all('a', href=True):
                                            href = link.get('href')
                                            if href and ('mode=ss_detail' in href or 'å°èª¬æƒ…å ±' in link.get_text()):
                                                link['href'] = f'../{info_file_name}'
                                                self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯å†ä¿®æ­£: {href} -> ../{info_file_name}")
                                        
                                        with open(comments_file_full_path, 'w', encoding='utf-8') as f:
                                            f.write(str(soup_comments))
                    else:
                        print("âš ï¸ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        info_file_name = None
                else:
                    print("âš ï¸ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    info_file_name = None
            else:
                info_file_name = None
            
            # ğŸ†• ç¸¦æ›¸ããƒšãƒ¼ã‚¸å‡¦ç†æ©Ÿèƒ½
            print("ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã¨é–¢é€£ãƒªãƒ³ã‚¯ã‚’å‡¦ç†ä¸­...")
            # ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ä¿®æ­£ã®ãŸã‚ã€é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¸¡ã™
            index_file_for_vertical = os.path.basename(index_file_path) if index_file_path else None
            comments_file_for_vertical = f"æ„Ÿæƒ³/æ„Ÿæƒ³ - ãƒšãƒ¼ã‚¸1.html" if comments_file_name else None
            
            vertical_result = self.process_vertical_reading_links(
                soup, 
                output_dir, 
                title,
                index_file=index_file_for_vertical,
                info_file=info_file_name,
                comments_file=comments_file_for_vertical
            )
            if vertical_result:
                if 'vertical_page' in vertical_result:
                    print(f"ğŸ“– ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {os.path.basename(vertical_result['vertical_page']['file_path'])}")
                    additional_links = vertical_result['vertical_page'].get('additional_links', [])
                    if additional_links:
                        print(f"ğŸ”— ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…é–¢é€£ãƒªãƒ³ã‚¯: {len(additional_links)}å€‹ç™ºè¦‹")
                
                if 'info_page' in vertical_result:
                    print(f"ğŸ“ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ï¼ˆç¸¦æ›¸ãç”¨ï¼‰ä¿å­˜å®Œäº†: {os.path.basename(vertical_result['info_page'])}")
            else:
                print("âš ï¸ ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã¾ãŸã¯å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                vertical_result = None
            
            # ç¸¦æ›¸ããƒšãƒ¼ã‚¸å‡¦ç†å®Œäº†å¾Œã€æ—¢å­˜ã®å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¨æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’ä¿®æ­£
            if vertical_result and 'vertical_page' in vertical_result:
                vertical_file_name = os.path.basename(vertical_result['vertical_page']['file_path'])
                print("æ—¢å­˜ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
                
                # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ä¿®æ­£
                if info_file_name and info_file_path and os.path.exists(info_file_path):
                    print(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­: {info_file_name}")
                    with open(info_file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    soup_info = BeautifulSoup(content, 'html.parser')
                    soup_info = self.fix_vertical_links_in_all_pages(
                        soup_info,
                        index_file_name,
                        None,  # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸è‡ªèº«ãªã®ã§info_file=None
                        vertical_file_name
                    )
                    
                    with open(info_file_path, 'w', encoding='utf-8') as f:
                        f.write(str(soup_info))
                    print(f"âœ“ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ä¿®æ­£å®Œäº†")
                
                # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ä¿®æ­£ï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸ã‚ã‚‹å ´åˆï¼‰
                if comments_file_name:
                    comments_dir = os.path.join(output_dir, "æ„Ÿæƒ³")
                    if os.path.exists(comments_dir):
                        print("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
                        for filename in os.listdir(comments_dir):
                            if filename.endswith('.html'):
                                file_path = os.path.join(comments_dir, filename)
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                
                                soup_comment = BeautifulSoup(content, 'html.parser')
                                soup_comment = self.fix_vertical_links_in_all_pages(
                                    soup_comment,
                                    f"../{index_file_name}" if index_file_name else "../ç›®æ¬¡.html",
                                    f"../{info_file_name}" if info_file_name else None,
                                    f"../{vertical_file_name}"
                                )
                                
                                with open(file_path, 'w', encoding='utf-8') as f:
                                    f.write(str(soup_comment))
                        print(f"âœ“ æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ä¿®æ­£å®Œäº†")
        
        if not chapter_links:
            print("ç« ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å˜ä¸€ãƒšãƒ¼ã‚¸ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
            
            # å˜ä¸€ãƒšãƒ¼ã‚¸ã®å ´åˆã§ã‚‚ç¸¦æ›¸ããƒšãƒ¼ã‚¸å‡¦ç†ã‚’å®Ÿè¡Œ
            print("ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã¨é–¢é€£ãƒªãƒ³ã‚¯ã‚’å‡¦ç†ä¸­...")
            # å˜ä¸€ãƒšãƒ¼ã‚¸ã®å ´åˆã€index_file_pathã¯æœªå®šç¾©ãªã®ã§ç›®æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«åã¯æ¨å®š
            index_file_for_vertical = f"{safe_title} - ç›®æ¬¡.html" if index_file_path else None
            comments_file_for_vertical = f"æ„Ÿæƒ³/æ„Ÿæƒ³ - ãƒšãƒ¼ã‚¸1.html" if comments_file_name else None
            
            vertical_result = self.process_vertical_reading_links(
                soup, 
                output_dir, 
                title,
                index_file=index_file_for_vertical,
                info_file=info_file_name,
                comments_file=comments_file_for_vertical
            )
            if vertical_result:
                if 'vertical_page' in vertical_result:
                    print(f"ğŸ“– ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {os.path.basename(vertical_result['vertical_page']['file_path'])}")
                    additional_links = vertical_result['vertical_page'].get('additional_links', [])
                    if additional_links:
                        print(f"ğŸ”— ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…é–¢é€£ãƒªãƒ³ã‚¯: {len(additional_links)}å€‹ç™ºè¦‹")
                
                if 'info_page' in vertical_result:
                    print(f"ğŸ“ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ï¼ˆç¸¦æ›¸ãç”¨ï¼‰ä¿å­˜å®Œäº†: {os.path.basename(vertical_result['info_page'])}")
            else:
                print("âš ï¸ ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã¾ãŸã¯å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                vertical_result = None
            
            # å˜ä¸€ãƒšãƒ¼ã‚¸ã®å ´åˆ
            chapter_content = self.extract_chapter_content(soup, novel_url)
            if chapter_content:
                chapters = [{
                    'title': title,
                    'content': chapter_content
                }]
            else:
                print("æœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
        else:
            # å„ç« ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            chapters = []
            chapter_mapping = {}  # URL -> ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            saved_chapters = []
            
            # ã¾ãšç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
            if index_file_path:
                index_filename = os.path.basename(index_file_path)
            else:
                index_filename = None
            
            # ç« ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’äº‹å‰ã«ä½œæˆï¼ˆå‰ã®è©±ãƒ»æ¬¡ã®è©±ãƒªãƒ³ã‚¯ç”¨ï¼‰
            print("ç« ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆä¸­...")
            # åˆæœŸãƒãƒƒãƒ”ãƒ³ã‚°ã¯ç©ºã§é–‹å§‹ï¼ˆçŸ­ç¸®å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆã‚’å›é¿ï¼‰
            chapter_mapping = {}
            print(f"äº‹å‰ãƒãƒƒãƒ”ãƒ³ã‚°æº–å‚™å®Œäº†: {len(chapter_links)}ç« ")
            
            for i, chapter_url in enumerate(chapter_links, 1):
                print(f"ç«  {i}/{len(chapter_links)} ã‚’å–å¾—ä¸­: {chapter_url}")
                
                # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®é©å¿œçš„å¾…æ©Ÿ
                if i > 1:
                    wait_time = min(3 + (i // 10), 8)  # ç« æ•°ãŒå¢—ãˆã‚‹ã»ã©å¾…æ©Ÿæ™‚é–“ã‚’å¢—åŠ 
                    print(f"ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ {wait_time} ç§’å¾…æ©Ÿ...")
                    time.sleep(wait_time)
                
                chapter_soup = self.get_page(chapter_url)
                
                if chapter_soup:
                    # ãƒªã‚½ãƒ¼ã‚¹å‡¦ç†ã¯ç›®æ¬¡ãƒšãƒ¼ã‚¸ã§å®Œäº†æ¸ˆã¿ã®ãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹èª¿æ•´ã®ã¿
                    chapter_soup = self.adjust_resource_paths_only(chapter_soup, output_dir)
                    
                    # ç« ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                    chapter_title = chapter_soup.find('title')
                    if chapter_title:
                        chapter_title_text = chapter_title.get_text(strip=True)
                    else:
                        chapter_title_text = f"ç¬¬{i}è©±"
                    
                    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒªãƒ³ã‚¯ä¿®æ­£ï¼ˆç¬¬1å›ç›®ï¼šä»®ã®ãƒãƒƒãƒ”ãƒ³ã‚° + å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³å¯¾å¿œï¼‰
                    chapter_soup = self.fix_local_navigation_links(
                        chapter_soup, 
                        chapter_mapping, 
                        chapter_url, 
                        os.path.basename(index_file_path) if index_file_path else None,
                        info_file_name,
                        comments_file_name
                    )
                    
                    # ç¸¦æ›¸ããƒšãƒ¼ã‚¸å¯¾å¿œã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ä¿®æ­£
                    vertical_file_name = None
                    if vertical_result and 'vertical_page' in vertical_result:
                        vertical_file_name = os.path.basename(vertical_result['vertical_page']['file_path'])
                    
                    info_file_name_for_vertical = None
                    if vertical_result and 'info_page' in vertical_result:
                        info_file_name_for_vertical = os.path.basename(vertical_result['info_page'])
                    
                    if vertical_file_name or info_file_name_for_vertical:
                        chapter_soup = self.update_navigation_links_with_vertical_pages(
                            chapter_soup,
                            os.path.basename(index_file_path) if index_file_path else None,
                            info_file_name_for_vertical or info_file_name,
                            vertical_file_name
                        )
                    
                    # ç« ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                    safe_chapter_title = re.sub(r'[<>:"/\\|?*]', '_', chapter_title_text)
                    chapter_file_path = self.save_complete_page(
                        chapter_soup, 
                        chapter_url,
                        safe_chapter_title,
                        output_dir, 
                        chapter_url
                    )
                    
                    if chapter_file_path:
                        chapter_filename = os.path.basename(chapter_file_path)
                        # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ›´æ–°
                        chapter_mapping[chapter_url] = chapter_filename
                        print(f"ãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°: {chapter_url} -> {chapter_filename}")
                        saved_chapters.append({
                            'url': chapter_url,
                            'title': chapter_title_text,
                            'file_path': chapter_file_path,
                            'filename': chapter_filename
                        })
                        print(f"ç«  {i} ä¿å­˜å®Œäº†: {chapter_title_text} -> {chapter_filename}")
                    else:
                        print(f"ç«  {i} ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    print(f"ç«  {i} ã®ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ç¬¬2å›ç›®ï¼šå…¨ç« ã®ç›¸äº’ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
            print("ç« é–“ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
            for chapter_info in saved_chapters:
                if os.path.exists(chapter_info['file_path']):
                    with open(chapter_info['file_path'], 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    soup = self.fix_local_navigation_links(
                        soup, 
                        chapter_mapping, 
                        chapter_info['url'], 
                        os.path.basename(index_file_path) if index_file_path else None,
                        info_file_name,
                        comments_file_name
                    )
                    
                    with open(chapter_info['file_path'], 'w', encoding='utf-8') as f:
                        f.write(str(soup))
            
            # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚‚ä¿®æ­£
            if index_file_path and os.path.exists(index_file_path):
                print("ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
                with open(index_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                soup = self.fix_local_navigation_links(
                    soup, 
                    chapter_mapping, 
                    novel_url, 
                    None,
                    info_file_name,
                    comments_file_name
                )
                
                # ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚‚ä¿®æ­£
                vertical_file_name = None
                if vertical_result and 'vertical_page' in vertical_result:
                    vertical_file_name = os.path.basename(vertical_result['vertical_page']['file_path'])
                
                if vertical_file_name:
                    soup = self.fix_vertical_links_in_all_pages(
                        soup,
                        None,  # ç›®æ¬¡ãƒšãƒ¼ã‚¸è‡ªèº«ãªã®ã§index_file=None
                        info_file_name,
                        vertical_file_name
                    )
                
                with open(index_file_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
            
            # æ—§å½¢å¼ã®ç« ãƒ‡ãƒ¼ã‚¿ã‚‚ä½œæˆï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
            chapters = [{'title': ch['title'], 'content': ''} for ch in saved_chapters]
        
        if not chapters:
            print("å–å¾—ã§ããŸç« ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
        print(f"å°èª¬ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: {output_dir}")
        print(f"å–å¾—ã—ãŸç« æ•°: {len(chapters)}")
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿”ã™ï¼ˆGUIäº’æ›æ€§ã®ãŸã‚ï¼‰
        return output_dir
        
    def get_cache_stats(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            'cached_resources': len(self.resource_cache),
            'cache_entries': list(self.resource_cache.keys())[:10]  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
        }

    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾"""
        if self.driver:
            self.driver.quit()
            print("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã—ãŸ")
        
        stats = self.get_cache_stats()
        self.debug_log(f"ãƒªã‚½ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ: {stats['cached_resources']}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ")

    def extract_vertical_reading_link(self, soup):
        """ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’HTMLæ§‹é€ ã‹ã‚‰æŠ½å‡º"""
        try:
            # topicPathå†…ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’æ¢ã™
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href and 'mode=ss_detail3' in href:
                    self.debug_log(f"ç¸¦æ›¸ããƒªãƒ³ã‚¯ç™ºè¦‹: {href}")
                    return href
            return None
        except Exception as e:
            self.debug_log(f"ç¸¦æ›¸ããƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def extract_novel_info_link(self, soup):
        """å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã‚’HTMLæ§‹é€ ã‹ã‚‰æŠ½å‡º"""
        try:
            # topicPathå†…ã®å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href and 'mode=ss_detail' in href and 'mode=ss_detail3' not in href:
                    self.debug_log(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ç™ºè¦‹: {href}")
                    return href
            return None
        except Exception as e:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def extract_additional_links_from_vertical_page(self, soup):
        """ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        try:
            additional_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href and any(mode in href for mode in ['mode=review', 'mode=rating_input', 'mode=correct_view']):
                    additional_links.append(href)
                    self.debug_log(f"è¿½åŠ ãƒªãƒ³ã‚¯ç™ºè¦‹: {href}")
            return additional_links
        except Exception as e:
            self.debug_log(f"è¿½åŠ ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return []
    
    def save_vertical_reading_page(self, vertical_url, output_dir, novel_title, index_file=None, info_file=None, comments_file=None):
        """ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‚’ä¿å­˜ï¼ˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ä¿®æ­£ã¨PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜é–‹å§‹: {vertical_url}")
            
            # URLãŒç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            if vertical_url.startswith('//'):
                vertical_url = 'https:' + vertical_url
            elif vertical_url.startswith('/'):
                vertical_url = 'https://syosetu.org' + vertical_url
            
            # ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‚’å–å¾—
            soup = self.get_page(vertical_url)
            if not soup:
                self.debug_log("ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—", "ERROR")
                return None
            
            # 1. ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
            if index_file or info_file or comments_file:
                self.debug_log("ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
                soup = self.fix_navigation_links_in_vertical_page(
                    soup, 
                    index_file=index_file,
                    info_file=info_file,
                    comments_file=comments_file
                )
            
            # 2. ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®è¿½åŠ ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            additional_links = self.extract_additional_links_from_vertical_page(soup)
            
            # 3. PDFãƒªãƒ³ã‚¯ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«åŒ–
            self.debug_log("PDFãƒªãƒ³ã‚¯ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ã‚’å®Ÿè¡Œä¸­...")
            soup = self.download_and_localize_pdf_links(soup, output_dir, novel_title)
            
            # 4. ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‚’ä¿å­˜
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
            filename = f"{safe_title} - ç¸¦æ›¸ã"
            
            file_path = self.save_complete_page(soup, vertical_url, filename, output_dir, vertical_url)
            
            if file_path:
                self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {file_path}")
                return {
                    'file_path': file_path,
                    'additional_links': additional_links
                }
            return None
            
        except Exception as e:
            self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def save_novel_info_page_with_vertical_link(self, info_url, output_dir, novel_title):
        """å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ï¼ˆç¸¦æ›¸ããƒªãƒ³ã‚¯å¯¾å¿œï¼‰"""
        try:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜é–‹å§‹: {info_url}")
            
            # URLãŒç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            if info_url.startswith('//'):
                info_url = 'https:' + info_url
            elif info_url.startswith('/'):
                info_url = 'https://syosetu.org' + info_url
            
            # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            soup = self.get_page(info_url)
            if not soup:
                self.debug_log("å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—", "ERROR")
                return None
            
            # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
            filename = f"{safe_title} - å°èª¬æƒ…å ±"
            
            file_path = self.save_complete_page(soup, info_url, filename, output_dir, info_url)
            
            if file_path:
                self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {file_path}")
                return file_path
            return None
            
        except Exception as e:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def process_vertical_reading_links(self, soup, output_dir, novel_title, index_file=None, info_file=None, comments_file=None):
        """ç¸¦æ›¸ããƒªãƒ³ã‚¯ã®å‡¦ç†ï¼ˆçµ±åˆæ©Ÿèƒ½ï¼šãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ä¿®æ­£å¯¾å¿œï¼‰"""
        try:
            result = {}
            
            # ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            vertical_link = self.extract_vertical_reading_link(soup)
            if vertical_link:
                self.debug_log(f"ç¸¦æ›¸ããƒªãƒ³ã‚¯ã‚’å‡¦ç†ä¸­: {vertical_link}")
                vertical_result = self.save_vertical_reading_page(
                    vertical_link, 
                    output_dir, 
                    novel_title,
                    index_file=index_file,
                    info_file=info_file,
                    comments_file=comments_file
                )
                if vertical_result:
                    result['vertical_page'] = vertical_result
                    self.debug_log("ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†ï¼ˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ä¿®æ­£æ¸ˆã¿ï¼‰")
            
            # å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            info_link = self.extract_novel_info_link(soup)
            if info_link:
                info_result = self.save_novel_info_page_with_vertical_link(info_link, output_dir, novel_title)
                if info_result:
                    result['info_page'] = info_result
            
            return result if result else None
            
        except Exception as e:
            self.debug_log(f"ç¸¦æ›¸ããƒªãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None
    
    def update_navigation_links_with_vertical_pages(self, soup, index_file, info_file, vertical_file):
        """ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’æ›´æ–°ï¼ˆç¸¦æ›¸ããƒšãƒ¼ã‚¸å¯¾å¿œï¼‰"""
        try:
            # ç›®æ¬¡ãƒªãƒ³ã‚¯ã®æ›´æ–°
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                if text == "ç›®æ¬¡" or ('/novel/' in href and href.endswith('/')):
                    link['href'] = index_file
                    self.debug_log(f"ç›®æ¬¡ãƒªãƒ³ã‚¯æ›´æ–°: {href} -> {index_file}")
                elif text == "ç¸¦æ›¸ã" or 'mode=ss_detail3' in href:
                    link['href'] = vertical_file
                    self.debug_log(f"ç¸¦æ›¸ããƒªãƒ³ã‚¯æ›´æ–°: {href} -> {vertical_file}")
                elif text == "å°èª¬æƒ…å ±" or ('mode=ss_detail' in href and 'mode=ss_detail3' not in href):
                    link['href'] = info_file
                    self.debug_log(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯æ›´æ–°: {href} -> {info_file}")
            
            return soup
            
        except Exception as e:
            self.debug_log(f"ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return soup

    def fix_vertical_links_in_all_pages(self, soup, index_file, info_file, vertical_file):
        """å…¨ãƒšãƒ¼ã‚¸ã®ç¸¦æ›¸ããƒªãƒ³ã‚¯ä¿®æ­£ï¼ˆç›®æ¬¡ãƒ»æ„Ÿæƒ³ãƒ»å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰"""
        try:
            # å…¨ã¦ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’æ›´æ–°
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # ç›®æ¬¡ãƒªãƒ³ã‚¯ã®æ›´æ–°
                if text == "ç›®æ¬¡" or ('/novel/' in href and href.endswith('/')):
                    if index_file:
                        link['href'] = index_file
                        self.debug_log(f"ç›®æ¬¡ãƒªãƒ³ã‚¯æ›´æ–°: {href} -> {index_file}")
                
                # ç¸¦æ›¸ããƒªãƒ³ã‚¯ã®æ›´æ–°
                elif text == "ç¸¦æ›¸ã" or 'mode=ss_detail3' in href:
                    if vertical_file:
                        link['href'] = vertical_file
                        self.debug_log(f"ç¸¦æ›¸ããƒªãƒ³ã‚¯æ›´æ–°: {href} -> {vertical_file}")
                
                # å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã®æ›´æ–°
                elif text == "å°èª¬æƒ…å ±" or ('mode=ss_detail' in href and 'mode=ss_detail3' not in href):
                    if info_file:
                        link['href'] = info_file
                        self.debug_log(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯æ›´æ–°: {href} -> {info_file}")
            
            return soup
            
        except Exception as e:
            self.debug_log(f"å…¨ãƒšãƒ¼ã‚¸ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return soup

    def extract_pdf_links_from_vertical_page(self, soup):
        """ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‹ã‚‰PDFãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        try:
            pdf_links = []
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ã§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šç²¾å¯†ãªæ¡ä»¶ï¼‰
                href_lower = href.lower()
                if href and (
                    'txtdownload' in href_lower or 
                    'pdfdownload' in href_lower or 
                    'epubdownload' in href_lower or
                    ('/conv/pdf/' in href_lower and href_lower.startswith('/conv/')) or
                    ('/conv/txt/' in href_lower and href_lower.startswith('/conv/')) or 
                    ('/conv/epub/' in href_lower and href_lower.startswith('/conv/')) or
                    ('api/download' in href_lower and 'format=' in href_lower)
                ):
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if href.startswith('//'):
                        full_url = 'https:' + href
                    elif href.startswith('/'):
                        full_url = self.base_url + href
                    elif not href.startswith('http'):
                        full_url = urljoin(self.base_url, href)
                    else:
                        full_url = href
                    
                    pdf_links.append(href)  # å…ƒã®hrefã‚’è¿”ã™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
                    self.debug_log(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ç™ºè¦‹: {href}")
            
            return pdf_links
            
        except Exception as e:
            self.debug_log(f"PDFãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return []

    def download_file(self, url, output_dir, filename=None):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰"""
        try:
            self.debug_log(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: å…ƒURL={url}")
            
            # çµ¶å¯¾URLã«å¤‰æ›
            if url.startswith('//'):
                url = 'https:' + url
            elif url.startswith('/'):
                url = self.base_url + url
            elif not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            self.debug_log(f"æ­£è¦åŒ–å¾ŒURL: {url}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
            if not filename:
                parsed = urlparse(url)
                filename = os.path.basename(parsed.path)
                if not filename or '.' not in filename:
                    # URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨æ¸¬
                    if 'txtdownload' in url:
                        if 'code=sjis' in url:
                            filename = "å°èª¬_SJISç‰ˆ.txt"
                        elif 'code=utf8' in url:
                            filename = "å°èª¬_UTF8ç‰ˆ.txt"
                        else:
                            filename = "å°èª¬.txt"
                    elif 'pdfdownload' in url:
                        filename = "å°èª¬.pdf"
                    else:
                        filename = "download_file"
            
            self.debug_log(f"ãƒ•ã‚¡ã‚¤ãƒ«åæ±ºå®š: {filename}")
            
            # å‡ºåŠ›ãƒ‘ã‚¹ã‚’ä½œæˆ
            output_path = os.path.join(output_dir, filename)
            self.debug_log(f"ä¿å­˜å…ˆãƒ‘ã‚¹: {output_path}")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            self.debug_log("HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹...")
            response = self.cloudscraper.get(url, timeout=30)
            self.debug_log(f"HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹: status={response.status_code}, content-length={len(response.content)}")
            response.raise_for_status()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            self.debug_log("ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿é–‹å§‹...")
            with open(output_path, 'wb') as f:
                f.write(response.content)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
            file_size = os.path.getsize(output_path)
            self.debug_log(f"âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {output_path} (ã‚µã‚¤ã‚º: {file_size}ãƒã‚¤ãƒˆ)")
            return output_path
            
        except Exception as e:
            self.debug_log(f"âœ— ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            import traceback
            self.debug_log(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}", "ERROR")
            return None

    def download_and_localize_pdf_links(self, soup, output_dir, novel_title):
        """PDFãƒªãƒ³ã‚¯ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒªãƒ³ã‚¯ã«å¤‰æ›ï¼ˆãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰"""
        try:
            self.debug_log("PDFãƒªãƒ³ã‚¯ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ã‚’é–‹å§‹...")
            
            # PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            pdf_links = self.extract_pdf_links_from_vertical_page(soup)
            self.debug_log(f"æŠ½å‡ºã•ã‚ŒãŸPDFãƒªãƒ³ã‚¯æ•°: {len(pdf_links)}")
            
            if not pdf_links:
                self.debug_log("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return soup
            
            # æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚’è©³ç´°è¡¨ç¤º
            for i, link in enumerate(pdf_links):
                self.debug_log(f"PDFãƒªãƒ³ã‚¯{i+1}: {link}")
            
            download_count = 0
            # å„ãƒªãƒ³ã‚¯ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«åŒ–
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                link_text = link.get_text(strip=True)
                self.debug_log(f"æ¤œæŸ»ä¸­ã®ãƒªãƒ³ã‚¯: href='{href}', text='{link_text}'")
                
                if href in pdf_links:
                    self.debug_log(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒªãƒ³ã‚¯ç™ºè¦‹: {href}")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
                    if 'SJIS' in link_text or 'sjis' in href:
                        filename = f"{novel_title}_SJISç‰ˆ.txt"
                    elif 'UTF' in link_text or 'utf8' in href:
                        filename = f"{novel_title}_UTF8ç‰ˆ.txt"
                    elif 'PDF' in link_text or 'pdf' in href:
                        filename = f"{novel_title}.pdf"
                    else:
                        filename = f"{novel_title}_ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰.txt"
                    
                    self.debug_log(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ: {href} -> {filename}")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    local_path = self.download_file(href, output_dir, filename)
                    
                    if local_path:
                        # ãƒªãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›´æ–°
                        relative_path = os.path.basename(local_path)
                        link['href'] = relative_path
                        download_count += 1
                        self.debug_log(f"âœ“ ãƒªãƒ³ã‚¯ãƒ­ãƒ¼ã‚«ãƒ«åŒ–æˆåŠŸ: {href} -> {relative_path}")
                    else:
                        self.debug_log(f"âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {href}")
            
            self.debug_log(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {download_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")
            return soup
            
        except Exception as e:
            self.debug_log(f"PDFãƒªãƒ³ã‚¯ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            import traceback
            self.debug_log(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}", "ERROR")
            return soup

    def fix_navigation_links_in_vertical_page(self, soup, index_file=None, info_file=None, comments_file=None):
        """ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿®æ­£"""
        try:
            # ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®å…¨ã¦ã®ãƒªãƒ³ã‚¯ã‚’èª¿æŸ»
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                if not href:
                    continue
                
                # ç›®æ¬¡ãƒªãƒ³ã‚¯ã®ä¿®æ­£
                if text == "ç›®æ¬¡" or ('/novel/' in href and href.endswith('/')):
                    if index_file:
                        link['href'] = index_file
                        self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ç›®æ¬¡ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {index_file}")
                
                # å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã®ä¿®æ­£
                elif text == "å°èª¬æƒ…å ±" or ('mode=ss_detail' in href and 'mode=ss_detail3' not in href):
                    if info_file:
                        link['href'] = info_file
                        self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {info_file}")
                
                # æ„Ÿæƒ³ãƒªãƒ³ã‚¯ã®ä¿®æ­£
                elif text == "æ„Ÿæƒ³" or 'mode=review' in href:
                    if comments_file:
                        link['href'] = comments_file
                        self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…æ„Ÿæƒ³ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {comments_file}")
                
                # è©•ä¾¡ã€ã“ã“ã™ãã€èª¤å­—å ±å‘Šãªã©ã®ãƒªãƒ³ã‚¯ã¯ç„¡åŠ¹åŒ–
                elif any(mode in href for mode in ['mode=rating_input', 'mode=ss_detail_like_line', 'mode=correct_view']):
                    link['href'] = 'javascript:void(0);'
                    link['style'] = 'color: #999; cursor: not-allowed; text-decoration: none;'
                    self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ãƒªãƒ³ã‚¯ç„¡åŠ¹åŒ–: {href} ({text})")
            
            return soup
            
        except Exception as e:
            self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return soup

    def save_vertical_reading_page_with_full_fixes(self, vertical_url, output_dir, novel_title, index_file=None, info_file=None, comments_file=None):
        """ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‚’ä¿å­˜ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼šãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ä¿®æ­£ã¨PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å«ã‚€ï¼‰"""
        try:
            self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜é–‹å§‹ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰: {vertical_url}")
            
            # URLãŒç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            if vertical_url.startswith('//'):
                vertical_url = 'https:' + vertical_url
            elif vertical_url.startswith('/'):
                vertical_url = 'https://syosetu.org' + vertical_url
            
            # ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‚’å–å¾—
            soup = self.get_page(vertical_url)
            if not soup:
                self.debug_log("ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—", "ERROR")
                return None
            
            # 1. ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
            soup = self.fix_navigation_links_in_vertical_page(
                soup, 
                index_file=index_file,
                info_file=info_file,
                comments_file=comments_file
            )
            
            # 2. PDFãƒªãƒ³ã‚¯ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«åŒ–
            soup = self.download_and_localize_pdf_links(soup, output_dir, novel_title)
            
            # 3. ç¸¦æ›¸ããƒšãƒ¼ã‚¸å†…ã®è¿½åŠ ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            additional_links = self.extract_additional_links_from_vertical_page(soup)
            
            # 4. ç¸¦æ›¸ããƒšãƒ¼ã‚¸ã‚’ä¿å­˜
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
            filename = f"{safe_title} - ç¸¦æ›¸ã"
            
            file_path = self.save_complete_page(soup, vertical_url, filename, output_dir, vertical_url)
            
            if file_path:
                self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰: {file_path}")
                return {
                    'file_path': file_path,
                    'additional_links': additional_links
                }
            return None
            
        except Exception as e:
            self.debug_log(f"ç¸¦æ›¸ããƒšãƒ¼ã‚¸ä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰: {e}", "ERROR")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    scraper = None
    
    try:
        scraper = HamelnFinalScraper()
        
        print("ãƒãƒ¼ãƒ¡ãƒ«ãƒ³å°èª¬ä¿å­˜ãƒ„ãƒ¼ãƒ«ï¼ˆæœ€çµ‚ç‰ˆï¼‰")
        print("å®Œå…¨ãƒ¢ãƒ¼ãƒ‰ï¼ˆCSSãƒ»ç”»åƒãƒ»JavaScriptå«ã‚€å®Œå…¨ä¿å­˜ï¼‰")
        print("=" * 50)
        
        import sys
        if len(sys.argv) > 1:
            novel_url = sys.argv[1]
            print(f"æŒ‡å®šã•ã‚ŒãŸURL: {novel_url}")
        else:
            novel_url = input("å°èª¬ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
        
        if not novel_url:
            print("URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return
        
        result = scraper.scrape_novel(novel_url)
        if result:
            print(f"\nâœ“ ä¿å­˜å®Œäº†: {result}")
        else:
            print("\nâœ— ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
    except KeyboardInterrupt:
        print("\n\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if scraper:
            scraper.close()

# ç¸¦æ›¸ããƒªãƒ³ã‚¯å‡¦ç†ã‚’HamelnFinalScraperã‚¯ãƒ©ã‚¹ã«è¿½åŠ 
def add_vertical_reading_methods():
    """ç¸¦æ›¸ããƒªãƒ³ã‚¯å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ã‚’HamelnFinalScraperã‚¯ãƒ©ã‚¹ã«è¿½åŠ """
    
if __name__ == "__main__":
    main()
