#!/usr/bin/env python3
"""
ãƒãƒ¼ãƒ¡ãƒ«ãƒ³å°èª¬ä¿å­˜ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ - æœ€çµ‚ç‰ˆ
å®Œå…¨ãªãƒšãƒ¼ã‚¸ä¿å­˜æ©Ÿèƒ½ï¼ˆCSSã€JavaScriptã€ç”»åƒç­‰ã‚’å«ã‚€ï¼‰
"""

import time
import os
import re
import base64
import requests
import cloudscraper
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from urllib.parse import urljoin, urlparse
from PIL import Image
import io
import logging
import traceback
from datetime import datetime
import gzip
import zlib
import brotli
import copy

class HamelnFinalScraper:
    def __init__(self, base_url="https://syosetu.org"):
        self.base_url = base_url
        self.driver = None
        self.cloudscraper = None
        self.session = requests.Session()
        self.debug_mode = True
        
        # ğŸ›ï¸ æ©Ÿèƒ½åˆ¶å¾¡ãƒ•ãƒ©ã‚°ï¼ˆNortonæ¤œå‡ºå•é¡Œè§£æ±ºã«ã‚ˆã‚Šã€æ–°æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–ï¼‰
        self.enable_novel_info_saving = True   # å°èª¬æƒ…å ±ä¿å­˜æ©Ÿèƒ½
        self.enable_comments_saving = True     # æ„Ÿæƒ³ä¿å­˜æ©Ÿèƒ½
        
        self.setup_logging()
        self.setup_scrapers()
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
        logging.basicConfig(
            level=logging.DEBUG if self.debug_mode else logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('hameln_scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–é–‹å§‹")
        
    def debug_log(self, message, level="INFO"):
        """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚¢ãƒ—ãƒªå†…è¡¨ç¤ºï¼‹å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼‰"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        formatted_message = f"[{timestamp}] {level}: {message}"
        print(formatted_message)
        
        # å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å‡ºåŠ›
        try:
            log_file = "hameln_debug.log"
            with open(log_file, 'a', encoding='utf-8') as f:
                full_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{full_timestamp}] {level}: {message}\n")
        except Exception as e:
            print(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        if level == "ERROR":
            self.logger.error(message)
        elif level == "WARNING":
            self.logger.warning(message)
        elif level == "DEBUG":
            self.logger.debug(message)
        else:
            self.logger.info(message)
        
    def setup_scrapers(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’è¨­å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        try:
            self.debug_log("CloudScraperåˆæœŸåŒ–é–‹å§‹")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒªã‚¹ãƒˆ
            self.user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            self.current_ua_index = 0
            
            # CloudScraperè¨­å®šï¼ˆã‚ˆã‚Šé€²æ­©çš„ãªè¨­å®šï¼‰
            self.cloudscraper = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'desktop': True
                },
                delay=10,  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶
                debug=False
            )
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
            self.cloudscraper.headers.update({
                'User-Agent': self.user_agents[0],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            })
            
            self.debug_log("CloudScraperè¨­å®šå®Œäº†")
            
            # Selenium/ChromeåˆæœŸåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆCloudScraperã®ã¿ä½¿ç”¨ï¼‰
            self.debug_log("Chrome/ChromiumãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€CloudScraperã®ã¿ä½¿ç”¨")
            self.driver = None
            
        except Exception as e:
            self.debug_log(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            self.debug_log(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}", "ERROR")
            self.driver = None
            
    def rotate_user_agent(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
        new_ua = self.user_agents[self.current_ua_index]
        self.cloudscraper.headers.update({'User-Agent': new_ua})
        self.debug_log(f"User-Agentã‚’å¤‰æ›´: {new_ua[:50]}...")
        return new_ua
    
    def decompress_response(self, response):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®åœ§ç¸®è§£å‡å‡¦ç†ï¼ˆCloudScraperã®è‡ªå‹•å‡¦ç†ã‚’æ´»ç”¨ï¼‰"""
        # CloudScraperã¯è‡ªå‹•çš„ã«åœ§ç¸®ã‚’è§£å‡ã™ã‚‹ãŸã‚ã€response.textã‚’ç›´æ¥ä½¿ç”¨
        try:
            html_content = response.text
            self.debug_log(f"CloudScraperã§å–å¾—ã•ã‚ŒãŸHTMLãƒ†ã‚­ã‚¹ãƒˆé•·: {len(html_content)}")
            return html_content
        except Exception as e:
            self.debug_log(f"HTMLãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•è§£å‡ã‚’è©¦ã™
            content = response.content
            encoding = response.headers.get('content-encoding', '').lower()
            
            self.debug_log(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£å‡é–‹å§‹ - Content-Encoding: {encoding}")
            
            try:
                if encoding == 'gzip':
                    content = gzip.decompress(content)
                elif encoding == 'deflate':
                    content = zlib.decompress(content)
                elif encoding == 'br':
                    content = brotli.decompress(content)
                
                return content.decode('utf-8')
            except Exception as decode_error:
                self.debug_log(f"æ‰‹å‹•è§£å‡ã‚‚å¤±æ•—: {decode_error}", "ERROR")
                return ""
    
    def get_page(self, url, retry_count = 3):
        """ãƒšãƒ¼ã‚¸å–å¾—: CloudScraper â†’ Selenium ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        self.debug_log(f"ãƒšãƒ¼ã‚¸å–å¾—é–‹å§‹: {url}")
        
        # URLå½¢å¼ã®æ¤œè¨¼
        if not url.startswith('http'):
            self.debug_log(f"ç„¡åŠ¹ãªURLå½¢å¼: {url}", "ERROR")
            return None
        
        # ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã‚’è¨­ã‘ã¦ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è»½æ¸›
        time.sleep(2)  # ãƒ™ãƒ¼ã‚·ãƒƒã‚¯ãªå¾…æ©Ÿæ™‚é–“
        
        # ã¾ãšCloudScraperã‚’è©¦ã™ï¼ˆé«˜é€Ÿï¼‰
        for attempt in range(retry_count):
            try:
                if attempt > 0:
                    self.debug_log(f"CloudScraperå†è©¦è¡Œ {attempt + 1}/{retry_count}")
                    # User-Agentã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                    self.rotate_user_agent()
                    # å¾…æ©Ÿæ™‚é–“ã‚’æ¼¸é€²çš„ã«å¢—ã‚„ã™
                    wait_time = 5 + (attempt * 3)
                    self.debug_log(f"ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã‚’å–ã‚Šã¾ã™: {wait_time}ç§’")
                    time.sleep(wait_time)
                else:
                    self.debug_log("CloudScraperã§åˆå›è©¦è¡Œä¸­...")
                    
                response = self.cloudscraper.get(url, timeout=30)
                
                self.debug_log(f"CloudScraperãƒ¬ã‚¹ãƒãƒ³ã‚¹: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹={response.status_code}, ã‚µã‚¤ã‚º={len(response.content)}bytes")
                
                if response.status_code == 403:
                    self.debug_log("CloudScraper: 403 Forbidden - botæ¤œçŸ¥ã«ã‚ˆã‚Šæ‹’å¦", "WARNING")
                    if attempt < retry_count - 1:
                        continue  # å†è©¦è¡Œ
                elif response.status_code == 404:
                    self.debug_log("CloudScraper: 404 Not Found - ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ã¾ã›ã‚“", "WARNING")
                    return None  # 404ã¯å†è©¦è¡Œã—ãªã„
                elif response.status_code == 503:
                    self.debug_log("CloudScraper: 503 Service Unavailable - ã‚µãƒ¼ãƒãƒ¼ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä¸­", "WARNING")
                    if attempt < retry_count - 1:
                        continue  # å†è©¦è¡Œ
                elif response.status_code == 429:
                    self.debug_log("CloudScraper: 429 Too Many Requests - ãƒ¬ãƒ¼ãƒˆåˆ¶é™", "WARNING")
                    if attempt < retry_count - 1:
                        wait_time = 30 + (attempt * 15)  # ã‚ˆã‚Šé•·ã„å¾…æ©Ÿ
                        self.debug_log(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãŸã‚ {wait_time}ç§’å¾…æ©Ÿ...")
                        time.sleep(wait_time)
                        continue
                else:
                    response.raise_for_status()
                
                # åœ§ç¸®è§£å‡å‡¦ç†
                html_content = self.decompress_response(response)
                
                if not html_content:
                    self.debug_log("HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã§ã™", "ERROR")
                    if attempt < retry_count - 1:
                        continue
                    return None
                
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # ãƒšãƒ¼ã‚¸å†…å®¹ã®è©³ç´°åˆ†æ
                self.analyze_page_content(soup, "CloudScraper")
                
                # ãƒšãƒ¼ã‚¸ãŒæ­£å¸¸ã«å–å¾—ã§ããŸã‹ãƒã‚§ãƒƒã‚¯
                if self.validate_page(soup, url):
                    self.debug_log(f"CloudScraperã§å–å¾—æˆåŠŸ: {url}")
                    return soup
                else:
                    self.debug_log("CloudScraperã§å–å¾—ã—ãŸãƒšãƒ¼ã‚¸ãŒç„¡åŠ¹", "WARNING")
                    if attempt < retry_count - 1:
                        continue  # å†è©¦è¡Œ
                    else:
                        self.debug_log("CloudScraperã§ã®å…¨è©¦è¡ŒãŒå¤±æ•—ã€Seleniumã§å†è©¦è¡Œ...", "WARNING")
                        break
                        
            except requests.exceptions.Timeout:
                self.debug_log("CloudScraperã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", "ERROR")
                if attempt < retry_count - 1:
                    continue
            except requests.exceptions.ConnectionError:
                self.debug_log("CloudScraperæ¥ç¶šã‚¨ãƒ©ãƒ¼", "ERROR")
                if attempt < retry_count - 1:
                    continue
            except Exception as e:
                self.debug_log(f"CloudScraperã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
                self.debug_log(f"CloudScraperã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}", "DEBUG")
                if attempt < retry_count - 1:
                    continue
        
        # Seleniumã§å†è©¦è¡Œ
        if self.driver:
            for attempt in range(retry_count):
                try:
                    self.debug_log(f"Seleniumè©¦è¡Œ {attempt + 1}/{retry_count}")
                    
                    self.driver.get(url)
                    
                    # Cloudflareã®èªè¨¼ãƒã‚§ãƒƒã‚¯å¾…æ©Ÿï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·ï¼‰
                    try:
                        WebDriverWait(self.driver, 15).until(
                            lambda driver: "Cloudflare" not in driver.title and "Just a moment" not in driver.title
                        )
                        self.debug_log("Cloudflareèªè¨¼ã‚¯ãƒªã‚¢")
                    except:
                        self.debug_log("Cloudflareèªè¨¼å¾…æ©Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€ç¶šè¡Œ...", "WARNING")
                    
                    # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
                    time.sleep(8)
                    
                    # ç”»åƒã®é…å»¶èª­ã¿è¾¼ã¿ã‚’å¼·åˆ¶å®Ÿè¡Œï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§ç™ºç«ï¼‰
                    try:
                        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)
                        self.driver.execute_script("window.scrollTo(0, 0);")
                        time.sleep(2)
                    except Exception as e:
                        print(f"ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                    
                    # ãƒšãƒ¼ã‚¸å†…å®¹ã®è©³ç´°åˆ†æ
                    self.analyze_page_content(soup, "Selenium")
                    
                    if self.validate_page(soup, url):
                        self.debug_log(f"Seleniumã§å–å¾—æˆåŠŸ: {url}")
                        return soup
                    else:
                        self.debug_log(f"ç„¡åŠ¹ãªãƒšãƒ¼ã‚¸ã€å†è©¦è¡Œ {attempt + 1}/{retry_count}", "WARNING")
                        time.sleep(3)
                        
                except Exception as e:
                    self.debug_log(f"Seleniumè©¦è¡Œã‚¨ãƒ©ãƒ¼ {attempt + 1}/{retry_count}: {e}", "ERROR")
                    self.debug_log(f"Seleniumã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}", "DEBUG")
                    if attempt < retry_count - 1:
                        time.sleep(5)
        else:
            self.debug_log("Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“", "ERROR")
        
        self.debug_log(f"å…¨ã¦ã®æ–¹æ³•ã§å¤±æ•—: {url}", "ERROR")
        return None
        
    def analyze_page_content(self, soup, method):
        """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’è©³ç´°åˆ†æ"""
        if not soup:
            self.debug_log(f"{method}: soupãŒNone", "ERROR")
            return
            
        title = soup.title.string if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        body_length = len(soup.get_text()) if soup else 0
        
        self.debug_log(f"{method}å–å¾—ãƒšãƒ¼ã‚¸åˆ†æ:")
        self.debug_log(f"  - ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        self.debug_log(f"  - æœ¬æ–‡é•·: {body_length}æ–‡å­—")
        
        # HTMLã®æœ€åˆã®500æ–‡å­—ã‚’ç¢ºèª
        html_snippet = str(soup)[:500]
        self.debug_log(f"  - HTMLå…ˆé ­éƒ¨åˆ†: {html_snippet}...")
        
        # ç‰¹å®šã®è¦ç´ ã®å­˜åœ¨ç¢ºèª
        elements_check = {
            'h1': len(soup.find_all('h1')),
            'div': len(soup.find_all('div')),
            'a': len(soup.find_all('a')),
            'novel_body': len(soup.find_all('div', class_='novel_body')),
            'novel_view': len(soup.find_all('div', class_='novel_view'))
        }
        
        for element, count in elements_check.items():
            self.debug_log(f"  - {element}è¦ç´ æ•°: {count}")
        
        # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
        error_indicators = ['404', 'Error', 'Forbidden', 'Access Denied', 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹']
        for indicator in error_indicators:
            if indicator in title:
                self.debug_log(f"  - ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸æ¤œå‡º: {indicator}", "WARNING")
                
    def validate_page(self, soup, url):
        """ãƒšãƒ¼ã‚¸ãŒæ­£å¸¸ã«å–å¾—ã§ããŸã‹ãƒã‚§ãƒƒã‚¯"""
        if not soup:
            self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: soupãŒNone", "ERROR")
            return False
            
        title = soup.title.string if soup.title else ""
        
        # Cloudflareã®èªè¨¼ãƒšãƒ¼ã‚¸ãƒã‚§ãƒƒã‚¯
        if 'Cloudflare' in title or 'Just a moment' in title:
            self.debug_log(f"ãƒšãƒ¼ã‚¸æ¤œè¨¼: Cloudflareèªè¨¼ãƒšãƒ¼ã‚¸æ¤œå‡º - {title}", "ERROR")
            return False
            
        # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒã‚§ãƒƒã‚¯
        error_keywords = ['404', 'Error', 'Forbidden', 'Access Denied', 'Not Found', 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹']
        for keyword in error_keywords:
            if keyword in title:
                self.debug_log(f"ãƒšãƒ¼ã‚¸æ¤œè¨¼: ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸æ¤œå‡º - {keyword} in {title}", "ERROR")
                return False
            
        # æœ€ä½é™ã®å†…å®¹ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        text_content = soup.get_text(strip=True)
        
        # åŸºæœ¬çš„ãªHTMLæ§‹é€ ãƒã‚§ãƒƒã‚¯ï¼ˆbodyã‚¿ã‚°ãŒãªã„å ´åˆã‚‚è¨±å¯ï¼‰
        if not soup.find('body'):
            self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: bodyã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆSPAã®å¯èƒ½æ€§ï¼‰", "WARNING")
            # bodyã‚¿ã‚°ãŒãªãã¦ã‚‚ã€æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Œã°ç¶šè¡Œ
            if len(text_content) > 500:  # ååˆ†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆ
                self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: bodyã‚¿ã‚°ãªã—ã§ã‚‚ååˆ†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ãŸã‚ç¶šè¡Œ", "INFO")
            else:
                self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: bodyã‚¿ã‚°ãªã—ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚ä¸ååˆ†", "ERROR")
                return False
        elif len(text_content) < 100:
            self.debug_log(f"ãƒšãƒ¼ã‚¸æ¤œè¨¼: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå°‘ãªã™ãã¾ã™ ({len(text_content)}æ–‡å­—)", "WARNING")
            return False
        
        # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ãƒã‚§ãƒƒã‚¯
        if 'syosetu.org' in url:
            # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ç¢ºèª
            if not any(indicator in text_content for indicator in ['ãƒãƒ¼ãƒ¡ãƒ«ãƒ³', 'syosetu', 'å°èª¬']):
                self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ãƒšãƒ¼ã‚¸ã§ã¯ãªã„å¯èƒ½æ€§", "WARNING")
                return False
        
        self.debug_log("ãƒšãƒ¼ã‚¸æ¤œè¨¼: æ­£å¸¸ãªãƒšãƒ¼ã‚¸")
        return True
            
    def download_resource(self, url, base_path):
        """ãƒªã‚½ãƒ¼ã‚¹ï¼ˆç”»åƒã€CSSã€JSç­‰ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            # çµ¶å¯¾URLã«å¤‰æ›
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)
            if not filename or '.' not in filename:
                # æ‹¡å¼µå­ãŒãªã„å ´åˆã¯æ¨æ¸¬
                if 'css' in url:
                    filename = f"style_{hash(url) % 10000}.css"
                elif 'js' in url:
                    filename = f"script_{hash(url) % 10000}.js"
                elif any(ext in url for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg']):
                    ext = url.split('.')[-1].split('?')[0]
                    filename = f"image_{hash(url) % 10000}.{ext}"
                else:
                    filename = f"resource_{hash(url) % 10000}.txt"
            
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ
            local_path = os.path.join(base_path, filename)
            
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿è­·ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç ´æé˜²æ­¢ï¼‰
            if os.path.exists(local_path):
                print(f"æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼ˆä¸Šæ›¸ãé˜²æ­¢ï¼‰: {filename}")
                return filename
            
            # ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = self.cloudscraper.get(url, timeout=10)
            response.raise_for_status()
            
            # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è€ƒæ…®
            if filename.endswith('.css'):
                response.encoding = 'utf-8'
                with open(local_path, 'w', encoding='utf-8') as f:
                    f.write(response.text)
            else:
                with open(local_path, 'wb') as f:
                    f.write(response.content)
            
            print(f"ãƒªã‚½ãƒ¼ã‚¹ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            print(f"ãƒªã‚½ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return url  # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®URLã‚’è¿”ã™
            
    def adjust_resource_paths_only(self, soup, output_dir):
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã›ãšã€ãƒ‘ã‚¹èª¿æ•´ã®ã¿å®Ÿè¡Œï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰"""
        resources_dir_name = "resources"
        
        # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®hrefå±æ€§ã‚’èª¿æ•´
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href and href.startswith(('http', '//')):
                filename = os.path.basename(urlparse(href).path)
                if filename:
                    link['href'] = f"./{resources_dir_name}/{filename}"
        
        # JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã®srcå±æ€§ã‚’èª¿æ•´
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src and src.startswith(('http', '//')):
                filename = os.path.basename(urlparse(src).path)
                if filename:
                    script['src'] = f"./{resources_dir_name}/{filename}"
        
        # ç”»åƒã®srcå±æ€§ã‚’èª¿æ•´
        for img in soup.find_all('img', src=True):
            src = img.get('src')
            if src and src.startswith(('http', '//')):
                filename = os.path.basename(urlparse(src).path)
                if filename:
                    img['src'] = f"./{resources_dir_name}/{filename}"
        
        print(f"ğŸ“‚ ãƒ‘ã‚¹èª¿æ•´ã®ã¿å®Œäº†ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return soup
    
    def process_html_resources(self, soup, base_path):
        """HTMLã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‡¦ç†ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ä¿å­˜ï¼‰"""
        # ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ä½¿ç”¨
        resources_dir_name = getattr(self, 'browser_compatible_name', 'resources')
        resources_dir = os.path.join(base_path, resources_dir_name)
        os.makedirs(resources_dir, exist_ok=True)
        
        print("=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ãƒªã‚½ãƒ¼ã‚¹ä¿å­˜é–‹å§‹ ===")
        
        # 1. CSS ãƒªãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆã‚¹ã‚¿ã‚¤ãƒ«ã‚·ãƒ¼ãƒˆï¼‰
        css_links = soup.find_all('link', {'rel': 'stylesheet'})
        print(f"CSS ãƒªãƒ³ã‚¯æ•°: {len(css_links)}")
        for link in css_links:
            href = link.get('href')
            if href:
                print(f"CSSå‡¦ç†ä¸­: {href}")
                local_file = self.download_and_process_css(href, resources_dir)
                link['href'] = f"./{resources_dir_name}/{local_file}"
        
        # 2. ã™ã¹ã¦ã®å¤–éƒ¨ãƒªãƒ³ã‚¯ãƒªã‚½ãƒ¼ã‚¹ï¼ˆãƒ•ã‚¡ãƒ“ã‚³ãƒ³ã€ã‚¢ã‚¤ã‚³ãƒ³ç­‰ï¼‰
        icon_rels = ['icon', 'shortcut icon', 'apple-touch-icon', 'apple-touch-icon-precomposed', 
                     'mask-icon', 'fluid-icon']
        icon_links = soup.find_all('link', {'rel': icon_rels})
        print(f"ã‚¢ã‚¤ã‚³ãƒ³ãƒªãƒ³ã‚¯æ•°: {len(icon_links)}")
        for link in icon_links:
            href = link.get('href')
            if href:
                print(f"ã‚¢ã‚¤ã‚³ãƒ³å‡¦ç†ä¸­: {href}")
                local_file = self.download_resource(href, resources_dir)
                if local_file != href:
                    link['href'] = f"./{resources_dir_name}/{local_file}"
        
        # 3. JavaScript ãƒ•ã‚¡ã‚¤ãƒ«
        scripts = soup.find_all('script', {'src': True})
        print(f"JavaScript ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(scripts)}")
        for script in scripts:
            src = script.get('src')
            if src:
                print(f"JSå‡¦ç†ä¸­: {src}")
                local_file = self.download_resource(src, resources_dir)
                if local_file != src:
                    script['src'] = f"./{resources_dir_name}/{local_file}"
        
        # 4. å…¨ã¦ã®ç”»åƒï¼ˆimg ã‚¿ã‚°ï¼‰
        images = soup.find_all('img', {'src': True})
        print(f"ç”»åƒæ•°: {len(images)}")
        for img in images:
            src = img.get('src')
            if src:
                print(f"ç”»åƒå‡¦ç†ä¸­: {src}")
                local_file = self.download_resource(src, resources_dir)
                if local_file != src:
                    img['src'] = f"./{resources_dir_name}/{local_file}"
        
        # 5. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ã®èƒŒæ™¯ç”»åƒ
        styled_elements = soup.find_all(style=True)
        print(f"ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«è¦ç´ æ•°: {len(styled_elements)}")
        for element in styled_elements:
            style = element.get('style')
            if 'url(' in style:
                import re
                urls = re.findall(r'url\([\'"]?([^\'"]+)[\'"]?\)', style)
                for url in urls:
                    print(f"ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ç”»åƒå‡¦ç†ä¸­: {url}")
                    local_file = self.download_resource(url, resources_dir)
                    if local_file != url:
                        style = style.replace(url, f"./{resources_dir_name}/{local_file}")
                element['style'] = style
        
        # 6. ãã®ä»–ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚½ãƒ¼ã‚¹ï¼ˆvideo, audio, embed, objectï¼‰
        media_tags = ['video', 'audio', 'embed', 'object', 'source']
        for tag_name in media_tags:
            elements = soup.find_all(tag_name)
            for element in elements:
                # src, data, href å±æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                for attr in ['src', 'data', 'href', 'poster']:
                    url = element.get(attr)
                    if url and url.startswith(('http', '//')):
                        print(f"{tag_name}ã®ãƒªã‚½ãƒ¼ã‚¹å‡¦ç†ä¸­: {url}")
                        local_file = self.download_resource(url, resources_dir)
                        if local_file != url:
                            element[attr] = f"./{resources_dir_name}/{local_file}"
        
        # 7. CSSå†…ã®@importæ–‡ã‚„font-faceç­‰ã®å‡¦ç†ã‚’å¼·åŒ–
        for style_tag in soup.find_all('style'):
            if style_tag.string:
                css_content = style_tag.string
                # @importæ–‡ã‚’å‡¦ç†
                import re
                imports = re.findall(r'@import\s+[\'"]([^\'"]+)[\'"]', css_content)
                for import_url in imports:
                    print(f"CSS @importå‡¦ç†ä¸­: {import_url}")
                    local_file = self.download_and_process_css(import_url, resources_dir)
                    css_content = css_content.replace(import_url, f"./{resources_dir_name}/{local_file}")
                
                # url()å‚ç…§ã‚’å‡¦ç†ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ãƒ»çµ¶å¯¾ãƒ‘ã‚¹ä¸¡æ–¹ã«å¯¾å¿œï¼‰
                urls = re.findall(r'url\([\'"]?([^\'"]+)[\'"]?\)', css_content)
                for url in urls:
                    # çµ¶å¯¾URLãƒ»ç›¸å¯¾URLãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã™ã¹ã¦ã‚’å‡¦ç†
                    if url.startswith(('http', '//', '/')):
                        # çµ¶å¯¾URLã®å ´åˆ
                        print(f"CSSå†…ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {self.base_url if url.startswith('/') and not url.startswith('//') else 'https:' if url.startswith('//') else ''}{url}")
                        local_file = self.download_resource(url, resources_dir)
                        if local_file != url:
                            # CSSå†…ã®ç›¸å¯¾ãƒ‘ã‚¹ãƒ»çµ¶å¯¾ãƒ‘ã‚¹ä¸¡æ–¹ã«å¯¾å¿œ
                            css_content = css_content.replace(f"url({url})", f"url({local_file})")
                            css_content = css_content.replace(f"url('{url}')", f"url('{local_file}')")
                            css_content = css_content.replace(f'url("{url}")', f'url("{local_file}")')
                    elif url and not url.startswith(('data:', '#', './')):
                        # ç›¸å¯¾ãƒ•ã‚¡ã‚¤ãƒ«åã®å ´åˆï¼ˆbanner.pngç­‰ï¼‰- æ—¢ã«ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿
                        full_url = f"https://img.syosetu.org/image/{url}"
                        print(f"CSSå†…ç›¸å¯¾ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {full_url}")
                        local_file = self.download_resource(full_url, resources_dir)
                        if local_file and local_file != url:
                            # ç›¸å¯¾ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç½®æ›
                            css_content = css_content.replace(f"url({url})", f"url({local_file})")
                            css_content = css_content.replace(f"url('{url}')", f"url('{local_file}')")
                            css_content = css_content.replace(f'url("{url}")', f'url("{local_file}")')
                            print(f"CSSå†…ç”»åƒãƒ‘ã‚¹æ›´æ–°: {url} -> {local_file}")
                
                style_tag.string = css_content
        
        # 8. data-srcå±æ€§ï¼ˆé…å»¶èª­ã¿è¾¼ã¿ç”»åƒï¼‰ã‚‚å‡¦ç†
        lazy_images = soup.find_all(attrs={'data-src': True})
        print(f"é…å»¶èª­ã¿è¾¼ã¿ç”»åƒæ•°: {len(lazy_images)}")
        for img in lazy_images:
            data_src = img.get('data-src')
            if data_src:
                print(f"é…å»¶èª­ã¿è¾¼ã¿ç”»åƒå‡¦ç†ä¸­: {data_src}")
                local_file = self.download_resource(data_src, resources_dir)
                if local_file != data_src:
                    img['data-src'] = f"./{resources_dir_name}/{local_file}"
                    # srcsetã‚„data-srcsetå±æ€§ã‚‚å­˜åœ¨ã™ã‚‹å ´åˆã¯å‡¦ç†
                    if img.get('data-srcset'):
                        img['data-srcset'] = f"./{resources_dir_name}/{local_file}"
                    # é…å»¶èª­ã¿è¾¼ã¿ç”»åƒã‚’srcã«ã‚‚è¨­å®š
                    if not img.get('src'):
                        img['src'] = f"./{resources_dir_name}/{local_file}"
        
        # 9. ãã®ä»–ã®ç”»åƒå±æ€§ã‚‚å‡¦ç†ï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®å±æ€§ï¼‰
        for attr in ['data-original', 'data-lazy-src', 'data-echo']:
            attr_images = soup.find_all(attrs={attr: True})
            print(f"{attr}å±æ€§ç”»åƒæ•°: {len(attr_images)}")
            for img in attr_images:
                img_src = img.get(attr)
                if img_src:
                    print(f"{attr}å±æ€§ç”»åƒå‡¦ç†ä¸­: {img_src}")
                    local_file = self.download_resource(img_src, resources_dir)
                    if local_file != img_src:
                        img[attr] = f"./{resources_dir_name}/{local_file}"
        
        print("=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ãƒªã‚½ãƒ¼ã‚¹ä¿å­˜å®Œäº† ===")
        return soup
    
    def fix_local_navigation_links(self, soup, chapter_mapping, current_chapter_url, index_file = None, novel_info_file = None, comments_file = None):
        """ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã«ä¿®æ­£ï¼ˆå¼·åŒ–ç‰ˆ + å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³å¯¾å¿œï¼‰"""
        print("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç”¨ã«ä¿®æ­£ä¸­...")
        
        # 1. ç›®æ¬¡ãƒªãƒ³ã‚¯ã®ä¿®æ­£ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
        index_patterns = [
            # åŸºæœ¬çš„ãªç›®æ¬¡URLãƒ‘ã‚¿ãƒ¼ãƒ³
            lambda tag: tag.name == 'a' and tag.get('href') and '/novel/' in tag.get('href') and tag.get('href').endswith('/'),
            # å®Œå…¨URLå½¢å¼
            lambda tag: tag.name == 'a' and tag.get('href') == 'https://syosetu.org/novel/378070/',
            # ç›®æ¬¡ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒªãƒ³ã‚¯
            lambda tag: tag.name == 'a' and 'ç›®æ¬¡' in tag.get_text() and 'syosetu.org' in str(tag.get('href', ''))
        ]
        
        if index_file:
            for pattern in index_patterns:
                links = soup.find_all(pattern)
                for link in links:
                    old_href = link.get('href')
                    if old_href and old_href != index_file:
                        link['href'] = index_file
                        print(f"ç›®æ¬¡ãƒªãƒ³ã‚¯ä¿®æ­£: {old_href} -> {index_file}")
        
        # 2. ç« é–“ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã®ä¿®æ­£
        # å…¨ã¦ã®aã‚¿ã‚°ã‚’èª¿æŸ»ã—ã¦ã€ç« URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href')
            if not href:
                continue
            
            # çµ¶å¯¾URLã«æ­£è¦åŒ–
            normalized_href = href
            if href.startswith('/'):
                normalized_href = self.base_url + href
            elif href.startswith('./') and href.endswith('.html'):
                # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã‚’çµ¶å¯¾URLã«å¤‰æ›
                relative_part = href[2:]
                try:
                    novel_id = current_chapter_url.split('/')[-2]
                    normalized_href = f"{self.base_url}/novel/{novel_id}/{relative_part}"
                except:
                    continue
            elif not href.startswith('http') and not (href.endswith('.html') and any(char in href for char in ['ç¬¬', 'è©±'])):
                continue
            
            # ãƒãƒ£ãƒ—ã‚¿ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if normalized_href in chapter_mapping:
                local_file = chapter_mapping[normalized_href]
                if href != local_file:
                    link['href'] = local_file
                    print(f"ç« ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {local_file}")
            
            # çŸ­ç¸®å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚‚å‡¦ç†ï¼ˆç¬¬1è©±.htmlç­‰ï¼‰
            elif href.endswith('.html') and any(char in href for char in ['ç¬¬', 'è©±']):
                print(f"çŸ­ç¸®å½¢å¼ãƒªãƒ³ã‚¯å€™è£œæ¤œå‡º: {href}")
                # çŸ­ç¸®å½¢å¼ã‹ã‚‰ç« ç•ªå·ã‚’æŠ½å‡ºã—ã¦ãƒãƒƒãƒãƒ³ã‚°
                import re
                chapter_match = re.search(r'ç¬¬(\d+)è©±', href)
                if chapter_match:
                    chapter_num = chapter_match.group(1)
                    print(f"çŸ­ç¸®ãƒªãƒ³ã‚¯æ¤œå‡º: {href}, ç« ç•ªå·: {chapter_num}")
                    
                    # chapter_mappingã‹ã‚‰ãƒãƒƒãƒã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¢ã™
                    found_mapping = False
                    for url, filename in chapter_mapping.items():
                        # URLã‹ã‚‰ç« ç•ªå·ã‚’æŠ½å‡º
                        url_match = re.search(r'/(\d+)\.html', url)
                        if url_match and url_match.group(1) == chapter_num:
                            print(f"ãƒãƒƒãƒ”ãƒ³ã‚°ç™ºè¦‹: URL={url}, ç« ç•ªå·={url_match.group(1)}, ãƒ•ã‚¡ã‚¤ãƒ«å={filename}")
                            link['href'] = filename
                            print(f"çŸ­ç¸®ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {filename}")
                            found_mapping = True
                            break
                    
                    if not found_mapping:
                        # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã¯ç„¡åŠ¹åŒ–
                        print(f"å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯: {href} -> ç„¡åŠ¹åŒ–")
                        link['href'] = 'javascript:void(0);'
                        link['class'] = link.get('class', []) + ['disabled']
                        link['style'] = 'color: #999; cursor: not-allowed; text-decoration: none;'
        
        # 3. ç©ºãƒªãƒ³ã‚¯ã‚„ç„¡åŠ¹ãªãƒªãƒ³ã‚¯ã®å‡¦ç†
        empty_links = soup.find_all('a', href='#')
        for link in empty_links:
            link_text = link.get_text().strip()
            if link_text == 'Ã—':
                # ã€ŒÃ—ã€ãƒªãƒ³ã‚¯ã¯ãã®ã¾ã¾ä¿æŒï¼ˆå…ƒã®çŠ¶æ…‹ã‚’ç¶­æŒï¼‰
                print(f"Ã—ãƒªãƒ³ã‚¯ä¿æŒ: {link_text}")
            elif 'æ¬¡ã®è©±' in link_text:
                # ç„¡åŠ¹ãªã€Œæ¬¡ã®è©±ã€ãƒªãƒ³ã‚¯ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯å¤‰æ›´ã—ãªã„ï¼‰
                link['href'] = 'javascript:void(0);'
                link['class'] = link.get('class', []) + ['disabled']
                link['style'] = 'color: #999; cursor: not-allowed; text-decoration: none;'
                print(f"ç„¡åŠ¹ãªæ¬¡ã®è©±ãƒªãƒ³ã‚¯ä¿®æ­£: {link_text} -> ç„¡åŠ¹åŒ–")
        
        # 4. ğŸ†• å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ä¿®æ­£
        if novel_info_file or comments_file:
            info_and_comment_links = soup.find_all('a', href=True)
            for link in info_and_comment_links:
                href = link.get('href')
                if not href:
                    continue
                
                # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ä¿®æ­£
                if novel_info_file and ('mode=ss_detail' in href or 'å°èª¬æƒ…å ±' in link.get_text()):
                    link['href'] = novel_info_file
                    print(f"å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {novel_info_file}")
                
                # æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ä¿®æ­£
                elif comments_file and ('mode=review' in href or 'æ„Ÿæƒ³' in link.get_text()):
                    link['href'] = comments_file
                    print(f"æ„Ÿæƒ³ãƒªãƒ³ã‚¯ä¿®æ­£: {href} -> {comments_file}")
        
        return soup
    
    def download_and_process_css(self, url, resources_dir):
        """CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…éƒ¨ã®ç”»åƒå‚ç…§ã‚‚å‡¦ç†ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        try:
            # çµ¶å¯¾URLã«å¤‰æ›
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)
            if not filename or '.' not in filename:
                filename = f"style_{hash(url) % 10000}.css"
            
            print(f"CSSè©³ç´°å‡¦ç†ä¸­: {url}")
            
            # CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = self.cloudscraper.get(url, timeout=10)
            response.raise_for_status()
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«è¨­å®š
            response.encoding = 'utf-8'
            css_content = response.text
            
            # CSSå†…ã®ã™ã¹ã¦ã®ãƒªã‚½ãƒ¼ã‚¹å‚ç…§ã‚’å‡¦ç†
            import re
            
            # 1. url()å‚ç…§ã‚’å‡¦ç†ï¼ˆèƒŒæ™¯ç”»åƒã€ãƒ•ã‚©ãƒ³ãƒˆç­‰ï¼‰- å®Œå…¨ãªãƒãƒƒãƒãƒ³ã‚°ã§å‡¦ç†
            import re
            
            def replace_url_func(match):
                full_match = match.group(0)  # url(...) å…¨ä½“
                img_url = match.group(1)     # URLéƒ¨åˆ†ã®ã¿
                
                if img_url.startswith('data:'):
                    return full_match  # ãƒ‡ãƒ¼ã‚¿URLã¯ãã®ã¾ã¾
                
                original_img_url = img_url
                
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                if not img_url.startswith('http'):
                    if img_url.startswith('//'):
                        img_url = 'https:' + img_url
                    elif img_url.startswith('/'):
                        base_domain = '/'.join(url.split('/')[:3])
                        img_url = base_domain + img_url
                    else:
                        # ç›¸å¯¾ãƒ‘ã‚¹
                        base_css_url = '/'.join(url.split('/')[:-1])
                        img_url = urljoin(base_css_url + '/', img_url)
                
                # URLæ­£è¦åŒ–
                cleaned_url = img_url.split(')')[0]
                if '?' in cleaned_url:
                    cleaned_url = cleaned_url.split('?')[0]
                
                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                print(f"CSSå†…ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {cleaned_url}")
                local_img = self.download_resource(cleaned_url, resources_dir)
                if local_img != cleaned_url:
                    browser_compatible_path = f"./{local_img}"
                    print(f"CSSå†…ãƒ‘ã‚¹ç½®æ›: {original_img_url} -> {browser_compatible_path}")
                    return full_match.replace(original_img_url, browser_compatible_path)
                else:
                    return full_match
            
            # æ­£è¦è¡¨ç¾ã§ url() ã‚’æ¤œå‡ºã—ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§ç½®æ›
            css_content = re.sub(r'url\([\'"]?([^\'"]+?)[\'"]?\)', replace_url_func, css_content)
            
            # 2. @importæ–‡ã‚’å‡¦ç†
            imports = re.findall(r'@import\s+[\'"]([^\'"]+)[\'"]', css_content)
            for import_url in imports:
                if not import_url.startswith('http'):
                    if import_url.startswith('//'):
                        import_url = 'https:' + import_url
                    elif import_url.startswith('/'):
                        base_domain = '/'.join(url.split('/')[:3])
                        import_url = base_domain + import_url
                    else:
                        base_css_url = '/'.join(url.split('/')[:-1])
                        import_url = urljoin(base_css_url + '/', import_url)
                
                print(f"CSS @importå‡¦ç†: {import_url}")
                local_css = self.download_and_process_css(import_url, resources_dir)
                if local_css != import_url:
                    # @importæ–‡ã§æ­£ç¢ºã«ç½®æ›
                    browser_compatible_css = f"./{local_css}"
                    css_content = css_content.replace(f'@import "{import_url}"', f'@import "{browser_compatible_css}"')
                    css_content = css_content.replace(f"@import '{import_url}'", f"@import '{browser_compatible_css}'")
            
            # 3. @font-faceå†…ã®srcå‚ç…§ã‚‚å‡¦ç†ï¼ˆä¸Šè¨˜ã®url()å‡¦ç†ã§æ—¢ã«å‡¦ç†æ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            # ãƒ•ã‚©ãƒ³ãƒˆã¯æ—¢ã«ä¸Šè¨˜ã®url()å‡¦ç†ã§å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€é‡è¤‡å‡¦ç†ã‚’é¿ã‘ã‚‹
            
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ
            local_path = os.path.join(resources_dir, filename)
            with open(local_path, 'w', encoding='utf-8') as f:
                f.write(css_content)
            
            print(f"CSSå‡¦ç†å®Œäº†: {filename}")
            return filename
            
        except Exception as e:
            print(f"CSSå‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return self.download_resource(url, resources_dir)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
    def extract_novel_info(self, soup):
        """å°èª¬ã®åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡ºï¼ˆ2024å¹´ç‰ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³å¯¾å¿œï¼‰"""
        info = {}
        
        self.debug_log("å°èª¬æƒ…å ±æŠ½å‡ºé–‹å§‹")
        
        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆGeminiåˆ†æã«ã‚ˆã‚‹ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹åŒ–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼‰
        title_selectors = [
            # â˜… Geminiç™ºè¦‹ï¼šãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®æ•°å­—ã‚¯ãƒ©ã‚¹æ§‹é€ 
            ('div', {'class': 'section1'}),  # ã‚¿ã‚¤ãƒˆãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³å€™è£œ
            ('div', {'class': 'section2'}),  # ã‚¿ã‚¤ãƒˆãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³å€™è£œ
            ('h1', {'class': lambda x: x and any(cls.startswith('section') for cls in x if isinstance(cls, str))}),
            # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç”¨
            ('h1', {'class': 'p-novel-title'}),
            ('h1', {'class': 'novel-title'}),
            ('div', {'class': 'p-novel-title'}),
            ('span', {'class': 'novel-title'}),
            # å¾“æ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('h1', {'class': 'title'}),
            ('h1', {'class': 'novel_title'}),
            ('div', {'class': 'novel_title'}),
            ('h1', {}),
            ('title', {})
        ]
        
        self.debug_log("ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºè©¦è¡Œä¸­...")
        for tag, attrs in title_selectors:
            self.debug_log(f"ã‚¿ã‚¤ãƒˆãƒ«ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            title_elem = soup.find(tag, attrs) if attrs else soup.find(tag)
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®å ´åˆã€ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä½™åˆ†ãªæ–‡å­—ã‚’é™¤å»
                if ' - ãƒãƒ¼ãƒ¡ãƒ«ãƒ³' in title_text:
                    title_text = title_text.replace(' - ãƒãƒ¼ãƒ¡ãƒ«ãƒ³', '')
                if title_text and title_text not in ['Unknown Title', '']:
                    info['title'] = title_text
                    self.debug_log(f"ã‚¿ã‚¤ãƒˆãƒ«å–å¾—æˆåŠŸ: {title_text}")
                    break
            else:
                self.debug_log(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ {tag} {attrs} ã§è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # ä½œè€…æŠ½å‡ºï¼ˆå¹…åºƒã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼‰
        author_selectors = [
            # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç”¨
            ('a', {'class': 'p-novel-author'}),
            ('span', {'class': 'p-novel-author'}),
            ('div', {'class': 'novel-author'}),
            ('a', {'class': 'author-link'}),
            # å¾“æ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('a', {'href': lambda x: x and '/user/' in x}),
            ('div', {'class': 'author'}),
            ('span', {'class': 'author'}),
            ('a', {'class': 'author'})
        ]
        
        self.debug_log("ä½œè€…æŠ½å‡ºè©¦è¡Œä¸­...")
        for tag, attrs in author_selectors:
            self.debug_log(f"ä½œè€…ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            if callable(attrs.get('href')):
                author_elem = soup.find(tag, href=attrs['href'])
            else:
                author_elem = soup.find(tag, attrs)
            
            if author_elem:
                author_text = author_elem.get_text(strip=True)
                if author_text and author_text not in ['Unknown Author', '']:
                    info['author'] = author_text
                    self.debug_log(f"ä½œè€…å–å¾—æˆåŠŸ: {author_text}")
                    break
            else:
                self.debug_log(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ {tag} {attrs} ã§è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # æƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã®è©³ç´°èª¿æŸ»
        if not info.get('title') or not info.get('author'):
            self.debug_log("åŸºæœ¬æƒ…å ±å–å¾—å¤±æ•—ã€è©³ç´°èª¿æŸ»ã‚’å®Ÿè¡Œ", "WARNING")
            self.investigate_page_structure(soup)
        
        return info
        
    def investigate_page_structure(self, soup):
        """ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’è©³ç´°èª¿æŸ»"""
        self.debug_log("=== ãƒšãƒ¼ã‚¸æ§‹é€ è©³ç´°èª¿æŸ» ===")
        
        # åˆ©ç”¨å¯èƒ½ãªh1ã‚¿ã‚°
        h1_tags = soup.find_all('h1')
        self.debug_log(f"h1ã‚¿ã‚°æ•°: {len(h1_tags)}")
        for i, h1 in enumerate(h1_tags[:5]):
            classes = h1.get('class', [])
            text = h1.get_text(strip=True)[:50]
            self.debug_log(f"  h1[{i}]: class={classes}, text={text}...")
        
        # åˆ©ç”¨å¯èƒ½ãªãƒªãƒ³ã‚¯
        links = soup.find_all('a', href=True)
        self.debug_log(f"ãƒªãƒ³ã‚¯æ•°: {len(links)}")
        user_links = [link for link in links if '/user/' in link.get('href', '')]
        self.debug_log(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªãƒ³ã‚¯æ•°: {len(user_links)}")
        for i, link in enumerate(user_links[:3]):
            classes = link.get('class', [])
            text = link.get_text(strip=True)[:30]
            href = link.get('href')
            self.debug_log(f"  userlink[{i}]: class={classes}, text={text}..., href={href}")
        
        # ç‰¹æ®Šãªã‚¯ãƒ©ã‚¹åã‚’æ¤œç´¢
        divs_with_class = soup.find_all('div', class_=True)
        unique_classes = set()
        for div in divs_with_class:
            for cls in div.get('class', []):
                if any(keyword in cls.lower() for keyword in ['title', 'author', 'novel', 'name']):
                    unique_classes.add(cls)
        
        self.debug_log(f"é–¢é€£ã‚¯ãƒ©ã‚¹å: {sorted(unique_classes)}")
        
        # spanã‚¿ã‚°ã‚‚ãƒã‚§ãƒƒã‚¯
        spans_with_class = soup.find_all('span', class_=True)
        span_classes = set()
        for span in spans_with_class:
            for cls in span.get('class', []):
                if any(keyword in cls.lower() for keyword in ['title', 'author', 'novel', 'name']):
                    span_classes.add(cls)
        
        self.debug_log(f"é–¢é€£spanã‚¯ãƒ©ã‚¹å: {sorted(span_classes)}")
        
    def extract_novel_info_url(self, soup):
        """ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡º"""
        try:
            # topicPathã‹ã‚‰å°èª¬æƒ…å ±ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            topic_path = soup.find('ol', class_='topicPath')
            if topic_path:
                info_link = topic_path.find('a', href=lambda x: x and 'mode=ss_detail' in x)
                if info_link:
                    href = info_link.get('href')
                    if href.startswith('?'):
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https://syosetu.org/{href}"
                    return href
            
            self.debug_log("å°èª¬æƒ…å ±URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
            return None
        except Exception as e:
            self.debug_log(f"å°èª¬æƒ…å ±URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def extract_comments_url(self, soup):
        """ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡º"""
        try:
            # topicPathã‹ã‚‰æ„Ÿæƒ³ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            topic_path = soup.find('ol', class_='topicPath')
            if topic_path:
                comments_link = topic_path.find('a', href=lambda x: x and 'mode=review' in x)
                if comments_link:
                    href = comments_link.get('href')
                    if href.startswith('?'):
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https://syosetu.org/{href}"
                    elif href.startswith('//'):
                        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https:{href}"
                    elif href.startswith('/'):
                        # ãƒ‘ã‚¹ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        return f"https://syosetu.org{href}"
                    return href
            
            self.debug_log("æ„Ÿæƒ³URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
            return None
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def save_novel_info_page(self, info_url, output_dir, novel_title):
        """å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—ãƒ»ä¿å­˜"""
        try:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {info_url}")
            
            # å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            info_soup = self.get_page(info_url)
            if not info_soup:
                self.debug_log("å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
            
            # ä¿å­˜å‡¦ç†
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
            info_filename = f"{safe_title} - å°èª¬æƒ…å ±"
            
            info_file_path = self.save_complete_page(
                info_soup,
                info_url,
                info_filename,
                output_dir,
                info_url
            )
            
            if info_file_path:
                self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {os.path.basename(info_file_path)}")
                return info_file_path
            else:
                self.debug_log("å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
                
        except Exception as e:
            self.debug_log(f"å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def save_comments_page(self, comments_url, output_dir, novel_title):
        """æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—ãƒ»ä¿å­˜ï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰"""
        try:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {comments_url}")
            
            # ğŸ†• è¤‡æ•°ãƒšãƒ¼ã‚¸ã®æ„Ÿæƒ³ã‚’å–å¾—
            all_pages_content = self.get_all_comments_pages(comments_url)
            if not all_pages_content:
                self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
            
            # ä¿å­˜å‡¦ç†
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', novel_title)
            comments_filename = f"{safe_title} - æ„Ÿæƒ³"
            
            comments_file_path = self.save_complete_page(
                all_pages_content,
                comments_url,
                comments_filename,
                output_dir,
                comments_url
            )
            
            if comments_file_path:
                self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {os.path.basename(comments_file_path)}")
                return comments_file_path
            else:
                self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
                return None
                
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def get_all_comments_pages(self, base_comments_url):
        """ğŸ†• è¤‡æ•°ãƒšãƒ¼ã‚¸ã®æ„Ÿæƒ³ã‚’å…¨ã¦å–å¾—ã—ã¦çµ±åˆ"""
        try:
            self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã‚’é–‹å§‹")
            
            # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            first_page_soup = self.get_page(base_comments_url)
            if not first_page_soup:
                return None
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
            page_links = self.detect_comments_pagination(first_page_soup, base_comments_url)
            
            if len(page_links) <= 1:
                # å˜ä¸€ãƒšãƒ¼ã‚¸ã®å ´åˆ
                self.debug_log("æ„Ÿæƒ³ã¯1ãƒšãƒ¼ã‚¸ã®ã¿ã§ã™")
                return first_page_soup
            
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸æ•°: {len(page_links)}ãƒšãƒ¼ã‚¸")
            
            # å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            all_comments = []
            
            for page_num, page_url in enumerate(page_links, 1):
                self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ {page_num}/{len(page_links)} ã‚’å–å¾—ä¸­: {page_url}")
                
                if page_num == 1:
                    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã¯æ—¢ã«å–å¾—æ¸ˆã¿
                    page_soup = first_page_soup
                else:
                    # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã‚’å–å¾—
                    time.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                    page_soup = self.get_page(page_url)
                    if not page_soup:
                        self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ {page_num} ã®å–å¾—ã«å¤±æ•—", "WARNING")
                        continue
                
                # æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                comments_content = self.extract_comments_content(page_soup)
                if comments_content:
                    all_comments.extend(comments_content)
            
            if not all_comments:
                self.debug_log("æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
                return first_page_soup
            
            # çµ±åˆã•ã‚ŒãŸHTMLã‚’ä½œæˆ
            integrated_soup = self.create_integrated_comments_page(first_page_soup, all_comments, len(page_links))
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆå®Œäº†: {len(all_comments)}ä»¶ã®æ„Ÿæƒ³ã‚’çµ±åˆ")
            
            return integrated_soup
            
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return None

    def detect_comments_pagination(self, soup, base_url):
        """ğŸ†• æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º"""
        try:
            page_links = []
            base_page_num = self.extract_page_number(base_url)
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
            pagination_selectors = [
                # ä¸€èˆ¬çš„ãªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                'div.pagination a',
                'div.pager a', 
                'div.page-nav a',
                # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                'a[href*="mode=review"][href*="page="]',
                'a[href*="&page="]'
            ]
            
            for selector in pagination_selectors:
                pagination_links = soup.select(selector)
                if pagination_links:
                    self.debug_log(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ç™ºè¦‹: {selector} ({len(pagination_links)}å€‹ã®ãƒªãƒ³ã‚¯)")
                    
                    for link in pagination_links:
                        href = link.get('href')
                        if href and 'page=' in href:
                            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                            if href.startswith('?'):
                                # ?page=2 å½¢å¼
                                full_url = base_url.split('?')[0] + href
                            elif href.startswith('./'):
                                # ./?page=2 å½¢å¼
                                full_url = base_url.split('?')[0] + href[2:]  # ./ ã‚’å‰Šé™¤ã—ã¦å‡¦ç†
                            elif href.startswith('//'):
                                # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                                full_url = f"https:{href}"
                            elif href.startswith('/'):
                                # /path?page=2 å½¢å¼
                                full_url = 'https://syosetu.org' + href
                            elif href.startswith('http'):
                                # https://... å½¢å¼
                                full_url = href
                            else:
                                continue
                            
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ãƒ™ãƒ¼ã‚¹ï¼‰
                            page_num = self.extract_page_number(full_url)
                            if not any(self.extract_page_number(existing_url) == page_num for existing_url in page_links):
                                page_links.append(full_url)
                    break
            
            # ãƒ™ãƒ¼ã‚¹URLãŒãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
            if not any(self.extract_page_number(url) == base_page_num for url in page_links):
                page_links.append(base_url)
            
            # ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
            page_links.sort(key=lambda url: self.extract_page_number(url))
            
            self.debug_log(f"æ¤œå‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸: {len(page_links)}ãƒšãƒ¼ã‚¸")
            for i, url in enumerate(page_links, 1):
                self.debug_log(f"  ãƒšãƒ¼ã‚¸{i}: {url}")
            
            return page_links
            
        except Exception as e:
            self.debug_log(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return [base_url]

    def extract_page_number(self, url):
        """URLã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º"""
        try:
            import re
            match = re.search(r'page=(\d+)', url)
            return int(match.group(1)) if match else 1
        except:
            return 1

    def extract_comments_content(self, soup):
        """ğŸ†• æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
        try:
            comments = []
            
            # æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
            comment_selectors = [
                'div.review-item',
                'div.comment-item', 
                'div.impression',
                'tr[id*="review"]',  # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®æ„Ÿæƒ³
                'div[class*="review"]',
                'div[class*="comment"]'
            ]
            
            for selector in comment_selectors:
                comment_elements = soup.select(selector)
                if comment_elements:
                    self.debug_log(f"æ„Ÿæƒ³è¦ç´ ç™ºè¦‹: {selector} ({len(comment_elements)}ä»¶)")
                    comments.extend(comment_elements)
                    break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‹ã‚‰æ„Ÿæƒ³ã‚’æŠ½å‡º
            if not comments:
                table_rows = soup.find_all('tr')
                for row in table_rows:
                    # æ„Ÿæƒ³ã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¡Œã‚’æ¤œå‡º
                    text = row.get_text().strip()
                    if len(text) > 20 and any(keyword in text for keyword in ['é¢ç™½', 'è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„', 'æ„Ÿå‹•', 'ç¶šã']):
                        comments.append(row)
            
            self.debug_log(f"æŠ½å‡ºã•ã‚ŒãŸæ„Ÿæƒ³: {len(comments)}ä»¶")
            return comments
            
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return []

    def create_integrated_comments_page(self, base_soup, all_comments, total_pages):
        """ğŸ†• çµ±åˆã•ã‚ŒãŸæ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ"""
        try:
            # ãƒ™ãƒ¼ã‚¹HTMLã‚’ã‚³ãƒ”ãƒ¼
            integrated_soup = copy.deepcopy(base_soup)
            
            # æ—¢å­˜ã®æ„Ÿæƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‰Šé™¤
            for selector in ['div.review-item', 'div.comment-item', 'tr[id*="review"]']:
                existing_comments = integrated_soup.select(selector)
                for comment in existing_comments:
                    comment.decompose()
            
            # æ„Ÿæƒ³ã‚’æŒ¿å…¥ã™ã‚‹å ´æ‰€ã‚’ç‰¹å®š
            content_area = integrated_soup.find('div', class_='content') or integrated_soup.find('div', class_='main') or integrated_soup.find('body')
            
            if content_area and all_comments:
                # å…¨æ„Ÿæƒ³ã‚’æŒ¿å…¥
                for comment in all_comments:
                    if comment:  # Noneãƒã‚§ãƒƒã‚¯
                        content_area.append(copy.deepcopy(comment))
            
            self.debug_log("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆå®Œäº†")
            return integrated_soup
            
        except Exception as e:
            self.debug_log(f"æ„Ÿæƒ³ãƒšãƒ¼ã‚¸çµ±åˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            import traceback
            self.debug_log(f"çµ±åˆã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}", "ERROR")
            return base_soup
        
    def get_chapter_links(self, soup, base_novel_url):
        """ç« ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹åŒ–ç‰ˆï¼‰"""
        chapter_links = []
        
        print("ç« ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ä¸­...")
        
        # ç¾åœ¨ã®ä½œå“IDã‚’æŠ½å‡º
        novel_id_match = re.search(r'/novel/(\d+)', base_novel_url)
        if not novel_id_match:
            print("ä½œå“IDã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            return []
        
        novel_id = novel_id_match.group(1)
        print(f"å¯¾è±¡ä½œå“ID: {novel_id}")
        
        # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ç« ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ï¼ˆä½œå“IDé™å®šï¼‰
        chapter_selectors = [
            # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®ä¸€èˆ¬çš„ãªç« ãƒªã‚¹ãƒˆ
            ('div', {'class': 'chapter_list'}),
            ('ul', {'class': 'episode_list'}),
            ('div', {'class': 'episode_list'}),
            # ç‰¹å®šä½œå“ã®ç« ã®ã¿ï¼ˆä½œå“IDé™å®šï¼‰
            ('a', {'href': lambda x: x and f'/novel/{novel_id}/' in x and x.count('/') >= 4 and x.endswith('.html')}),
            # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã®ç« ãƒªãƒ³ã‚¯ï¼ˆ./2.html, ./3.htmlç­‰ï¼‰
            ('a', {'href': lambda x: x and re.match(r'\./\d+\.html$', x)}),
            ('li', {'class': 'chapter'}),
            ('div', {'class': 'novel_sublist'})
        ]
        
        for tag, attrs in chapter_selectors:
            print(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            
            if callable(attrs.get('href')):
                elements = soup.find_all(tag, href=attrs['href'])
            else:
                elements = soup.find_all(tag, attrs)
            
            print(f"è¦‹ã¤ã‹ã£ãŸè¦ç´ æ•°: {len(elements)}")
            
            for element in elements:
                if tag == 'a':
                    href = element.get('href')
                    title = element.get_text(strip=True)
                    if href:
                        full_url = urljoin(base_novel_url, href)
                        
                        # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã®ç« ãƒªãƒ³ã‚¯ã®å ´åˆã€ä½œå“IDæ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç›´æ¥è¿½åŠ 
                        if re.match(r'\./\d+\.html$', href):
                            if full_url not in chapter_links:
                                chapter_links.append(full_url)
                                print(f"âœ“ ç« ãƒªãƒ³ã‚¯è¿½åŠ ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰: {title[:30]}... -> {full_url}")
                            else:
                                print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                        # çµ¶å¯¾ãƒ‘ã‚¹å½¢å¼ã®å ´åˆã¯ä½œå“IDæ¤œè¨¼
                        elif f'/novel/{novel_id}/' in full_url:
                            if full_url not in chapter_links:
                                chapter_links.append(full_url)
                                print(f"âœ“ ç« ãƒªãƒ³ã‚¯è¿½åŠ ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰: {title[:30]}... -> {full_url}")
                            else:
                                print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                        else:
                            print(f"âœ— ä½œå“IDä¸ä¸€è‡´ã§ã‚¹ã‚­ãƒƒãƒ—: {full_url} (æœŸå¾…ID: {novel_id})")
                else:
                    # div ã‚„ ul ã®å ´åˆã¯å†…éƒ¨ã®aã‚¿ã‚°ã‚’æ¢ã™
                    links = element.find_all('a', href=True)
                    print(f"ã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒªãƒ³ã‚¯æ•°: {len(links)}")
                    for link in links:
                        href = link.get('href')
                        if href and '/novel/' in href:
                            title = link.get_text(strip=True)
                            full_url = urljoin(self.base_url, href)
                            # ä½œå“IDæ¤œè¨¼
                            if f'/novel/{novel_id}/' in full_url:
                                if full_url not in chapter_links:
                                    chapter_links.append(full_url)
                                    print(f"âœ“ ç« ãƒªãƒ³ã‚¯è¿½åŠ : {title[:30]}... -> {full_url}")
                                else:
                                    print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                            else:
                                print(f"âœ— ä½œå“IDä¸ä¸€è‡´ã§ã‚¹ã‚­ãƒƒãƒ—: {full_url} (æœŸå¾…ID: {novel_id})")
        
        # é€šå¸¸ã®ãƒªãƒ³ã‚¯æ¤œç´¢ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä½œå“IDæ¤œè¨¼ä»˜ãï¼‰
        if not chapter_links:
            print("é€šå¸¸ã®ãƒªãƒ³ã‚¯æ¤œç´¢ã«åˆ‡ã‚Šæ›¿ãˆ...")
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã®ç« ãƒªãƒ³ã‚¯ã‚‚ãƒã‚§ãƒƒã‚¯
                if re.match(r'\./\d+\.html$', href):
                    full_url = urljoin(base_novel_url, href)
                    if full_url not in chapter_links:
                        chapter_links.append(full_url)
                        print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç« ãƒªãƒ³ã‚¯ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰: {text[:30]}... -> {full_url}")
                    else:
                        print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                # ã‚ˆã‚Šå³å¯†ãªç« ãƒªãƒ³ã‚¯ã®æ¡ä»¶ï¼ˆä½œå“IDæ¤œè¨¼ã‚’å«ã‚€ï¼‰
                elif (href and '/novel/' in href and 
                    len(href.split('/')) >= 4 and 
                    href != base_novel_url and
                    not any(x in href for x in ['user', 'tag', 'search', 'ranking'])):
                    
                    full_url = urljoin(self.base_url, href)
                    
                    # ä½œå“IDæ¤œè¨¼ã‚’è¿½åŠ 
                    if f'/novel/{novel_id}/' in full_url:
                        if full_url not in chapter_links:
                            chapter_links.append(full_url)
                            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç« ãƒªãƒ³ã‚¯ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰: {text[:30]}... -> {full_url}")
                        else:
                            print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {full_url}")
                    else:
                        print(f"âœ— ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œå“IDä¸ä¸€è‡´ã§ã‚¹ã‚­ãƒƒãƒ—: {full_url} (æœŸå¾…ID: {novel_id})")
        
        # é‡è¤‡å‰Šé™¤ã¨ä¸¦ã³é †ç¢ºèª
        unique_links = []
        for link in chapter_links:
            if link not in unique_links:
                unique_links.append(link)
        
        print(f"æœ€çµ‚çš„ãªç« æ•°: {len(unique_links)}")
        return unique_links
        
    def create_complete_html(self, title, author, chapters, output_dir):
        """å®Œå…¨ãªHTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        
        # ãƒ¡ã‚¤ãƒ³HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        html_template = f"""<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="ja">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title} - ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ç‰ˆï¼‰</title>
    <style>
        body {{
            font-family: 'Helvetica Neue', Arial, 'Hiragino Kaku Gothic ProN', 'Hiragino Sans', Meiryo, sans-serif;
            line-height: 1.8;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fafafa;
        }}
        .header {{
            border-bottom: 2px solid #ddd;
            padding-bottom: 20px;
            margin-bottom: 30px;
            text-align: center;
        }}
        .title {{
            font-size: 2.5em;
            color: #333;
            margin-bottom: 10px;
        }}
        .author {{
            font-size: 1.3em;
            color: #666;
            margin-bottom: 20px;
        }}
        .navigation {{
            background-color: white;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }}
        .nav-title {{
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #444;
        }}
        .chapter-nav {{
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }}
        .chapter-link {{
            background-color: #007acc;
            color: white;
            padding: 8px 15px;
            text-decoration: none;
            border-radius: 3px;
            font-size: 0.9em;
        }}
        .chapter-link:hover {{
            background-color: #005a9e;
        }}
        .content {{
            background-color: white;
            padding: 30px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        .chapter {{
            margin-bottom: 50px;
            padding-bottom: 30px;
            border-bottom: 1px solid #eee;
        }}
        .chapter:last-child {{
            border-bottom: none;
        }}
        .chapter-title {{
            font-size: 1.8em;
            color: #444;
            border-left: 4px solid #007acc;
            padding-left: 15px;
            margin-bottom: 20px;
            scroll-margin-top: 20px;
        }}
        .chapter-content {{
            font-size: 1.1em;
            line-height: 1.9;
        }}
        .chapter-content p {{
            margin-bottom: 15px;
            text-indent: 1em;
        }}
        .back-to-top {{
            position: fixed;
            bottom: 20px;
            right: 20px;
            background-color: #007acc;
            color: white;
            padding: 10px 15px;
            border-radius: 50%;
            text-decoration: none;
            font-size: 1.2em;
        }}
        .back-to-top:hover {{
            background-color: #005a9e;
        }}
        @media (max-width: 768px) {{
            .chapter-nav {{
                flex-direction: column;
            }}
            .chapter-link {{
                text-align: center;
            }}
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1 class="title">{title}</h1>
        <div class="author">ä½œè€…: {author}</div>
    </div>
    
    <div class="navigation">
        <div class="nav-title">ç›®æ¬¡</div>
        <div class="chapter-nav">
            {self.create_chapter_navigation(chapters)}
        </div>
    </div>
    
    <div class="content">
        {self.create_chapters_html(chapters)}
    </div>
    
    <a href="#" class="back-to-top">â†‘</a>
    
    <script>
        // ã‚¹ãƒ ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {{
            anchor.addEventListener('click', function (e) {{
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {{
                    target.scrollIntoView({{
                        behavior: 'smooth'
                    }});
                }}
            }});
        }});
        
        // ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹ãƒœã‚¿ãƒ³
        document.querySelector('.back-to-top').addEventListener('click', function(e) {{
            e.preventDefault();
            window.scrollTo({{
                top: 0,
                behavior: 'smooth'
            }});
        }});
    </script>
</body>
</html>"""
        
        return html_template
        
    def create_chapter_navigation(self, chapters):
        """ç« ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        nav_html = []
        for i, chapter in enumerate(chapters, 1):
            title = chapter.get('title', f'ç¬¬{i}ç« ')
            nav_html.append(f'<a href="#chapter-{i}" class="chapter-link">{title}</a>')
        return '\n'.join(nav_html)
        
    def create_chapters_html(self, chapters):
        """ç« ã®HTMLã‚’ä½œæˆï¼ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³é¢¨ï¼‰"""
        chapters_html = []
        for i, chapter in enumerate(chapters, 1):
            title = chapter.get('title', f'ç¬¬{i}ç« ')
            content = chapter.get('content', '')
            
            # ãƒãƒ¼ãƒ¡ãƒ«ãƒ³é¢¨ã®ç« æ§‹é€ 
            chapter_html = f'''
            <div id="chapter-{i}" class="chapter-separator">
                <h2>{title}</h2>
                <br/>
                <div>
                    <div id="honbun">
                        {content}
                    </div>
                </div>
            </div>'''
            chapters_html.append(chapter_html)
        return '\n'.join(chapters_html)
        
    def extract_chapter_content(self, soup, chapter_url):
        """ç« ã®æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆ2024å¹´ç‰ˆãƒãƒ¼ãƒ¡ãƒ«ãƒ³å¯¾å¿œï¼‰"""
        self.debug_log(f"æœ¬æ–‡æŠ½å‡ºé–‹å§‹: {chapter_url}")
        
        # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®æœ¬æ–‡ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        content_selectors = [
            # â˜… å®Ÿéš›ã®ãƒãƒ¼ãƒ¡ãƒ«ãƒ³æ§‹é€ ï¼ˆ2024å¹´æœ€æ–°ï¼‰
            ('div', {'id': 'honbun'}),  # â† ã“ã‚ŒãŒå®Ÿéš›ã®æœ¬æ–‡IDï¼
            ('div', {'id': 'entry_box'}),  # æœ¬æ–‡ã‚’å«ã‚€å¤–å´ã®ã‚³ãƒ³ãƒ†ãƒŠ
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®å¤ã„æ§‹é€ 
            ('div', {'class': 'section3'}),
            ('div', {'class': 'section1'}),
            ('div', {'class': 'section2'}),
            ('div', {'class': 'section4'}),
            ('div', {'class': 'section5'}),
            ('div', {'class': 'section6'}),
            ('div', {'class': 'section7'}),
            ('div', {'class': 'section8'}),
            ('div', {'class': 'section9'}),
            # æ•°å­—ä»˜ãsectionã‚¯ãƒ©ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            ('div', {'class': lambda x: x and any(cls.startswith('section') and len(cls) > 7 and cls[7:].isdigit() for cls in x if isinstance(cls, str))}),
            # 2024å¹´ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç”¨ã®æ–°ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('div', {'class': 'p-novel-text'}),
            ('div', {'class': 'novel-text'}),
            ('section', {'class': 'p-novel-text'}),
            ('div', {'class': 'p-chapter-text'}),
            ('div', {'class': 'chapter-text'}),
            ('div', {'class': 'p-story-text'}),
            ('div', {'class': 'story-text'}),
            ('div', {'class': 'episode-text'}),
            ('div', {'class': 'p-episode-text'}),
            ('div', {'class': 'p-content-text'}),
            ('div', {'class': 'content-text'}),
            # IDãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('div', {'id': 'novel_body'}),
            ('div', {'id': 'main_text'}),
            ('div', {'id': 'chapter_body'}),
            ('div', {'id': 'story_body'}),
            ('div', {'id': 'content_body'}),
            ('div', {'id': 'episode_body'}),
            # å¾“æ¥ã®ãƒãƒ¼ãƒ¡ãƒ«ãƒ³æœ¬æ–‡ã‚¯ãƒ©ã‚¹
            ('div', {'class': 'novel_body'}),
            ('div', {'class': 'novel_view'}),
            ('div', {'class': 'novel_content'}),
            ('div', {'class': 'chapter_body'}),
            ('div', {'class': 'ss_body'}),
            ('div', {'class': 'contents'}),
            ('div', {'class': 'main_content'}),
            ('div', {'class': 'story_content'}),
            # ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            ('article', {'class': None}),
            ('main', {'class': None}),
            ('section', {'class': None}),
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰
            ('div', {'class': lambda x: x and any(keyword in ' '.join(x).lower() for keyword in ['body', 'content', 'text', 'story', 'chapter', 'episode', 'novel'])}),
            # æœ€å¾Œã®æ‰‹æ®µï¼šå¤§ããªdivã‚¿ã‚°ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒå¤šã„ï¼‰
            ('div', {'data-content': 'main'}),
            ('div', {'role': 'main'}),
        ]
        
        for tag, attrs in content_selectors:
            self.debug_log(f"æœ¬æ–‡ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œ: {tag} {attrs}")
            
            try:
                if callable(attrs.get('class')):
                    elements = soup.find_all(tag, class_=attrs['class'])
                elif callable(attrs.get('id')):
                    elements = soup.find_all(tag, id=attrs['id'])
                elif attrs:
                    elements = soup.find_all(tag, attrs)
                else:
                    elements = soup.find_all(tag)
                
                self.debug_log(f"è¦‹ã¤ã‹ã£ãŸè¦ç´ æ•°: {len(elements)}")
                
                for element in elements:
                    content_text = element.get_text(strip=True)
                    content_length = len(content_text)
                    
                    # ã‚ˆã‚Šè©³ç´°ãªæ¡ä»¶ãƒã‚§ãƒƒã‚¯
                    if content_length > 50:  # åŸºæº–ã‚’ç·©å’Œï¼ˆçŸ­ã„ç« ã«ã‚‚å¯¾å¿œï¼‰
                        # æœ¬æ–‡ã‚‰ã—ã„å†…å®¹ã‹ãƒã‚§ãƒƒã‚¯
                        if self.is_likely_novel_content(content_text):
                            self.debug_log(f"æœ¬æ–‡å–å¾—æˆåŠŸ: {content_length}æ–‡å­—")
                            # å®Œå…¨ãªè¦‹ãŸç›®ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€å…ƒã®HTMLæ§‹é€ ã‚’ä¿æŒ
                            return self.preserve_original_formatting(element)
                        else:
                            self.debug_log(f"æœ¬æ–‡å€™è£œã ãŒå†…å®¹ãŒé©åˆ‡ã§ãªã„: {content_length}æ–‡å­—")
                    else:
                        self.debug_log(f"è¦ç´ ãŒçŸ­ã™ãã¾ã™: {content_length}æ–‡å­—")
                        
            except Exception as e:
                self.debug_log(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è©¦è¡Œã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
        
        self.debug_log("æœ¬æ–‡å–å¾—å¤±æ•—: é©åˆ‡ãªè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
        
        # è©³ç´°èª¿æŸ»ã‚’å®Ÿè¡Œ
        self.investigate_content_structure(soup)
        
        # æœ€å¾Œã®æ‰‹æ®µï¼šæœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¦ç´ ã‚’é¸æŠ
        self.debug_log("æœ€å¾Œã®æ‰‹æ®µï¼šæœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’æ¤œç´¢")
        longest_element = None
        longest_length = 0
        
        for div in soup.find_all('div'):
            content_text = div.get_text(strip=True)
            if len(content_text) > longest_length and len(content_text) > 100:
                # æ˜ã‚‰ã‹ã«ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if not any(keyword in content_text for keyword in ['ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒ˜ãƒƒãƒ€ãƒ¼', 'ãƒ•ãƒƒã‚¿ãƒ¼']):
                    longest_element = div
                    longest_length = len(content_text)
        
        if longest_element:
            self.debug_log(f"æœ€é•·ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’ä½¿ç”¨: {longest_length}æ–‡å­—")
            return self.preserve_original_formatting(longest_element)
        
        return ""
        
    def preserve_original_formatting(self, element):
        """å…ƒã®HTMLæ§‹é€ ã‚’ä¿æŒã—ã¦è¦‹ãŸç›®ã‚’å®Œå…¨å†ç¾"""
        # å…ƒã®HTMLã‚¿ã‚°ã‚’ä¿æŒ
        html_content = str(element)
        
        # ä¸è¦ãªå±æ€§ã‚’å‰Šé™¤ï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®ãŸã‚ï¼‰
        import re
        # onclick, onloadç­‰ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
        html_content = re.sub(r'\s*on\w+\s*=\s*["\'][^"\'>]*["\']', '', html_content)
        # data-trackç­‰ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°å±æ€§ã‚’å‰Šé™¤
        html_content = re.sub(r'\s*data-track\w*\s*=\s*["\'][^"\'>]*["\']', '', html_content)
        
        self.debug_log(f"å…ƒã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿æŒ: {len(html_content)}ãƒã‚¤ãƒˆ")
        return html_content
        
    def is_likely_novel_content(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆãŒå°èª¬ã®æœ¬æ–‡ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        # åŸºæœ¬çš„ãªé•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        if len(text) < 30:  # ã•ã‚‰ã«ç·©å’Œ
            return False
        
        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚„ä¸è¦ãªè¦ç´ ã‚’é™¤å¤–
        exclusion_keywords = [
            'ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒ˜ãƒƒãƒ€ãƒ¼', 'ãƒ•ãƒƒã‚¿ãƒ¼',
            'ã‚µã‚¤ãƒ‰ãƒãƒ¼', 'åºƒå‘Š', 'ã‚¢ãƒ‰ãƒã‚¿ã‚¤ã‚º', 'ã‚³ãƒ¡ãƒ³ãƒˆ',
            'ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'ãŠçŸ¥ã‚‰ã›', 'åˆ©ç”¨è¦ç´„', 'æ¤œç´¢',
            'ãƒ­ã‚°ã‚¤ãƒ³', 'ãƒã‚¤ãƒšãƒ¼ã‚¸', 'ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',
            'ã‚¿ã‚°ä¸€è¦§', 'ã‚«ãƒ†ã‚´ãƒª', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«',
            'ãƒ•ã‚©ãƒ­ãƒ¼', 'ã„ã„ã­', 'ã‚·ã‚§ã‚¢', 'ãƒ„ã‚¤ãƒ¼ãƒˆ',
            'ã‚³ãƒ”ãƒ¼', 'URL', 'ãƒªãƒ³ã‚¯', 'ã‚½ãƒ¼ã‚·ãƒ£ãƒ«'
        ]
        
        for keyword in exclusion_keywords:
            if keyword in text:
                self.debug_log(f"é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {keyword}")
                return False
        
        # å°èª¬ã‚‰ã—ã„è¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        novel_indicators = [
            'ã€‚', 'ã€Œ', 'ã€', 'ã ', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™',
            'ã—ãŸ', 'ã™ã‚‹', 'ãã®', 'ã“ã®', 'ã‚ã®', 'ãŒ', 'ã‚’', 'ã«', 'ã¯', 'ã§'
        ]
        
        indicator_count = sum(1 for indicator in novel_indicators if indicator in text)
        if indicator_count < 2:  # 3â†’2ã«ç·©å’Œ
            self.debug_log(f"å°èª¬ã‚‰ã—ã„è¦ç´ ãŒå°‘ãªã„: {indicator_count}/{len(novel_indicators)}")
            return False
        
        # â˜… ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ã®ä½œå“èª¬æ˜æ–‡ã¯æœ¬æ–‡ã§ã¯ãªã„ï¼ˆé™¤å¤–ï¼‰
        if ('ç·åˆè©•ä¾¡ï¼š' in text or 'è©•ä¾¡ï¼š' in text or 'é€£è¼‰ï¼š' in text or 
            'æ›´æ–°æ—¥æ™‚ï¼š' in text or 'å°èª¬æƒ…å ±' in text or 'href="//syosetu.org/' in str(text)):
            self.debug_log("ãƒãƒ¼ãƒ¡ãƒ«ãƒ³ç‰¹æœ‰ã®ä½œå“èª¬æ˜æ–‡ã¨ã—ã¦é™¤å¤–")
            return False
        
        self.debug_log(f"å°èª¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã—ã¦èªè­˜: {indicator_count}å€‹ã®æŒ‡æ¨™ã‚’ç¢ºèª")
        return True
        
    def investigate_content_structure(self, soup):
        """æœ¬æ–‡æ§‹é€ ã‚’è©³ç´°èª¿æŸ»"""
        self.debug_log("=== æœ¬æ–‡æ§‹é€ è©³ç´°èª¿æŸ» ===")
        
        # ãƒ†ã‚­ã‚¹ãƒˆé‡ã®å¤šã„è¦ç´ ã‚’æ¤œç´¢
        text_elements = []
        for element in soup.find_all(['div', 'section', 'article', 'main', 'p']):
            text = element.get_text(strip=True)
            if len(text) > 50:
                text_elements.append({
                    'tag': element.name,
                    'class': element.get('class', []),
                    'id': element.get('id', ''),
                    'length': len(text),
                    'preview': text[:100] + '...' if len(text) > 100 else text
                })
        
        # é•·ã•é †ã§ã‚½ãƒ¼ãƒˆ
        text_elements.sort(key=lambda x: x['length'], reverse=True)
        
        self.debug_log(f"ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ä¸Šä½5ä»¶:")
        for i, elem in enumerate(text_elements[:5]):
            self.debug_log(f"  [{i+1}] {elem['tag']} class={elem['class']} id={elem['id']} length={elem['length']}")
            self.debug_log(f"      preview: {elem['preview']}")
        
        # ã‚¯ãƒ©ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        all_classes = set()
        for div in soup.find_all('div', class_=True):
            all_classes.update(div.get('class', []))
        
        content_related_classes = [cls for cls in all_classes 
                                 if any(keyword in cls.lower() 
                                       for keyword in ['text', 'content', 'body', 'story', 'novel', 'chapter', 'section'])]
        
        self.debug_log(f"æœ¬æ–‡é–¢é€£ã‚¯ãƒ©ã‚¹åå€™è£œ: {sorted(content_related_classes)}")
    
    def process_single_chapter(self, chapter_url):
        """å˜ä¸€ã®ç« ãƒšãƒ¼ã‚¸ã‚’å®Œå…¨ä¿å­˜"""
        print(f"=== å˜è©±å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ===")
        print(f"URL: {chapter_url}")
        soup = self.get_page(chapter_url)
        
        if not soup:
            print("ç« ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç°¡å˜ã«æŠ½å‡ºï¼ˆtitleã‚¿ã‚°ã‹ã‚‰ï¼‰
        title = soup.title.string if soup.title else "ç« ãƒšãƒ¼ã‚¸"
        print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        
        # #honbunè¦ç´ ã®å­˜åœ¨ã‚’ç¢ºèª
        honbun_element = soup.find('div', {'id': 'honbun'})
        if honbun_element:
            print(f"#honbunè¦ç´ ã‚’ç™ºè¦‹: {len(honbun_element.get_text(strip=True))}æ–‡å­—")
            self.debug_log(f"#honbunè¦ç´ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {honbun_element.get_text(strip=True)[:200]}...")
        else:
            print("è­¦å‘Š: #honbunè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.debug_log("è­¦å‘Š: #honbunè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - æ§‹é€ èª¿æŸ»ã‚’å®Ÿè¡Œ")
            # ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’èª¿æŸ»
            all_divs = soup.find_all('div', id=True)
            print(f"IDä»˜ãdivè¦ç´ æ•°: {len(all_divs)}")
            for div in all_divs[:10]:  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
                div_id = div.get('id')
                text_preview = div.get_text(strip=True)[:50]
                print(f"  ID: {div_id}, ãƒ†ã‚­ã‚¹ãƒˆ: {text_preview}...")
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆãƒ–ãƒ©ã‚¦ã‚¶äº’æ›å½¢å¼ï¼‰
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_dir = os.path.join("saved_novels", safe_title)
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’è¨­å®š
        self.browser_compatible_name = safe_title + "_files"
        
        # å®Œå…¨ãªãƒšãƒ¼ã‚¸ã¨ã—ã¦ä¿å­˜ï¼ˆæ§‹é€ ã‚’ãã®ã¾ã¾ä¿æŒï¼‰
        base_url = "https://syosetu.org"
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_file = self.save_complete_page(soup, base_url, safe_title, output_dir, chapter_url)
        print(f"å®Œå…¨ãªãƒšãƒ¼ã‚¸ã¨ã—ã¦ä¿å­˜: 1ãƒšãƒ¼ã‚¸")
        
        return output_file
    
    def save_complete_page(self, soup, base_url, 
                          title, save_dir, page_url):
        """ãƒšãƒ¼ã‚¸ã‚’å®Œå…¨ãªå½¢ã§ä¿å­˜ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã¨åŒç­‰ï¼‰"""
        print("=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ä¿å­˜é–‹å§‹ ===")
        
        # ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’å–å¾—
        resources_dir_name = getattr(self, 'browser_compatible_name', 'resources')
        
        # ãƒªã‚½ãƒ¼ã‚¹ã‚’å‡¦ç†ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
        soup = self.process_html_resources(soup, save_dir)
        
        # ãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã®ã‚ˆã†ã«ã€å…ƒURLã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
        html_tag = soup.find('html')
        if html_tag:
            from bs4 import Comment
            comment = Comment(f' saved from url=({len(page_url):04d}){page_url} ')
            html_tag.insert(0, comment)
        
        # ãƒ™ãƒ¼ã‚¹URLã‚’è¨­å®š
        base_url = '/'.join(page_url.split('/')[:3])  # https://syosetu.org
        
        # ç›¸å¯¾ãƒªãƒ³ã‚¯ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã¨åŒæ§˜ï¼‰
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                if href.startswith('//'):
                    link['href'] = 'https:' + href
                elif href.startswith('/') and not href.startswith('//'):
                    link['href'] = base_url + href
                elif href.startswith('./'):
                    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                    current_dir = '/'.join(page_url.split('/')[:-1])
                    link['href'] = current_dir + '/' + href[2:]
        
        # ã™ã¹ã¦ã®ç”»åƒã®srcã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        for img in soup.find_all('img', src=True):
            src = img.get('src')
            if src and not src.startswith('./' + resources_dir_name + '/'):  # æ—¢ã«ãƒ­ãƒ¼ã‚«ãƒ«åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ
                if src.startswith('//'):
                    img['src'] = 'https:' + src
                elif src.startswith('/') and not src.startswith('//'):
                    img['src'] = base_url + src
        
        # CSS ã‚„ JS ã®å‚ç…§ã‚‚åŒæ§˜ã«å‡¦ç†
        for link in soup.find_all('link', href=True):
            href = link.get('href')
            if href and not href.startswith('./' + resources_dir_name + '/'):
                if href.startswith('//'):
                    link['href'] = 'https:' + href
                elif href.startswith('/') and not href.startswith('//'):
                    link['href'] = base_url + href
        
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src and not src.startswith('./' + resources_dir_name + '/'):
                if src.startswith('//'):
                    script['src'] = 'https:' + src
                elif src.startswith('/') and not src.startswith('//'):
                    script['src'] = base_url + src
        
        # ãƒ¡ã‚¿æƒ…å ±ã‚’ä¿æŒï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ä¿å­˜ã®ã‚ˆã†ã«ï¼‰
        head = soup.find('head')
        if head:
            # ä¿å­˜æ—¥æ™‚ã‚’è¿½åŠ 
            from datetime import datetime
            save_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            meta_save = soup.new_tag('meta')
            meta_save['name'] = 'save-date'
            meta_save['content'] = save_time
            head.append(meta_save)
            
            # ä¿å­˜å…ƒURLã‚’è¿½åŠ 
            meta_source = soup.new_tag('meta')
            meta_source['name'] = 'source-url'
            meta_source['content'] = page_url
            head.append(meta_source)
        
        # å®Œå…¨ãªHTMLã¨ã—ã¦ä¿å­˜
        safe_filename = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_file = os.path.join(save_dir, f"{safe_filename}.html")
        
        # UTF-8 BOMã‚’å«ã‚ã¦ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›æ€§ã‚’å‘ä¸Š
        html_content = str(soup)
        
        with open(output_file, 'w', encoding='utf-8-sig') as f:
            f.write(html_content)
        
        print(f"=== ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¬ãƒ™ãƒ«å®Œå…¨ä¿å­˜å®Œäº†: {output_file} ===")
        return output_file
        
    def scrape_novel(self, novel_url):
        """å°èª¬å…¨ä½“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ä¿å­˜ï¼ˆå®Œå…¨ãƒ¢ãƒ¼ãƒ‰ä¸€æœ¬åŒ–ï¼‰"""
        print(f"=== å°èª¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ===")
        print(f"URL: {novel_url}")
        
        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®åˆæœŸå¾…æ©Ÿ
        time.sleep(2)
        
        # ç« ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä¾‹: /novel/378070/2.htmlï¼‰
        is_chapter_page = bool(re.search(r'/novel/\d+/\d+\.html$', novel_url))
        print(f"ç« ãƒšãƒ¼ã‚¸åˆ¤å®š: {is_chapter_page}")
        
        if is_chapter_page:
            print("ç« ãƒšãƒ¼ã‚¸ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨è©±å–å¾—ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            # ç« ãƒšãƒ¼ã‚¸ã®URLã‹ã‚‰ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®URLã‚’æ§‹ç¯‰
            # ä¾‹: https://syosetu.org/novel/378070/2.html â†’ https://syosetu.org/novel/378070/
            match = re.search(r'(https?://[^/]+/novel/\d+)/', novel_url)
            print(f"URLæ­£è¦è¡¨ç¾ãƒãƒƒãƒçµæœ: {match}")
            if match:
                index_url = match.group(1) + '/'
                print(f"ç›®æ¬¡ãƒšãƒ¼ã‚¸URL: {index_url}")
                print(f"å…ƒã®URL: {novel_url}")
                print(f"URLæ¯”è¼ƒ: {index_url} != {novel_url} = {index_url != novel_url}")
                
                # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼šç¾åœ¨ã®URLãŒæ—¢ã«ç›®æ¬¡ãƒšãƒ¼ã‚¸ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                if index_url != novel_url:
                    print("ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨è©±å–å¾—ã‚’å®Ÿè¡Œã—ã¾ã™...")
                    # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨è©±å–å¾—ã‚’å®Ÿè¡Œ
                    return self.scrape_novel(index_url)
                else:
                    print("æ—¢ã«ç›®æ¬¡ãƒšãƒ¼ã‚¸ã§ã™ã€‚é€šå¸¸å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
            else:
                print("ç›®æ¬¡ãƒšãƒ¼ã‚¸URLã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å˜è©±å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
                return self.process_single_chapter(novel_url)
        
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        soup = self.get_page(novel_url)
            
        if not soup:
            print("ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
            
        # å°èª¬æƒ…å ±ã‚’æŠ½å‡º
        novel_info = self.extract_novel_info(soup)
        title = novel_info.get('title', 'Unknown Title')
        author = novel_info.get('author', 'Unknown Author')
        
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        print(f"ä½œè€…: {author}")
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        output_dir = os.path.join("saved_novels", safe_title)
        os.makedirs(output_dir, exist_ok=True)
        
        # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ï¼ˆè¤‡æ•°ç« ãŒã‚ã‚‹å ´åˆï¼‰
        index_file_path = None
        
        # ç« ãƒªãƒ³ã‚¯ã‚’å–å¾—
        chapter_links = self.get_chapter_links(soup, novel_url)
        print(f"ç« æ•°: {len(chapter_links)}")
        
        # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã¨ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if len(chapter_links) > 1:
            print("ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ä¸­...")
            # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
            index_file_path = self.save_complete_page(
                soup, 
                novel_url,
                f"{safe_title} - ç›®æ¬¡",
                output_dir, 
                novel_url
            )
            if index_file_path:
                print(f"ğŸ“– ç›®æ¬¡ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {os.path.basename(index_file_path)}")
            
            print("ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆCSSã€JSã€ç”»åƒç­‰ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            # ãƒªã‚½ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œï¼ˆå„ç« ã§ã¯ãƒªã‚½ãƒ¼ã‚¹å†å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            self.process_html_resources(soup, output_dir)
            print("ğŸ“ ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å„ç« ã§ã¯ãƒªã‚½ãƒ¼ã‚¹å†å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            
            # ğŸ†• æ–°æ©Ÿèƒ½: å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³ä¿å­˜ï¼ˆãƒ•ãƒ©ã‚°ã§åˆ¶å¾¡ï¼‰
            info_file_name = None
            comments_file_name = None
            
            if self.enable_novel_info_saving:
                print("å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ä¸­...")
                info_url = self.extract_novel_info_url(soup)
                if info_url:
                    info_file_path = self.save_novel_info_page(info_url, output_dir, title)
                    if info_file_path:
                        info_file_name = os.path.basename(info_file_path)
                        print(f"ğŸ“ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {info_file_name}")
                    else:
                        print("âš ï¸ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    print("âš ï¸ å°èª¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            if self.enable_comments_saving:
                print("æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ä¸­...")
                comments_url = self.extract_comments_url(soup)
                if comments_url:
                    comments_file_path = self.save_comments_page(comments_url, output_dir, title)
                    if comments_file_path:
                        comments_file_name = os.path.basename(comments_file_path)
                        print(f"ğŸ’¬ æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ä¿å­˜å®Œäº†: {comments_file_name}")
                    else:
                        print("âš ï¸ æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    print("âš ï¸ æ„Ÿæƒ³ãƒšãƒ¼ã‚¸ã®URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        if not chapter_links:
            print("ç« ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å˜ä¸€ãƒšãƒ¼ã‚¸ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
            # å˜ä¸€ãƒšãƒ¼ã‚¸ã®å ´åˆ
            chapter_content = self.extract_chapter_content(soup, novel_url)
            if chapter_content:
                chapters = [{
                    'title': title,
                    'content': chapter_content
                }]
            else:
                print("æœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
        else:
            # å„ç« ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            chapters = []
            chapter_mapping = {}  # URL -> ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            saved_chapters = []
            
            # ã¾ãšç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
            if index_file_path:
                index_filename = os.path.basename(index_file_path)
            else:
                index_filename = None
            
            # ç« ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’äº‹å‰ã«ä½œæˆï¼ˆå‰ã®è©±ãƒ»æ¬¡ã®è©±ãƒªãƒ³ã‚¯ç”¨ï¼‰
            print("ç« ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆä¸­...")
            # åˆæœŸãƒãƒƒãƒ”ãƒ³ã‚°ã¯ç©ºã§é–‹å§‹ï¼ˆçŸ­ç¸®å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆã‚’å›é¿ï¼‰
            chapter_mapping = {}
            print(f"äº‹å‰ãƒãƒƒãƒ”ãƒ³ã‚°æº–å‚™å®Œäº†: {len(chapter_links)}ç« ")
            
            for i, chapter_url in enumerate(chapter_links, 1):
                print(f"ç«  {i}/{len(chapter_links)} ã‚’å–å¾—ä¸­: {chapter_url}")
                
                # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®é©å¿œçš„å¾…æ©Ÿ
                if i > 1:
                    wait_time = min(3 + (i // 10), 8)  # ç« æ•°ãŒå¢—ãˆã‚‹ã»ã©å¾…æ©Ÿæ™‚é–“ã‚’å¢—åŠ 
                    print(f"ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ {wait_time} ç§’å¾…æ©Ÿ...")
                    time.sleep(wait_time)
                
                chapter_soup = self.get_page(chapter_url)
                
                if chapter_soup:
                    # ãƒªã‚½ãƒ¼ã‚¹å‡¦ç†ã¯ç›®æ¬¡ãƒšãƒ¼ã‚¸ã§å®Œäº†æ¸ˆã¿ã®ãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹èª¿æ•´ã®ã¿
                    chapter_soup = self.adjust_resource_paths_only(chapter_soup, output_dir)
                    
                    # ç« ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                    chapter_title = chapter_soup.find('title')
                    if chapter_title:
                        chapter_title_text = chapter_title.get_text(strip=True)
                    else:
                        chapter_title_text = f"ç¬¬{i}è©±"
                    
                    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒªãƒ³ã‚¯ä¿®æ­£ï¼ˆç¬¬1å›ç›®ï¼šä»®ã®ãƒãƒƒãƒ”ãƒ³ã‚° + å°èª¬æƒ…å ±ãƒ»æ„Ÿæƒ³å¯¾å¿œï¼‰
                    chapter_soup = self.fix_local_navigation_links(
                        chapter_soup, 
                        chapter_mapping, 
                        chapter_url, 
                        index_filename,
                        info_file_name,
                        comments_file_name
                    )
                    
                    # ç« ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                    safe_chapter_title = re.sub(r'[<>:"/\\|?*]', '_', chapter_title_text)
                    chapter_file_path = self.save_complete_page(
                        chapter_soup, 
                        chapter_url,
                        safe_chapter_title,
                        output_dir, 
                        chapter_url
                    )
                    
                    if chapter_file_path:
                        chapter_filename = os.path.basename(chapter_file_path)
                        # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ›´æ–°
                        chapter_mapping[chapter_url] = chapter_filename
                        print(f"ãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°: {chapter_url} -> {chapter_filename}")
                        saved_chapters.append({
                            'url': chapter_url,
                            'title': chapter_title_text,
                            'file_path': chapter_file_path,
                            'filename': chapter_filename
                        })
                        print(f"ç«  {i} ä¿å­˜å®Œäº†: {chapter_title_text} -> {chapter_filename}")
                    else:
                        print(f"ç«  {i} ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    print(f"ç«  {i} ã®ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ç¬¬2å›ç›®ï¼šå…¨ç« ã®ç›¸äº’ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£
            print("ç« é–“ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
            for chapter_info in saved_chapters:
                if os.path.exists(chapter_info['file_path']):
                    with open(chapter_info['file_path'], 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    soup = self.fix_local_navigation_links(
                        soup, 
                        chapter_mapping, 
                        chapter_info['url'], 
                        index_filename,
                        info_file_name,
                        comments_file_name
                    )
                    
                    with open(chapter_info['file_path'], 'w', encoding='utf-8') as f:
                        f.write(str(soup))
            
            # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚‚ä¿®æ­£
            if index_file_path and os.path.exists(index_file_path):
                print("ç›®æ¬¡ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ä¸­...")
                with open(index_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                soup = self.fix_local_navigation_links(
                    soup, 
                    chapter_mapping, 
                    novel_url, 
                    None,
                    info_file_name,
                    comments_file_name
                )
                
                with open(index_file_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
            
            # æ—§å½¢å¼ã®ç« ãƒ‡ãƒ¼ã‚¿ã‚‚ä½œæˆï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
            chapters = [{'title': ch['title'], 'content': ''} for ch in saved_chapters]
        
        if not chapters:
            print("å–å¾—ã§ããŸç« ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
        print(f"å°èª¬ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: {output_dir}")
        print(f"å–å¾—ã—ãŸç« æ•°: {len(chapters)}")
        
        # æœ€åˆã®ç« ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™ï¼ˆå¾“æ¥ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
        if saved_chapters:
            return saved_chapters[0]['file_path']
        else:
            return None
        
    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾"""
        if self.driver:
            self.driver.quit()
            print("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    scraper = None
    
    try:
        scraper = HamelnFinalScraper()
        
        print("ãƒãƒ¼ãƒ¡ãƒ«ãƒ³å°èª¬ä¿å­˜ãƒ„ãƒ¼ãƒ«ï¼ˆæœ€çµ‚ç‰ˆï¼‰")
        print("å®Œå…¨ãƒ¢ãƒ¼ãƒ‰ï¼ˆCSSãƒ»ç”»åƒãƒ»JavaScriptå«ã‚€å®Œå…¨ä¿å­˜ï¼‰")
        print("=" * 50)
        
        import sys
        if len(sys.argv) > 1:
            novel_url = sys.argv[1]
            print(f"æŒ‡å®šã•ã‚ŒãŸURL: {novel_url}")
        else:
            novel_url = input("å°èª¬ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
        
        if not novel_url:
            print("URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return
        
        result = scraper.scrape_novel(novel_url)
        if result:
            print(f"\nâœ“ ä¿å­˜å®Œäº†: {result}")
        else:
            print("\nâœ— ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
    except KeyboardInterrupt:
        print("\n\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if scraper:
            scraper.close()

if __name__ == "__main__":
    main()